Layer: encoder.5.0.block.0.weight
Parameters: tensor([1.0040, 1.0196, 1.0704, 1.1298, 1.0712, 1.0603, 1.0917, 1.0490, 1.0462,
        1.0612, 1.0613, 1.0652, 1.0652, 1.0995, 1.0740, 1.0476, 1.0782, 1.0395,
        1.0467, 1.0482, 1.0595, 1.0212, 1.0892, 1.0564, 1.0591, 1.0401, 1.0688,
        1.0644, 1.0795, 1.0583, 1.0961, 1.0673, 1.0423, 1.0371, 1.0704, 1.0984,
        1.0560, 1.0819, 1.1150, 1.0513, 1.0395, 1.0705, 1.0471, 1.1171, 1.0756,
        1.0950, 1.0425, 1.0132, 1.0667, 1.0646, 1.0535, 1.0627, 1.0746, 1.0333,
        1.0406, 1.0506, 1.0950, 1.0659, 1.0542, 1.1152, 1.1051, 1.0496, 1.0604,
        1.0909, 1.1683, 1.0517, 1.0722, 1.0735, 1.0947, 1.0375, 1.0749, 1.0275,
        1.0235, 1.0754, 1.0849, 1.0203, 1.1024, 1.0624, 1.0068, 1.0479, 1.0372,
        1.0356, 1.0895, 1.0444, 1.0799, 1.0163, 1.0800, 1.0689, 1.0467, 1.0579,
        1.0647, 1.0638, 1.0726, 1.1013, 1.0186, 1.0575, 1.0328, 1.0457, 1.0428,
        1.0432, 1.0654, 1.0768, 1.0599, 1.0159, 1.1010, 1.0476, 1.0689, 1.0837,
        1.0496, 1.0630, 1.0956, 1.0832, 1.0394, 1.0422, 1.0577, 1.0625, 1.0778,
        1.0391, 1.0598, 1.0762, 1.0847, 1.0916, 1.1236, 1.0519, 1.0955, 1.0524,
        1.0737, 1.0769, 1.1136, 1.0508, 1.0745, 1.0555, 1.0512, 1.0406, 1.0306,
        1.0552, 1.0989, 1.0317, 1.0444, 1.0923, 1.0401, 1.0403, 1.0319, 1.1142,
        1.0648, 1.0799, 1.0375, 1.0953, 1.0798, 1.0414, 1.0859, 1.0929, 1.0831,
        1.0774, 1.0578, 1.0532, 1.0767, 1.0957, 1.1136, 1.0403, 1.0819, 1.0746,
        1.0552, 1.0697, 1.0258, 1.0947, 1.0903, 1.0740, 1.1220, 1.0647, 1.0678,
        1.1130, 1.0924, 1.0170, 1.0599, 1.0582, 1.0503, 1.0619, 1.0062, 1.0201,
        1.0889, 1.0654, 1.0477, 1.0637, 1.0801, 1.0709, 1.0788, 1.0638, 1.0835,
        1.0617, 1.0874, 1.0522])

Layer: encoder.5.0.block.0.bias
Parameters: tensor([-1.6078e-02, -1.6273e-02,  1.2383e-02, -2.3829e-02,  1.8708e-02,
         1.6946e-02,  1.4111e-02, -1.2017e-02,  1.7149e-02, -3.2132e-02,
         1.2410e-02, -4.0590e-02,  5.3764e-03,  6.9027e-03, -2.3430e-02,
         8.9716e-03,  1.2777e-02,  3.0438e-02, -6.2387e-02, -2.0528e-02,
         1.7131e-02, -2.6253e-02, -3.1117e-02,  1.5881e-02,  3.3949e-03,
         7.9496e-03,  9.9044e-03, -1.5853e-02,  2.6235e-02, -2.7324e-03,
        -3.7921e-02, -5.8391e-03, -1.7872e-02, -4.7602e-02,  2.9936e-02,
        -1.5696e-02, -1.5856e-02,  2.4063e-03,  1.3723e-02,  5.0936e-03,
        -1.6100e-02,  2.6463e-02, -1.1127e-02,  1.1221e-03, -2.4758e-02,
         6.5538e-02,  1.0533e-02, -4.7662e-03, -1.3937e-02,  3.3054e-03,
         2.5036e-02, -3.0782e-03, -1.5611e-02,  2.8689e-03,  1.1901e-02,
         1.6200e-02,  3.2648e-02, -2.1268e-02,  1.2268e-02,  1.2687e-02,
        -1.7268e-02, -2.3912e-02, -1.5457e-02,  1.9240e-02, -8.9838e-03,
        -1.8814e-02,  3.3694e-02,  3.2360e-05, -3.4748e-04,  2.3958e-03,
         2.7666e-02, -1.9382e-02, -1.9382e-02, -1.3705e-02,  2.1839e-03,
        -2.3970e-02,  2.0861e-02, -1.2946e-02,  1.0923e-04, -4.0910e-03,
        -6.1317e-03,  6.5413e-02, -2.6748e-02, -1.6659e-02,  9.2704e-03,
         1.0450e-02,  4.4999e-03,  1.0988e-02, -1.2671e-02, -4.1485e-02,
        -3.3754e-02,  1.4674e-03,  9.4934e-03,  2.1821e-02,  2.9225e-04,
         3.1063e-02,  1.2220e-02, -4.4459e-03,  1.0390e-02,  1.8050e-02,
        -2.3337e-02, -1.8241e-02,  8.2383e-03,  1.9018e-02,  2.0319e-02,
         1.3866e-02, -4.3113e-03, -3.8966e-02,  3.4529e-02, -5.3830e-03,
         1.3873e-02,  6.7009e-02, -1.2489e-02, -1.1384e-03, -2.8559e-03,
        -2.1934e-02, -1.2523e-03, -2.0337e-02,  1.1227e-02,  1.4287e-02,
         1.0162e-02,  9.7022e-03, -1.2539e-03,  4.3092e-03,  1.7934e-02,
         3.9632e-02,  1.1429e-02,  2.0224e-02, -3.7302e-04, -7.2591e-03,
        -1.1631e-02, -7.0199e-03, -4.1661e-03, -1.8633e-02, -9.8691e-04,
        -3.5259e-02,  2.7380e-02,  1.1591e-02, -3.4748e-02, -3.2195e-02,
        -1.1440e-02, -3.1906e-02, -1.8460e-02, -1.7123e-02,  3.7281e-02,
        -9.0607e-03, -4.3940e-02, -5.0359e-02,  1.0282e-02, -1.2650e-02,
         1.3500e-02, -1.5335e-02,  2.9392e-02, -1.1979e-02, -1.5790e-03,
        -7.6614e-03,  1.8989e-02, -1.8019e-02, -1.4556e-02, -9.5855e-03,
        -3.8630e-03, -1.3162e-02,  6.4234e-02, -5.3949e-03, -2.0051e-02,
        -8.4334e-03,  3.9684e-02, -1.9391e-03,  1.6300e-02, -1.8553e-02,
        -1.1218e-03,  1.3178e-03, -4.1242e-03,  8.2383e-03, -2.5318e-02,
         1.8986e-02,  1.9129e-02, -6.3024e-03,  1.9254e-02, -1.8060e-02,
         4.9744e-02, -2.9559e-03,  8.1625e-03,  1.4531e-02,  3.0216e-02,
        -1.0660e-02,  1.0798e-03,  2.0625e-02, -1.2890e-02, -3.2998e-03,
         4.4649e-03,  3.8908e-02])

Layer: encoder.5.0.block.1.queries_projection._packed_params._packed_params
Parameters: (tensor([[-0.0127,  0.1235, -0.0381,  ...,  0.0963,  0.0926, -0.0763],
        [ 0.0690,  0.0091,  0.0036,  ...,  0.0490,  0.0327, -0.0036],
        [ 0.0054, -0.1054,  0.0073,  ..., -0.0018, -0.0690, -0.0527],
        ...,
        [ 0.0000, -0.0509, -0.0091,  ...,  0.0036, -0.0291, -0.0091],
        [ 0.0036,  0.0400, -0.0527,  ...,  0.1090,  0.0945,  0.0036],
        [ 0.0272,  0.0763, -0.0472,  ..., -0.0781, -0.0563,  0.0345]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.001816588919609785,
       zero_point=0), Parameter containing:
tensor([ 0.0582,  0.0707,  0.0011,  0.0647, -0.0917,  0.0399,  0.0052, -0.0059,
        -0.0688, -0.0395, -0.0314, -0.0218,  0.0510,  0.0398, -0.0179, -0.0484,
        -0.0135,  0.0802, -0.0442, -0.0401, -0.0159, -0.0567,  0.0141,  0.0041,
        -0.0494, -0.0640, -0.0183, -0.0905,  0.0279,  0.0230, -0.0517, -0.0658,
        -0.0373,  0.0302, -0.0377,  0.0237, -0.0276, -0.0310, -0.0360, -0.0247,
         0.0537, -0.0241,  0.0141, -0.0379, -0.0322,  0.0285,  0.0116, -0.0420,
        -0.0296,  0.0333,  0.0656, -0.0004,  0.0389, -0.0396, -0.0684, -0.0640,
         0.0368,  0.0558, -0.0199,  0.0476, -0.0495,  0.0126,  0.0807, -0.0327,
        -0.0358, -0.0068, -0.0571, -0.0261,  0.0277, -0.0615,  0.0187,  0.0391,
         0.0090, -0.0197,  0.0586,  0.0081,  0.0174,  0.0683, -0.0480,  0.0533,
        -0.0506, -0.0203,  0.0148,  0.0406, -0.0121, -0.0147,  0.0695,  0.0118,
        -0.0530, -0.0549, -0.0076, -0.0553, -0.0524,  0.0011,  0.0578, -0.0095,
        -0.0728,  0.0364,  0.0924, -0.0075,  0.0206, -0.0147, -0.0668,  0.0003,
         0.0020, -0.0147, -0.0292, -0.0354,  0.0647, -0.0298,  0.0833, -0.0954,
        -0.0692, -0.0662, -0.0454, -0.0252,  0.0222, -0.0060,  0.0550, -0.0065,
        -0.0068, -0.0451,  0.0272, -0.0539, -0.0407, -0.0142, -0.0719, -0.0531,
        -0.0618, -0.0240,  0.0407, -0.0241, -0.0513, -0.0216,  0.0529,  0.0593,
        -0.0353,  0.0345,  0.0371, -0.0040,  0.0152,  0.0547,  0.0362,  0.0072,
        -0.0554,  0.0013,  0.0592, -0.0228, -0.0911,  0.0444, -0.0017,  0.0495,
        -0.0906,  0.0425,  0.0584,  0.0182, -0.0526,  0.0221,  0.0682,  0.0274,
        -0.0503,  0.0669, -0.0590, -0.0019, -0.0916,  0.0187, -0.0224, -0.0472,
        -0.0579, -0.0174, -0.0422,  0.0282, -0.0825,  0.0281, -0.0143,  0.0170,
        -0.0215, -0.0275,  0.0086, -0.0136,  0.0369, -0.1071, -0.0044, -0.0259,
         0.0055, -0.0438, -0.0769,  0.0614, -0.0036, -0.0522, -0.1011,  0.0177],
       requires_grad=True))

Layer: encoder.5.0.block.1.values_projection._packed_params._packed_params
Parameters: (tensor([[-0.0350,  0.0612, -0.0044,  ..., -0.0787, -0.0809, -0.0743],
        [-0.0809, -0.1311,  0.0634,  ..., -0.0415,  0.0874,  0.0066],
        [-0.0437, -0.0721, -0.0131,  ..., -0.0328,  0.1136, -0.0284],
        ...,
        [ 0.0022, -0.0524,  0.0197,  ...,  0.0087, -0.0197, -0.0437],
        [ 0.0852,  0.0044, -0.0656,  ...,  0.0131,  0.0830, -0.0546],
        [-0.0503,  0.0328, -0.0262,  ...,  0.0284,  0.0765, -0.0109]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0021851519122719765,
       zero_point=0), Parameter containing:
tensor([ 0.0380, -0.0108, -0.0142, -0.0630, -0.0421, -0.0502, -0.0521,  0.0352,
         0.0027,  0.0153,  0.0021,  0.0417, -0.0048, -0.0172,  0.0264, -0.0535,
         0.0556,  0.0261, -0.0549,  0.0075, -0.0627,  0.0046, -0.0313,  0.0085,
        -0.0700,  0.0666,  0.0362, -0.0340, -0.0200, -0.0169,  0.0176,  0.0124,
        -0.0085, -0.0239, -0.0457, -0.0653, -0.0414, -0.0438,  0.0235, -0.0342,
         0.0265,  0.0013, -0.0506,  0.0563, -0.0530,  0.0079, -0.0149,  0.0037,
         0.0195, -0.0548, -0.0048, -0.0277,  0.0005, -0.0225, -0.0505,  0.0220,
         0.0223, -0.0088, -0.0068,  0.0446, -0.0044,  0.0523, -0.0546, -0.0016,
         0.0599,  0.0487,  0.0629, -0.0444, -0.0544,  0.0725, -0.0466, -0.0460,
         0.0541,  0.0352,  0.0460,  0.0555,  0.0398,  0.0423,  0.0064,  0.0439,
        -0.0577,  0.0561, -0.0086,  0.0528,  0.0057,  0.0019, -0.0079,  0.0050,
         0.0296, -0.0218, -0.0513, -0.0554, -0.0377, -0.0017,  0.0507,  0.0138,
         0.0234,  0.0035,  0.0516,  0.0314, -0.0418, -0.0332, -0.0592,  0.0726,
        -0.0603, -0.0556, -0.0636, -0.0633,  0.0465,  0.0323, -0.0490,  0.0223,
        -0.0150, -0.0351, -0.0436, -0.0569,  0.0578,  0.0200,  0.0353,  0.0200,
         0.0717,  0.0489,  0.0577, -0.0262,  0.0249, -0.0295, -0.0192,  0.0106,
         0.0161, -0.0489, -0.0087,  0.0293,  0.0716,  0.0674,  0.0670, -0.0508,
         0.0573,  0.0653, -0.0616,  0.0365,  0.0111, -0.0501,  0.0367,  0.0471,
        -0.0242, -0.0042,  0.0459, -0.0484, -0.0347,  0.0391, -0.0364, -0.0146,
        -0.0646, -0.0206,  0.0265, -0.0485,  0.0067, -0.0627, -0.0298, -0.0246,
         0.0466, -0.0267, -0.0251, -0.0330,  0.0752,  0.0777,  0.0531, -0.0613,
        -0.0499, -0.0334, -0.0521, -0.0223,  0.0512, -0.0260, -0.0019, -0.0329,
        -0.0039, -0.0241,  0.0518, -0.0205, -0.0309,  0.0420,  0.0471,  0.0353,
        -0.0217,  0.0186,  0.0647,  0.0449, -0.0517,  0.0084,  0.0244, -0.0577],
       requires_grad=True))

Layer: encoder.5.0.block.1.keys_projection._packed_params._packed_params
Parameters: (tensor([[ 0.0052,  0.0643, -0.0782,  ..., -0.0504,  0.1061,  0.0191],
        [-0.1095, -0.0035,  0.0626,  ...,  0.0226,  0.0504,  0.0000],
        [-0.0504, -0.0852,  0.0800,  ...,  0.0661,  0.0000,  0.0383],
        ...,
        [ 0.0000, -0.0104,  0.0417,  ..., -0.0504, -0.0383,  0.0539],
        [ 0.0974, -0.0122, -0.0383,  ...,  0.0452,  0.0522, -0.0243],
        [ 0.0574,  0.0869,  0.0626,  ...,  0.0417,  0.0713,  0.0313]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0017387830885127187,
       zero_point=0), Parameter containing:
tensor([ 0.0100,  0.0060, -0.0585,  0.0557, -0.0274,  0.0588,  0.0534,  0.0555,
        -0.0557,  0.0240,  0.0659,  0.0448, -0.0073,  0.0233,  0.0191, -0.0189,
        -0.0125, -0.0468, -0.0526, -0.0213,  0.0272, -0.0198,  0.0296, -0.0176,
         0.0001, -0.0178, -0.0574,  0.0306, -0.0668,  0.0714,  0.0002, -0.0646,
         0.0095, -0.0468,  0.0599,  0.0439,  0.0612,  0.0372, -0.0149,  0.0397,
         0.0421, -0.0227, -0.0450, -0.0337, -0.0109,  0.0396, -0.0551,  0.0387,
         0.0143,  0.0100, -0.0707, -0.0422,  0.0537, -0.0062,  0.0727, -0.0175,
        -0.0317, -0.0637, -0.0191, -0.0307,  0.0165, -0.0136, -0.0430, -0.0406,
        -0.0182, -0.0052,  0.0112, -0.0487,  0.0055,  0.0393, -0.0131,  0.0625,
         0.0147, -0.0061, -0.0496, -0.0695, -0.0499, -0.0569, -0.0632, -0.0636,
        -0.0219, -0.0234,  0.0088,  0.0696,  0.0236,  0.0133,  0.0148,  0.0244,
         0.0345, -0.0573, -0.0399, -0.0423, -0.0237,  0.0098, -0.0676, -0.0419,
         0.0445, -0.0070,  0.0284, -0.0305,  0.0393, -0.0472,  0.0350,  0.0014,
         0.0623,  0.0331, -0.0604,  0.0659, -0.0143, -0.0720, -0.0726,  0.0683,
        -0.0590,  0.0308, -0.0242, -0.0654, -0.0198, -0.0149, -0.0389, -0.0013,
         0.0459,  0.0480,  0.0484,  0.0124,  0.0197, -0.0634, -0.0109, -0.0704,
        -0.0282, -0.0140, -0.0136,  0.0569,  0.0603,  0.0613,  0.0568, -0.0423,
         0.0698, -0.0500, -0.0394, -0.0300,  0.0678,  0.0067,  0.0234, -0.0106,
         0.0530, -0.0002,  0.0059, -0.0773,  0.0697,  0.0113, -0.0184,  0.0334,
        -0.0426, -0.0697, -0.0415,  0.0451, -0.0435,  0.0787,  0.0247,  0.0201,
        -0.0035, -0.0068,  0.0272, -0.0099, -0.0615,  0.0590, -0.0133,  0.0381,
         0.0696, -0.0613,  0.0309,  0.0403,  0.0014, -0.0501,  0.0620, -0.0065,
         0.0261,  0.0453, -0.0661, -0.0620, -0.0325,  0.0269,  0.0105, -0.0444,
         0.0197,  0.0077, -0.0619, -0.0488,  0.0200, -0.0446,  0.0664, -0.0463],
       requires_grad=True))

Layer: encoder.5.0.block.1.final_projection._packed_params._packed_params
Parameters: (tensor([[ 0.0099, -0.0123,  0.0444,  ..., -0.0863, -0.0518,  0.1528],
        [ 0.1553, -0.1208, -0.1158,  ...,  0.0518,  0.1282,  0.0887],
        [-0.0148, -0.0246,  0.0789,  ...,  0.0961,  0.0690,  0.0049],
        ...,
        [ 0.1503,  0.0320,  0.0838,  ...,  0.0025, -0.0813,  0.0986],
        [ 0.0345, -0.0863, -0.0025,  ...,  0.0690, -0.0173,  0.0542],
        [ 0.0641, -0.0592,  0.0296,  ..., -0.1060,  0.0641, -0.0025]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0024645987432450056,
       zero_point=0), Parameter containing:
tensor([-4.5061e-02,  5.1251e-02, -1.1845e-02,  7.5974e-02,  1.6666e-02,
        -1.9714e-02, -5.8005e-03,  3.7652e-02,  1.4640e-02, -6.2749e-02,
         6.4713e-03,  1.2876e-02, -2.0077e-02,  6.2732e-02,  1.6301e-02,
        -2.5795e-03, -2.6519e-02,  2.2310e-02, -4.3505e-02, -5.1138e-02,
        -3.5702e-02, -6.4713e-02,  3.2990e-02, -1.9963e-02, -2.8530e-02,
        -9.5469e-04,  6.7655e-02,  4.6615e-02,  6.4175e-02, -2.3270e-02,
        -2.0380e-02,  3.7938e-02,  4.5630e-02, -8.0157e-02,  3.3612e-02,
        -1.9522e-02,  2.5804e-02, -5.0788e-02,  1.8344e-02,  2.3505e-02,
        -4.1560e-02, -4.6803e-02,  2.4042e-02,  5.3411e-03,  5.8957e-02,
        -4.1081e-02,  3.7734e-04,  6.1145e-02, -8.2095e-02,  2.2657e-02,
         4.9341e-02,  4.9735e-02,  4.5169e-02,  5.6259e-02,  5.2708e-02,
        -1.4060e-05, -2.4693e-02, -9.4351e-03,  3.7444e-02,  7.0044e-02,
         5.2069e-02, -3.6270e-02,  4.7126e-02, -7.4498e-02,  5.6346e-02,
         2.6131e-02, -7.5128e-02, -4.9148e-02, -5.5425e-02,  1.6000e-03,
        -1.4877e-03, -3.2502e-02, -6.2955e-03,  4.3695e-02,  9.2821e-03,
         3.3934e-02, -4.1643e-02, -3.6633e-03,  1.1566e-03,  3.5893e-02,
        -2.3276e-02,  4.1729e-02,  1.7800e-02, -8.8203e-02, -6.7972e-02,
        -3.7766e-02, -6.4942e-02,  9.0076e-03, -1.2069e-02, -2.6159e-02,
        -4.0899e-02,  4.7624e-02, -6.8657e-02,  6.3156e-02, -1.8354e-02,
        -7.4172e-02, -5.7429e-02, -6.4990e-02,  9.0046e-02,  5.6068e-03,
         3.9520e-02,  8.5049e-02,  3.9121e-02, -2.1824e-02, -1.2761e-02,
         2.7298e-02, -6.8872e-02,  9.1534e-03, -4.2241e-02, -6.2870e-02,
        -3.1967e-02, -6.6741e-03,  6.0484e-02,  4.0468e-04, -6.0060e-02,
        -8.3399e-03, -7.0721e-02, -6.6144e-02,  4.3157e-02, -2.6243e-02,
         1.4249e-02, -2.8142e-02, -6.3604e-02, -3.9538e-02,  4.0668e-02,
         5.4459e-02, -4.7331e-02,  3.0552e-02, -4.8171e-02, -5.3202e-02,
        -2.6586e-02,  4.9577e-02, -4.0971e-02, -1.4920e-02,  2.1770e-02,
         3.9469e-02,  8.2398e-02, -1.8872e-02,  7.8470e-02, -3.3627e-02,
         9.8868e-03,  8.3949e-03, -4.2282e-02, -2.3824e-02,  4.7076e-02,
        -7.5572e-03,  5.8071e-02, -1.6971e-02, -4.3051e-02,  4.5807e-02,
         8.0403e-02,  6.0236e-02, -3.3519e-02,  6.4799e-02, -6.4273e-02,
        -4.1404e-02, -4.8378e-02,  6.6074e-02,  4.8741e-02,  6.7132e-02,
         4.5757e-03,  8.8207e-03, -5.1363e-02, -2.4550e-02,  5.6278e-03,
        -4.2449e-02, -3.5725e-02, -2.8504e-02, -3.9445e-02, -1.8112e-03,
         4.1035e-02,  1.4346e-02, -4.1535e-03,  3.1990e-03, -3.5299e-02,
         2.8565e-02, -7.4803e-02,  5.1403e-02, -4.5531e-02, -7.5480e-02,
        -6.6026e-02, -6.6536e-02, -1.2084e-02,  8.6877e-03,  2.9733e-02,
        -3.0511e-02,  2.6170e-03,  3.8643e-02, -8.2236e-02, -7.4078e-02,
         2.5381e-02, -5.1540e-02], requires_grad=True))

Layer: encoder.5.1.block.0.weight
Parameters: tensor([0.8664, 0.9132, 0.9032, 0.9271, 0.9020, 0.9413, 0.9585, 0.9339, 0.9020,
        0.9248, 0.8638, 0.9689, 0.9115, 0.8984, 0.9297, 0.9144, 0.8346, 0.8713,
        0.9432, 0.8894, 0.9235, 0.9130, 0.9011, 0.8911, 0.8200, 0.8763, 0.8905,
        0.8848, 0.8987, 0.8596, 0.9266, 0.8857, 0.9264, 0.8507, 0.8829, 0.9521,
        0.9357, 0.9031, 0.8799, 0.8703, 0.8927, 0.8524, 0.9122, 0.9331, 0.9448,
        0.8858, 0.8946, 0.8902, 0.9670, 0.8493, 0.8737, 0.9203, 0.9080, 0.9065,
        0.9281, 0.9128, 0.9391, 0.9202, 0.9260, 0.9437, 0.9063, 0.9039, 0.8987,
        0.9167, 0.9048, 0.8761, 0.9341, 0.9169, 0.9346, 0.8983, 1.0289, 0.9326,
        0.8718, 0.9185, 0.9172, 0.8949, 0.9005, 0.8274, 0.9436, 0.9162, 0.8792,
        0.9154, 0.8992, 0.9007, 0.9355, 0.9067, 0.9022, 0.9246, 0.9331, 0.8889,
        0.8484, 0.9155, 0.9009, 0.8668, 0.9483, 0.9378, 0.8251, 0.9083, 0.8894,
        0.8763, 0.8766, 0.9206, 0.9368, 0.9215, 0.9547, 0.9055, 0.9199, 0.9558,
        0.9304, 0.8626, 0.8949, 0.9278, 0.8908, 0.9164, 0.8805, 0.9210, 0.9259,
        0.9149, 0.9556, 0.9071, 0.8401, 0.9187, 0.9245, 0.9060, 0.8543, 0.8859,
        0.9084, 0.8214, 0.8903, 0.9322, 0.9181, 0.9280, 0.8711, 0.8934, 0.9592,
        0.8819, 0.8914, 0.8876, 0.9338, 0.8592, 0.9110, 0.8602, 0.9126, 0.8957,
        0.8644, 0.9632, 0.8984, 0.8686, 0.9456, 0.9124, 0.8891, 0.8787, 0.8484,
        0.8967, 0.9004, 0.8997, 0.9228, 0.8950, 0.8711, 0.9120, 0.9054, 0.8928,
        0.9133, 0.8336, 0.9238, 0.9033, 0.9115, 0.9243, 0.9398, 0.8959, 0.9478,
        0.9230, 0.9206, 0.8960, 0.8733, 0.8717, 0.9129, 0.9338, 0.9034, 0.8852,
        0.9319, 0.8701, 0.9205, 0.8776, 0.9065, 0.9195, 0.9312, 0.8910, 0.9169,
        0.8725, 0.8933, 0.9239])

Layer: encoder.5.1.block.0.bias
Parameters: tensor([-0.0007, -0.0246, -0.0409,  0.0090, -0.0361,  0.0551, -0.0173,  0.0387,
        -0.0111,  0.0249, -0.0319,  0.0333, -0.0365, -0.0024,  0.0163, -0.0037,
        -0.0197, -0.0077,  0.0500,  0.0438, -0.0317,  0.0276, -0.0085,  0.0043,
        -0.0276,  0.0143, -0.0283,  0.0176,  0.0331,  0.0201, -0.0181,  0.0264,
         0.0234, -0.0026,  0.0011,  0.0444, -0.0159, -0.0566, -0.0203, -0.0121,
        -0.0200, -0.0132,  0.0164,  0.0321, -0.0249,  0.0073,  0.0169,  0.0046,
         0.0359,  0.0331, -0.0349,  0.0156,  0.0324, -0.0090,  0.0065,  0.0046,
         0.0180, -0.0412, -0.0098, -0.0033, -0.0171, -0.0016, -0.0725,  0.0182,
        -0.0171,  0.0414, -0.0313, -0.0148, -0.0498, -0.0244, -0.0504, -0.0284,
        -0.0131, -0.0131,  0.0206,  0.0276, -0.0212,  0.0004, -0.0201,  0.0135,
        -0.0166, -0.0255,  0.0097,  0.0461, -0.0426, -0.0126, -0.0004, -0.0171,
         0.0165, -0.0215, -0.0412,  0.0017, -0.0521,  0.0256, -0.0181,  0.0156,
         0.0146, -0.0246, -0.0294, -0.0254, -0.0054, -0.0056, -0.0305, -0.0023,
         0.0293,  0.0537,  0.0406,  0.0021,  0.0082, -0.0182,  0.0180, -0.0236,
        -0.0015, -0.0462, -0.0003, -0.0170,  0.0337,  0.0386, -0.0290,  0.0458,
         0.0080,  0.0138, -0.0399,  0.0207, -0.0432, -0.0384,  0.0279, -0.0095,
        -0.0323, -0.0412,  0.0125, -0.0600, -0.0145, -0.0169,  0.0379, -0.0247,
         0.0212,  0.0252, -0.0274,  0.0081, -0.0254, -0.0169, -0.0076,  0.0498,
        -0.0179,  0.0005,  0.0179,  0.0270, -0.0202, -0.0221, -0.0142,  0.0260,
        -0.0124,  0.0103, -0.0088,  0.0264, -0.0157, -0.0226,  0.0199, -0.0444,
         0.0043,  0.0267, -0.0099, -0.0347, -0.0300, -0.0029,  0.0006, -0.0304,
        -0.0180,  0.0052, -0.0171,  0.0075, -0.0289, -0.0082, -0.0144,  0.0094,
        -0.0122, -0.0154,  0.0288,  0.0190, -0.0190, -0.0053, -0.0158,  0.0271,
         0.0177, -0.0563, -0.0251, -0.0088,  0.0239,  0.0202, -0.0010, -0.0292])

Layer: encoder.5.1.block.1.0._packed_params._packed_params
Parameters: (tensor([[ 0.0521,  0.0118,  0.0370,  ...,  0.0303, -0.0437, -0.0437],
        [-0.0219, -0.0471, -0.0084,  ..., -0.0421, -0.0252, -0.0656],
        [-0.0471,  0.0454,  0.0454,  ...,  0.0168, -0.0017,  0.0000],
        ...,
        [ 0.0505,  0.0101,  0.0622,  ..., -0.0219,  0.0101,  0.0202],
        [ 0.0067, -0.0084,  0.0168,  ...,  0.0320,  0.0067,  0.0421],
        [ 0.0454, -0.0034, -0.0404,  ...,  0.0421,  0.0404,  0.0067]],
       size=(768, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0016822362085804343,
       zero_point=0), Parameter containing:
tensor([-7.5273e-02,  2.1969e-03,  1.5681e-02, -1.2614e-02, -9.9094e-02,
        -9.5546e-02, -9.2263e-02, -7.6591e-03, -2.5358e-02, -5.8678e-02,
        -9.2994e-02,  1.9307e-02, -6.7101e-02, -4.5706e-02,  2.1965e-03,
        -5.8222e-02,  3.2780e-02, -5.7394e-03, -6.5517e-02,  1.9012e-02,
        -4.1441e-02, -1.1813e-02, -4.8770e-02, -6.6007e-02,  2.2149e-02,
        -7.9240e-02, -7.1180e-03,  1.1682e-02, -6.9622e-02,  1.7131e-02,
         2.5554e-02,  2.2141e-02, -8.9963e-02, -8.8228e-02,  2.2565e-02,
        -8.4027e-02, -2.0846e-02,  5.8467e-03,  2.0494e-02, -7.1039e-02,
        -7.1811e-02, -7.5521e-02, -1.1504e-02,  7.9199e-03, -2.7444e-02,
        -5.7015e-02, -3.2155e-02, -6.5690e-02, -6.8802e-02,  3.0033e-03,
        -9.2280e-02, -3.8041e-02, -2.7101e-02,  2.2402e-02, -8.0670e-02,
        -2.3697e-02, -6.8790e-02, -3.2319e-02, -2.4608e-02, -5.6097e-02,
        -6.1554e-02,  1.1284e-02, -1.8062e-02, -4.7506e-02, -3.3227e-02,
        -1.0689e-02, -6.0401e-04, -6.5150e-02, -3.9287e-02,  1.2919e-02,
        -3.1267e-02, -3.0920e-02, -3.7415e-02, -5.6721e-02, -2.5102e-03,
         3.6817e-02, -7.2550e-02, -5.5144e-02, -1.0125e-01, -2.4640e-02,
         1.6587e-03, -7.3827e-02, -7.6223e-02, -8.0052e-02, -4.4292e-02,
         2.0247e-02, -9.9124e-02,  5.9252e-03, -7.7978e-02, -5.6407e-02,
         2.9535e-04,  5.6253e-03, -3.6935e-02, -8.8731e-03,  1.0523e-03,
        -5.5635e-02, -1.7634e-02, -6.6490e-02, -5.5912e-02, -6.4341e-02,
        -1.7246e-02, -1.0494e-01,  1.8709e-02, -9.4136e-02, -6.9043e-02,
        -7.2224e-02,  1.6002e-02,  2.1472e-02, -6.1365e-03, -9.4545e-03,
         5.9753e-03, -1.9067e-02,  2.3332e-02, -9.4311e-02, -9.0934e-02,
        -2.0579e-02, -5.0694e-02, -6.5790e-02,  4.7152e-03, -8.5416e-02,
        -9.6145e-02, -1.9999e-02, -6.0582e-02, -3.5314e-02,  3.3473e-02,
        -4.4727e-02, -8.5445e-02, -3.5321e-02, -2.9837e-02, -5.5029e-02,
        -3.2742e-03, -6.1126e-02,  1.5140e-02, -1.5697e-02, -7.3077e-03,
         2.7220e-05, -5.4408e-03, -3.1655e-02, -4.4516e-03,  1.6767e-02,
         1.6826e-02,  8.8546e-03,  1.8173e-02, -2.9608e-02, -6.3672e-02,
         6.9730e-03,  3.7846e-02, -3.8881e-02, -5.9800e-02, -7.6337e-02,
         1.6895e-02,  1.8361e-02, -6.2117e-02, -7.4096e-02, -9.0446e-02,
        -7.2824e-02, -9.1070e-02, -7.7741e-02, -5.8224e-03, -3.7469e-02,
        -8.3841e-02, -3.2957e-02, -7.7321e-02, -5.5499e-02, -6.8794e-02,
        -3.8349e-02,  5.8929e-03, -2.9099e-02, -7.3224e-02, -9.9300e-02,
        -6.2219e-02, -3.7641e-02,  1.3508e-02, -2.2693e-02, -6.7089e-02,
         3.0177e-02, -3.2381e-02, -6.2014e-02, -9.4993e-02, -7.9255e-02,
        -6.5416e-02, -2.2369e-02, -2.7315e-02, -7.0921e-02, -9.8040e-02,
         3.9590e-02, -1.0123e-01, -1.9491e-02, -3.9264e-02, -6.3226e-02,
        -7.3552e-02, -5.1780e-02, -7.3337e-02, -5.4616e-02,  2.6112e-03,
        -5.7504e-02,  1.5847e-02, -7.8714e-02, -3.2474e-02,  5.6569e-03,
        -6.2572e-02, -6.3553e-03, -3.7644e-02, -2.3186e-02, -2.5969e-02,
        -2.3465e-02, -9.3191e-02, -7.1575e-02,  3.1439e-02, -5.1204e-02,
         3.4706e-02, -8.8529e-02,  1.4649e-03, -4.6232e-02, -8.6847e-02,
        -1.0338e-02, -6.9047e-02, -3.1157e-02, -1.1405e-01, -6.8622e-02,
         5.0000e-03, -3.8358e-02, -8.1269e-02, -3.7526e-02, -6.6731e-03,
        -2.2232e-02, -1.6795e-02, -4.6108e-02, -3.7118e-02,  8.8166e-03,
        -1.0577e-02,  2.6856e-03,  7.5551e-03, -4.3342e-02, -7.4211e-02,
        -4.6577e-02, -1.0042e-04, -6.9901e-02,  1.6724e-02, -1.8708e-02,
        -2.2142e-02, -3.6241e-02, -5.8823e-02, -4.2788e-02,  8.6207e-03,
        -7.2043e-02, -7.1012e-02, -7.9333e-02,  9.6325e-03, -2.5775e-02,
        -3.3288e-02, -9.4188e-02, -8.3921e-02,  1.4128e-02, -4.5382e-03,
        -8.4759e-02, -5.9739e-02, -5.1221e-03, -2.1276e-02, -3.1634e-02,
        -7.6898e-02, -8.5085e-02, -2.6775e-02, -5.9442e-03, -7.2686e-02,
        -7.3951e-02, -7.9569e-02, -1.0357e-01, -4.1421e-02,  5.7900e-03,
        -3.5938e-02, -5.9600e-02,  2.8221e-02, -3.7110e-02, -1.6874e-02,
        -2.3449e-02, -1.7805e-02, -8.8585e-02, -6.2456e-02,  2.0561e-02,
        -2.1674e-02, -4.2422e-02, -5.9915e-02, -7.3668e-02,  4.0330e-02,
        -4.5122e-02, -8.4833e-02,  2.2222e-03, -7.7877e-02, -1.9417e-02,
        -4.9675e-02, -3.7234e-02, -1.9261e-02, -7.4424e-02, -5.7251e-02,
        -3.2098e-02, -5.9382e-02, -5.6509e-02, -1.5408e-03, -3.8495e-02,
        -7.4348e-02,  1.2062e-02, -8.5157e-02, -8.1691e-02, -1.1351e-02,
        -3.3255e-02,  3.4215e-03, -1.0359e-01, -2.3898e-02, -3.5385e-02,
        -5.6594e-02,  1.9431e-02,  1.7448e-02, -9.0452e-02, -8.3701e-02,
        -7.6364e-02, -7.9378e-02, -8.9005e-02, -6.4629e-03,  2.1270e-02,
         2.2129e-02, -1.1681e-02, -4.8220e-02, -7.6342e-02,  2.1056e-02,
        -1.2928e-02, -4.7298e-02, -9.3712e-02, -1.2980e-02, -6.8749e-02,
        -7.4581e-02,  9.5454e-03, -8.9351e-02, -2.6625e-02, -1.8846e-02,
        -8.5876e-02, -3.7065e-03, -1.4385e-02, -1.5910e-02,  8.0301e-03,
         1.1286e-02, -4.1226e-02,  1.7232e-02, -6.6699e-02, -8.8666e-02,
         9.9352e-03, -5.2179e-02, -4.2816e-02,  3.6219e-03, -1.0627e-02,
        -3.5488e-02, -6.8316e-02, -7.9599e-02, -5.8384e-02, -7.7101e-02,
         1.0207e-02, -3.9919e-02, -9.5023e-02, -3.7564e-02, -8.2562e-02,
         3.0016e-02, -1.4009e-02, -7.1959e-02, -9.6069e-02, -5.5702e-02,
        -1.9287e-02,  1.5178e-02, -9.1620e-02, -8.4949e-02, -3.7897e-03,
         1.9847e-02,  2.8177e-02, -7.0446e-02, -5.8803e-02, -8.7543e-02,
         2.8733e-02, -3.1947e-02, -9.3838e-02, -8.3912e-02,  1.8046e-02,
        -3.4819e-02,  1.2558e-02, -3.3425e-02, -2.7239e-02, -2.0824e-02,
        -3.2259e-03, -3.6515e-02, -6.6467e-02, -7.7862e-02, -3.7190e-02,
        -4.6287e-02, -5.5136e-02, -1.6311e-02, -6.5654e-02, -7.5602e-02,
        -1.4266e-02, -7.8497e-02,  3.1333e-02, -3.4734e-02, -4.6981e-02,
         1.1504e-02, -8.4304e-02, -7.4396e-02, -1.2768e-02, -6.2659e-02,
        -1.5614e-02, -2.0581e-02,  1.7643e-02,  1.3549e-02, -3.7552e-02,
         6.1400e-03, -7.9569e-02,  1.3783e-02, -5.1187e-02, -1.8408e-02,
         1.5868e-02,  5.1591e-03, -9.0166e-02,  8.6872e-03, -5.9512e-02,
         9.8207e-03, -8.4295e-02, -2.8748e-02, -2.5840e-02, -1.0440e-01,
         1.8464e-02, -1.9211e-03, -2.1118e-02, -2.7304e-02, -4.3135e-02,
        -6.6529e-02, -1.7666e-02, -4.2037e-02, -9.2070e-02, -3.7492e-02,
        -6.7336e-02, -5.2622e-02, -9.8299e-02,  1.9010e-02, -5.9456e-04,
        -9.5171e-02, -1.1318e-02, -3.2391e-02, -4.4198e-02, -1.8884e-03,
        -1.8613e-02, -1.8301e-02,  4.5882e-03,  1.7651e-02, -5.0049e-03,
        -5.6720e-02, -6.8346e-02, -4.2550e-02, -3.8968e-02,  2.1325e-02,
        -9.4584e-02, -7.6213e-02, -1.9948e-02, -4.7644e-02, -4.8774e-02,
        -5.1957e-02, -4.8286e-02, -3.7886e-02, -4.0973e-02, -5.9429e-02,
        -6.6302e-02, -5.8979e-02, -9.1107e-04, -6.9891e-03, -7.0226e-02,
        -5.0242e-02, -3.3157e-02, -8.3389e-02,  1.8273e-03, -5.9765e-02,
        -8.0799e-02, -5.9266e-02, -8.8152e-02,  1.9844e-02, -1.0592e-01,
        -5.7567e-02, -3.2359e-02, -8.0581e-02, -4.0993e-02,  2.4659e-02,
        -5.3814e-02, -2.5083e-02, -2.8987e-02, -5.7027e-02, -9.1973e-02,
         7.4972e-04, -4.1167e-02,  1.4088e-02,  1.5311e-02,  1.3104e-02,
        -6.2993e-03,  4.4924e-03, -4.6321e-02, -7.3178e-02, -7.4230e-02,
         9.5389e-03, -3.5206e-02,  1.4144e-04,  2.4928e-02, -4.7250e-02,
        -3.4209e-02,  4.7381e-03,  3.3613e-02,  2.5784e-02, -9.4365e-02,
        -8.2862e-02, -5.6197e-02, -5.5195e-02, -2.1514e-03, -7.7762e-02,
         1.3884e-02, -7.6272e-02, -1.1327e-02, -2.7909e-02,  2.7420e-02,
        -9.0081e-04, -3.5084e-02,  2.5113e-03, -6.3317e-02, -3.2889e-02,
        -7.9498e-02, -6.3344e-02, -1.9533e-02, -3.2041e-02, -6.2791e-02,
        -4.2669e-02, -9.8151e-03, -8.6917e-02, -1.6984e-02, -1.0023e-01,
        -8.8397e-03, -1.1450e-02, -7.8016e-02,  1.7159e-02, -4.4351e-02,
        -1.4912e-02,  1.5307e-02, -8.7432e-02, -7.6006e-02, -9.6575e-02,
        -8.5194e-02, -1.2253e-02, -2.8893e-02, -8.0717e-02, -8.5983e-02,
        -4.6671e-02, -1.5415e-02, -6.5742e-02, -7.7458e-03,  2.9534e-02,
        -6.8638e-02, -8.8075e-03, -6.5765e-02,  3.8432e-02, -4.2238e-02,
        -8.4214e-02, -6.8152e-02, -5.5611e-02, -7.7042e-02, -1.0205e-01,
        -6.9490e-02, -4.5934e-02, -6.3971e-02, -1.7483e-02, -6.7521e-02,
        -4.2439e-02, -8.8778e-02, -7.5379e-02, -4.1557e-03,  1.1931e-03,
        -9.2802e-02,  2.9330e-02, -8.8776e-02, -1.0348e-01, -1.2519e-04,
        -3.0262e-02, -1.3656e-02, -1.4370e-02,  1.1003e-02, -2.8521e-02,
        -6.9348e-02,  2.1687e-03, -4.5562e-02, -4.0739e-02,  1.8276e-02,
         3.5118e-03, -2.7221e-02, -8.0159e-02, -1.5795e-02, -9.0414e-02,
        -4.1760e-02,  9.7560e-03, -6.1976e-02, -6.4137e-03, -4.1516e-02,
        -9.9951e-02,  1.4806e-02, -5.5339e-02, -2.9150e-02, -9.8112e-03,
        -2.2249e-02, -1.0328e-01, -7.6895e-02, -4.6523e-02, -9.2082e-02,
         2.2444e-03, -5.4456e-02, -6.7668e-02, -3.2903e-03, -9.1050e-02,
        -1.7218e-03, -6.1574e-02, -7.6300e-02, -6.1130e-02, -9.8488e-02,
        -6.2076e-02,  4.5362e-03,  1.3000e-02, -1.1495e-02, -8.0207e-02,
        -7.9763e-02, -6.8670e-02, -3.6732e-02, -3.3032e-02, -1.1530e-02,
        -4.0174e-02,  1.3369e-02,  1.7667e-03, -2.6496e-02,  1.2834e-02,
        -3.2600e-02,  1.2203e-03, -1.1478e-02, -5.4078e-02, -2.2758e-02,
        -3.5040e-02, -9.4418e-02, -6.4705e-02, -4.8470e-02, -6.1518e-02,
        -1.0691e-03,  4.2141e-03,  1.6444e-02, -3.0545e-02, -4.7345e-02,
        -6.4976e-02, -5.6365e-02,  3.8226e-02,  5.1514e-04, -9.9817e-02,
        -1.1002e-02, -9.8065e-02, -8.4865e-02, -2.1874e-02, -9.5491e-02,
        -1.1358e-02, -8.1132e-02, -4.7264e-02, -3.4026e-02, -1.1410e-02,
         3.4815e-02, -7.7793e-02, -3.1056e-02, -4.1297e-02,  1.1271e-02,
         3.5576e-03, -3.4168e-02, -4.4711e-02, -7.7059e-02, -8.7050e-02,
        -6.9947e-03, -1.2279e-03, -4.0308e-02, -3.5092e-02, -9.7306e-03,
        -4.9754e-02, -6.9644e-02,  1.0922e-02,  2.0461e-02, -8.2730e-02,
        -2.4026e-02,  3.6188e-03,  7.4406e-03, -5.3545e-03, -4.6133e-02,
        -2.4448e-02, -4.6616e-02,  1.3700e-02,  1.4132e-04, -9.0126e-02,
        -3.6408e-02,  6.2613e-03, -5.7360e-02, -4.8201e-02, -1.0167e-02,
        -4.6631e-02,  8.9657e-03, -3.3836e-02,  1.2043e-02, -7.5535e-02,
         1.0523e-03,  2.9882e-03, -6.8553e-02, -5.3306e-03, -6.2597e-02,
         2.9147e-03, -7.7687e-02,  8.4316e-03, -8.7837e-02, -3.6052e-02,
        -6.1484e-02, -3.9371e-02, -5.6772e-02,  1.6694e-02, -7.3766e-02,
        -6.8216e-02, -1.7216e-02, -9.1906e-02, -1.4677e-02, -5.8619e-02,
        -2.5046e-02, -7.2140e-02, -2.1852e-02,  1.9906e-02, -2.6993e-02,
         7.3274e-04, -1.0010e-01, -4.3407e-02, -5.1010e-02, -1.5104e-02,
        -4.4110e-02,  2.2351e-02, -5.4132e-02,  1.6115e-02, -8.0469e-02,
        -8.7338e-02, -8.1021e-02, -4.3518e-02, -9.5842e-02, -6.7150e-02,
         2.0641e-02, -4.5420e-02, -5.5775e-02, -8.8555e-02, -8.9499e-02,
        -7.2043e-02, -2.4797e-02,  3.2445e-03, -2.8026e-02, -6.9087e-02,
        -1.0108e-02,  1.9592e-02, -8.0607e-02, -6.1777e-02, -9.4543e-02,
        -1.9409e-02,  2.3438e-02, -1.0797e-01,  2.0545e-02, -8.8642e-02,
        -4.1047e-02,  3.7210e-03, -7.4536e-02], requires_grad=True))

Layer: encoder.5.1.block.1.2._packed_params._packed_params
Parameters: (tensor([[-0.0050, -0.0288,  0.0070,  ...,  0.0298, -0.0159,  0.0577],
        [ 0.0040, -0.0109, -0.0209,  ..., -0.0219,  0.0249, -0.0298],
        [ 0.0010, -0.0139, -0.0239,  ...,  0.0010,  0.0348,  0.0199],
        ...,
        [ 0.0537,  0.0318,  0.0139,  ..., -0.0586, -0.0189,  0.0328],
        [-0.0408,  0.0119, -0.0060,  ..., -0.0159, -0.0308,  0.0000],
        [-0.0040,  0.0070, -0.0199,  ..., -0.0239, -0.0219,  0.0467]],
       size=(192, 768), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0009940473828464746,
       zero_point=0), Parameter containing:
tensor([-0.0194,  0.0254, -0.0203, -0.0199, -0.0243, -0.0069, -0.0136, -0.0582,
        -0.0227, -0.0327,  0.0329, -0.0464, -0.0392, -0.0214, -0.0446,  0.0046,
         0.0055, -0.0065, -0.0337, -0.0248, -0.0126, -0.0394,  0.0045,  0.0403,
        -0.0639, -0.0018, -0.0183, -0.0061, -0.0215,  0.0451,  0.0399,  0.0129,
        -0.0182, -0.0239,  0.0075, -0.0229, -0.0035,  0.0238, -0.0531,  0.0512,
        -0.0174,  0.0413,  0.0022, -0.0223,  0.0430,  0.0171, -0.0421,  0.0237,
        -0.0243,  0.0053,  0.0091, -0.0041,  0.0167,  0.0076,  0.0212,  0.0257,
        -0.0699, -0.0058,  0.0272,  0.0222,  0.0428,  0.0187,  0.0016, -0.0123,
        -0.0100,  0.0262, -0.0195, -0.0395, -0.0008, -0.0043,  0.0463,  0.0122,
        -0.0086, -0.0116, -0.0118,  0.0236, -0.0223,  0.0123,  0.0279, -0.0174,
         0.0072,  0.0207, -0.0615, -0.0288,  0.0168, -0.0034, -0.0621, -0.0405,
         0.0206, -0.0458, -0.0027, -0.0388, -0.0273, -0.0041, -0.0092, -0.0042,
        -0.0038,  0.0133,  0.0277,  0.0044,  0.0326,  0.0070,  0.0408, -0.0145,
        -0.0492, -0.0221,  0.0071, -0.0129, -0.0378, -0.0335, -0.0141,  0.0194,
        -0.0021, -0.0069,  0.0173,  0.0424, -0.0395,  0.0204,  0.0165,  0.0217,
         0.0277,  0.0295,  0.0303,  0.0202, -0.0424, -0.0124, -0.0359, -0.0174,
        -0.0499, -0.0027, -0.0631,  0.0212, -0.0103, -0.0139,  0.0021, -0.0471,
         0.0441,  0.0053,  0.0120,  0.0124, -0.0102,  0.0196,  0.0304,  0.0376,
         0.0525, -0.0325, -0.0170, -0.0150, -0.0470, -0.0126,  0.0125, -0.0052,
        -0.0143, -0.0201, -0.0107,  0.0307, -0.0320,  0.0295,  0.0058,  0.0091,
        -0.0508,  0.0039, -0.0049,  0.0529,  0.0545,  0.0322,  0.0277, -0.0391,
        -0.0028, -0.0307, -0.0272, -0.0154,  0.0271,  0.0086, -0.0351, -0.0430,
        -0.0418,  0.0200, -0.0594,  0.0014,  0.0111, -0.0702,  0.0179,  0.0076,
         0.0196,  0.0031, -0.0122, -0.0044, -0.0413, -0.0239, -0.0300,  0.0178],
       requires_grad=True))

