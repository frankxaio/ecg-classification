Layer: encoder.2.0.block.0.weight
Parameters: tensor([1.0683, 1.1038, 1.0350, 1.0554, 1.0545, 1.0968, 1.0435, 1.0464, 1.0424,
        1.0330, 1.0687, 1.0465, 1.0285, 1.0409, 1.0376, 1.0527, 1.0340, 1.0383,
        1.0693, 1.0516, 1.0870, 1.0546, 1.0515, 1.0133, 1.0666, 1.0618, 1.0167,
        1.0883, 1.0738, 1.0407, 1.0547, 1.0548, 1.0306, 1.0189, 1.0361, 1.0457,
        1.0166, 1.0672, 1.0638, 1.0604, 1.0296, 1.0497, 1.0919, 1.0701, 1.0854,
        1.0413, 1.0159, 1.0239, 1.0166, 1.0809, 1.0415, 1.0555, 1.0513, 1.0283,
        1.0021, 1.0298, 1.0211, 1.0502, 1.0596, 1.0428, 1.0692, 1.0472, 1.0645,
        1.0871, 1.0801, 1.0582, 1.0663, 1.0357, 1.0764, 1.0415, 1.0337, 1.0298,
        1.0500, 1.0724, 1.1178, 1.0711, 1.0815, 1.0112, 1.0684, 1.0699, 1.0131,
        1.0372, 1.0205, 0.9901, 1.0182, 1.0343, 1.0692, 1.0715, 1.0384, 1.0586,
        1.0698, 1.0338, 1.0445, 1.0453, 1.0465, 1.0413, 1.0643, 1.0003, 1.0596,
        1.0190, 1.0605, 1.0160, 0.9936, 1.0770, 1.0673, 1.0238, 1.0453, 1.0252,
        1.0623, 1.0521, 1.0450, 1.0459, 1.1072, 1.0517, 1.0281, 1.0441, 1.0597,
        1.0749, 1.0324, 1.0577, 1.1065, 1.0629, 1.0858, 1.0406, 1.0875, 1.0768,
        1.0579, 1.0378, 1.0388, 1.0467, 1.0440, 1.0442, 1.0870, 1.0864, 1.0348,
        1.0399, 1.0827, 1.0150, 1.0478, 1.0101, 1.0656, 1.0410, 1.0152, 1.0353,
        1.0281, 1.0667, 1.0512, 1.0563, 1.0402, 1.0478, 1.0174, 1.0094, 1.0656,
        1.0313, 1.0459, 1.0306, 1.0728, 1.0465, 1.0714, 1.0757, 1.0682, 1.0778,
        1.0566, 1.0521, 1.0196, 1.0586, 1.0045, 1.0909, 1.0369, 1.0134, 0.9898,
        1.0396, 1.0366, 1.0565, 1.0528, 1.0106, 1.0321, 1.0342, 1.1135, 0.9966,
        1.0753, 1.0765, 1.0382, 1.0453, 1.0385, 1.0471, 1.0371, 1.0347, 1.0597,
        1.0563, 1.0756, 1.0350])

Layer: encoder.2.0.block.0.bias
Parameters: tensor([-3.6446e-03, -6.5881e-03, -1.1986e-02, -2.4973e-03, -9.9924e-03,
        -1.4291e-02,  1.7567e-02, -1.6188e-02,  3.1986e-03,  1.4056e-02,
        -5.9809e-03, -1.7776e-02, -1.0477e-03, -2.1115e-02, -6.9671e-04,
        -2.7420e-03, -1.9178e-02, -6.8525e-03,  1.4848e-02,  1.5895e-02,
         1.1005e-02,  5.6878e-03, -6.9563e-03,  5.9341e-03, -1.0367e-02,
        -1.3444e-02,  3.8690e-03, -3.0317e-02,  1.3401e-02,  5.8541e-04,
         1.1199e-02,  8.0511e-03,  2.4858e-03, -1.8745e-02,  2.1255e-02,
         5.9271e-03,  1.1017e-02,  1.4185e-03,  1.5176e-03,  1.5505e-02,
        -8.5912e-03, -1.6486e-02,  2.2144e-02,  5.1069e-03, -3.1707e-02,
         2.0572e-02, -2.2695e-02,  1.0978e-02, -6.9593e-03,  2.7257e-03,
         2.3423e-02,  1.3376e-02,  9.1066e-03, -1.3712e-03, -1.5862e-02,
         2.0420e-02,  2.2255e-03, -7.1601e-03,  7.5766e-04,  1.2090e-02,
         9.9160e-03, -9.5357e-03, -2.3562e-03, -5.7854e-03, -1.1888e-03,
        -1.3136e-02,  1.2679e-02,  1.9396e-03,  2.3740e-02, -3.2642e-03,
        -1.4836e-02, -6.9739e-03, -1.7619e-02, -1.8351e-02,  2.7037e-03,
         2.7729e-03, -7.6364e-03, -9.2561e-03, -1.8954e-02,  8.0507e-03,
         1.1746e-02,  1.1925e-02, -3.1888e-02, -7.4455e-03,  9.7302e-03,
        -1.3232e-02, -1.4125e-02,  2.1757e-02, -1.0573e-02,  3.5075e-03,
        -3.2732e-02, -7.2386e-03,  2.0204e-02, -1.0977e-02,  1.5115e-02,
         2.5450e-03, -4.1520e-04, -1.0603e-02,  6.1987e-03, -1.3471e-02,
        -2.6834e-02, -1.6710e-02,  1.6658e-03, -1.3414e-02,  1.3979e-02,
         2.4177e-02,  6.1661e-03, -7.4873e-03,  1.7036e-02, -7.8963e-03,
        -7.3729e-04,  2.2395e-02, -1.7012e-03,  1.1685e-02,  2.2202e-02,
        -2.0007e-02,  1.0618e-02, -1.7621e-02,  6.3320e-03,  2.0332e-02,
        -1.7218e-02, -5.4499e-03, -1.0615e-02,  6.4871e-03, -2.2855e-02,
         5.4921e-03,  5.4272e-03, -2.9475e-02, -2.2723e-02, -2.8847e-03,
        -9.6929e-03, -5.8019e-03, -5.4211e-03, -9.9635e-03,  1.9887e-02,
        -1.0792e-02, -1.3309e-02, -9.6228e-03, -8.7096e-03, -1.8253e-02,
         4.4729e-03, -5.8649e-03,  1.6258e-02, -1.6976e-02, -1.8517e-03,
        -4.0630e-02, -8.2244e-03,  1.8420e-02, -4.0826e-03, -6.9102e-03,
        -2.2173e-02, -6.3609e-03,  2.6126e-02, -9.1581e-03,  2.0242e-03,
        -1.8304e-03, -1.4936e-02, -2.7712e-02, -2.2822e-03, -2.7136e-03,
        -6.8565e-03, -9.4793e-03,  5.0700e-03, -1.9538e-02, -2.4476e-02,
         1.3336e-02, -1.2966e-02,  1.7717e-03,  2.4539e-02, -3.3361e-04,
         2.6973e-02, -6.5365e-04,  1.4755e-02,  1.8234e-03,  7.7648e-03,
         7.7296e-03, -1.3725e-02,  4.2852e-03,  2.4533e-03,  1.0880e-02,
         5.0256e-03,  2.1404e-02,  2.1154e-03, -1.7917e-02, -1.5392e-02,
         2.0912e-02, -8.4101e-04, -3.7363e-02,  9.3017e-03, -5.0554e-03,
         6.3814e-05,  1.8658e-02])

Layer: encoder.2.0.block.1.queries_projection._packed_params._packed_params
Parameters: (tensor([[ 0.0659,  0.0557, -0.0203,  ...,  0.0051,  0.0135,  0.0524],
        [ 0.0186,  0.0845,  0.0490,  ..., -0.0372, -0.0608,  0.0405],
        [-0.1199,  0.0946,  0.0422,  ..., -0.0946,  0.0507, -0.1233],
        ...,
        [ 0.0726, -0.0084,  0.0186,  ..., -0.0203, -0.0287,  0.0000],
        [ 0.0456, -0.0051,  0.0608,  ..., -0.1216, -0.0338,  0.0135],
        [ 0.0051, -0.0338,  0.0439,  ...,  0.0473, -0.0203,  0.0203]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0016892320709303021,
       zero_point=0), Parameter containing:
tensor([-0.0474, -0.0591,  0.0354,  0.0356,  0.0047, -0.0257, -0.0453, -0.0116,
         0.0356,  0.0200,  0.0329, -0.0147,  0.0399, -0.0467,  0.0271, -0.0449,
        -0.0562,  0.0506,  0.0365, -0.0007, -0.0083, -0.0664,  0.0189, -0.0215,
         0.0105,  0.0515, -0.0146,  0.0300,  0.0451,  0.0180, -0.0243,  0.0550,
         0.0850, -0.0167, -0.0406, -0.0415,  0.0517,  0.0090,  0.0246,  0.0473,
        -0.0680, -0.0364, -0.0202,  0.0599,  0.0359,  0.0315,  0.0284, -0.0043,
        -0.0520,  0.0522, -0.0829,  0.0416,  0.0577,  0.0427,  0.0262,  0.1026,
         0.0340, -0.0464, -0.0228, -0.0404, -0.0023,  0.0222,  0.0028,  0.0480,
         0.0213, -0.0096,  0.0375,  0.0446,  0.0804, -0.0941, -0.0218, -0.0878,
         0.0012, -0.0336,  0.0021,  0.0437,  0.0027,  0.0693, -0.0531, -0.0227,
        -0.0637,  0.0196, -0.0526,  0.0869,  0.0410,  0.0274,  0.0809, -0.0611,
        -0.0492, -0.1160,  0.0094, -0.0385,  0.0739,  0.0549,  0.0510,  0.0356,
         0.1354, -0.0106, -0.0444, -0.0399, -0.0437,  0.0502, -0.0210,  0.0681,
         0.0557,  0.0002,  0.0106, -0.0545, -0.0645, -0.0085, -0.0273,  0.0278,
        -0.0790, -0.0440, -0.0046,  0.0896,  0.0093, -0.1086,  0.0340,  0.0217,
        -0.1201,  0.0837, -0.0037,  0.0102, -0.0507,  0.0486, -0.0395, -0.0102,
         0.0012, -0.0202,  0.0360,  0.0057,  0.0283, -0.0506,  0.0385, -0.0979,
        -0.0304, -0.0381,  0.0305,  0.0615,  0.0078,  0.0104,  0.0819, -0.0788,
        -0.0533,  0.0445,  0.0433, -0.0396, -0.0487, -0.0839, -0.0483,  0.0189,
         0.0456, -0.0183,  0.0405, -0.0615,  0.0946, -0.0663, -0.0071, -0.0489,
        -0.0027,  0.0488, -0.0059, -0.0240, -0.0779, -0.0231,  0.0443,  0.0553,
        -0.0729, -0.0794,  0.0382,  0.0330, -0.0044,  0.0138, -0.0115, -0.0436,
        -0.0390, -0.0644,  0.0246,  0.0177, -0.0186, -0.0277, -0.0305, -0.0711,
        -0.0247, -0.0575,  0.0279, -0.0415, -0.0487,  0.0317, -0.0035,  0.0784],
       requires_grad=True))

Layer: encoder.2.0.block.1.values_projection._packed_params._packed_params
Parameters: (tensor([[ 0.0189,  0.0142,  0.0708,  ...,  0.0519, -0.0660, -0.0283],
        [ 0.0684, -0.0047,  0.1109,  ...,  0.0236,  0.0354, -0.0472],
        [ 0.0731,  0.0212, -0.0755,  ..., -0.0944, -0.0472,  0.0354],
        ...,
        [-0.0991, -0.0165, -0.0354,  ..., -0.0637, -0.0330, -0.0071],
        [-0.0472, -0.0448, -0.0495,  ..., -0.0354, -0.0425,  0.0425],
        [ 0.0660, -0.0165, -0.0377,  ..., -0.0212,  0.0377,  0.0071]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0023588594049215317,
       zero_point=0), Parameter containing:
tensor([ 0.0334, -0.0700,  0.0336,  0.0698, -0.0622, -0.0227, -0.0196,  0.0694,
         0.0144,  0.0158,  0.0085,  0.0283,  0.0205, -0.0590,  0.0164,  0.0162,
         0.0164, -0.0097, -0.0309, -0.0020, -0.0551, -0.0324,  0.0687,  0.0277,
        -0.0071,  0.0630, -0.0033,  0.0383,  0.0046,  0.0138,  0.0549, -0.0185,
         0.0259,  0.0538,  0.0659,  0.0193, -0.0386, -0.0531,  0.0229, -0.0028,
        -0.0136, -0.0204, -0.0063, -0.0479,  0.0007,  0.0120, -0.0198, -0.0202,
         0.0524,  0.0228, -0.0103, -0.0686,  0.0224, -0.0495, -0.0356,  0.0022,
        -0.0196,  0.0594,  0.0218, -0.0406,  0.0566, -0.0506, -0.0536, -0.0449,
         0.0177, -0.0523, -0.0097,  0.0094, -0.0295,  0.0503,  0.0341, -0.0665,
        -0.0505, -0.0470, -0.0393,  0.0403,  0.0325, -0.0455,  0.0572,  0.0651,
         0.0665, -0.0228,  0.0473, -0.0705,  0.0145,  0.0382,  0.0608,  0.0021,
        -0.0592,  0.0564,  0.0703, -0.0422, -0.0617,  0.0439,  0.0421,  0.0046,
         0.0263, -0.0350, -0.0101,  0.0639, -0.0179, -0.0803,  0.0515,  0.0040,
        -0.0344, -0.0043,  0.0449, -0.0810, -0.0441,  0.0428,  0.0663, -0.0099,
         0.0007,  0.0257,  0.0029,  0.0552,  0.0034, -0.0509, -0.0204, -0.0626,
         0.0296,  0.0512,  0.0209,  0.0436,  0.0614,  0.0175, -0.0147,  0.0518,
        -0.0604,  0.0149,  0.0488,  0.0178, -0.0027, -0.0471, -0.0464,  0.0460,
        -0.0598,  0.0575,  0.0561, -0.0557, -0.0567, -0.0001, -0.0037, -0.0457,
        -0.0548, -0.0079,  0.0359,  0.0833, -0.0124,  0.0048, -0.0096, -0.0442,
         0.0694, -0.0558,  0.0205, -0.0589,  0.0219,  0.0625,  0.0178, -0.0767,
         0.0073, -0.0542, -0.0129, -0.0290, -0.0674,  0.0122,  0.0506,  0.0320,
         0.0377, -0.0208,  0.0084, -0.0291, -0.0353,  0.0344,  0.0649, -0.0416,
        -0.0941, -0.0224, -0.0470,  0.0513,  0.0421, -0.0470,  0.0280, -0.0281,
         0.0338, -0.0615, -0.0426, -0.0343, -0.0123, -0.0199, -0.0608, -0.0035],
       requires_grad=True))

Layer: encoder.2.0.block.1.keys_projection._packed_params._packed_params
Parameters: (tensor([[ 0.1019,  0.0182, -0.0289,  ...,  0.0137,  0.0350, -0.0745],
        [ 0.0243, -0.0076, -0.0106,  ...,  0.0213, -0.0426,  0.0684],
        [ 0.0882, -0.0258,  0.0304,  ..., -0.0517, -0.0547,  0.0760],
        ...,
        [-0.0228, -0.0578, -0.0091,  ...,  0.0289, -0.0487, -0.0319],
        [ 0.0532,  0.0335, -0.0699,  ..., -0.0532,  0.0380,  0.0471],
        [-0.0730, -0.0258, -0.0243,  ...,  0.0335, -0.0122, -0.0745]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.001520564896054566,
       zero_point=0), Parameter containing:
tensor([ 0.0136, -0.0529,  0.0368, -0.0257,  0.0147, -0.0493,  0.0347,  0.0636,
         0.0218,  0.0084, -0.0482,  0.0049,  0.0694,  0.0163,  0.0188, -0.0062,
        -0.0519, -0.0171, -0.0254, -0.0672, -0.0328,  0.0410, -0.0030,  0.0481,
        -0.0376,  0.0242,  0.0358,  0.0547,  0.0484,  0.0313,  0.0629,  0.0086,
        -0.0250,  0.0025,  0.0246,  0.0103,  0.0288,  0.0617, -0.0308, -0.0370,
        -0.0611,  0.0684,  0.0605,  0.0094,  0.0588, -0.0439,  0.0455,  0.0396,
         0.0326,  0.0098,  0.0161, -0.0178,  0.0039, -0.0339, -0.0141, -0.0723,
         0.0200, -0.0374, -0.0656, -0.0612,  0.0659, -0.0201, -0.0642, -0.0303,
         0.0644,  0.0177,  0.0248,  0.0241, -0.0580, -0.0673, -0.0529,  0.0467,
        -0.0796,  0.0768,  0.0590,  0.0439,  0.0554, -0.0161, -0.0479,  0.0776,
         0.0370, -0.0003,  0.0023,  0.0413,  0.0023,  0.0403, -0.0843, -0.0132,
         0.0640,  0.0513, -0.0251, -0.0313, -0.0568, -0.0678, -0.0502,  0.0488,
        -0.0196, -0.0258, -0.0478,  0.0027, -0.0122, -0.0788,  0.0208, -0.0244,
         0.0265, -0.0276,  0.0221, -0.0475, -0.0102, -0.0555,  0.0405,  0.0450,
        -0.0012, -0.0089, -0.0452, -0.0597, -0.0808,  0.0365,  0.0353, -0.0093,
         0.0712,  0.0678,  0.0508, -0.0763,  0.0577,  0.0390,  0.0541,  0.0303,
        -0.0261, -0.0331,  0.0153,  0.0365,  0.0567,  0.0188, -0.0640, -0.0370,
        -0.0631,  0.0413, -0.0154,  0.0278,  0.0388,  0.0480, -0.0088,  0.0635,
        -0.0746,  0.0365,  0.0555,  0.0788,  0.0326, -0.0244,  0.0673, -0.0533,
         0.0255, -0.0721,  0.0501, -0.0150,  0.0508,  0.0581, -0.0336,  0.0420,
        -0.0653, -0.0084, -0.0316,  0.0297,  0.0496,  0.0766, -0.0014, -0.0195,
        -0.0560, -0.0149,  0.0408,  0.0232, -0.0135, -0.0192,  0.0376, -0.0325,
         0.0543,  0.0107, -0.0678, -0.0577, -0.0647,  0.0266,  0.0326, -0.0666,
         0.0546,  0.0314,  0.0182,  0.0436,  0.0061,  0.0292,  0.0634,  0.0107],
       requires_grad=True))

Layer: encoder.2.0.block.1.final_projection._packed_params._packed_params
Parameters: (tensor([[-0.0355,  0.0284,  0.0024,  ...,  0.0284,  0.0308, -0.0592],
        [ 0.0971,  0.0237,  0.0568,  ...,  0.0568, -0.0166,  0.1137],
        [-0.0474,  0.0758,  0.0213,  ...,  0.0426, -0.0047,  0.0450],
        ...,
        [-0.0592,  0.1090,  0.0450,  ..., -0.1303,  0.0095, -0.0213],
        [-0.0497,  0.0332,  0.0545,  ...,  0.0497, -0.0687,  0.0213],
        [ 0.0876, -0.0758, -0.0047,  ...,  0.0734, -0.0497,  0.0474]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0023687107022851706,
       zero_point=0), Parameter containing:
tensor([ 0.0630,  0.0040,  0.0037,  0.0760,  0.0086, -0.0367, -0.0239,  0.0466,
         0.0009, -0.0672,  0.0540,  0.0512, -0.0157, -0.0589, -0.0204,  0.0609,
         0.0284,  0.0352, -0.0409, -0.0471,  0.0191, -0.0088, -0.0459,  0.0696,
        -0.0461, -0.0531, -0.0134,  0.0440,  0.0346,  0.0740,  0.0507,  0.0514,
         0.0238, -0.0414, -0.0549, -0.0603,  0.0519,  0.0020,  0.0393,  0.0582,
        -0.0365, -0.0633,  0.0712, -0.0013, -0.0012,  0.0372, -0.0775, -0.0281,
        -0.0251,  0.0250,  0.0271, -0.0368,  0.0424,  0.0792, -0.0728, -0.0266,
        -0.0252,  0.0435, -0.0292, -0.0497,  0.0432,  0.0136, -0.0528, -0.0289,
         0.0016,  0.0304,  0.0348,  0.0237, -0.0715,  0.0454, -0.0313,  0.0032,
         0.0230, -0.0307,  0.0501, -0.0658,  0.0381,  0.0277, -0.0407,  0.0218,
         0.0069,  0.0059, -0.0607, -0.0436, -0.0648, -0.0200, -0.0776, -0.0237,
         0.0541, -0.0339,  0.0110,  0.0450, -0.0445,  0.0191, -0.0198, -0.0114,
        -0.0457,  0.0549, -0.0027, -0.0369,  0.0046,  0.0662,  0.0827,  0.0019,
         0.0541,  0.0294,  0.0190, -0.0245,  0.0363,  0.0313, -0.0382,  0.0562,
         0.0299, -0.0480,  0.0570,  0.0310, -0.0164,  0.0047,  0.0275,  0.0295,
         0.0118, -0.0077, -0.0502,  0.0433,  0.0431, -0.0452, -0.0461, -0.0205,
         0.0052,  0.0429,  0.0069,  0.0456,  0.0411, -0.0709, -0.0430,  0.0399,
         0.0414,  0.0232, -0.0582,  0.0079, -0.0147, -0.0679, -0.0429,  0.0609,
         0.0650, -0.0241,  0.0697, -0.0803,  0.0351,  0.0546,  0.0610, -0.0045,
         0.0251,  0.0218,  0.0044, -0.0579, -0.0120,  0.0061,  0.0506,  0.0450,
        -0.0709,  0.0791,  0.0352,  0.0861,  0.0207, -0.0520,  0.0444,  0.0137,
         0.0632,  0.0374,  0.0199,  0.0541, -0.0656,  0.0015, -0.0803, -0.0543,
         0.0433,  0.0077, -0.0356, -0.0134, -0.0470, -0.0428,  0.0165, -0.0577,
         0.0035, -0.0601,  0.0232,  0.0247, -0.0016, -0.0389,  0.0447,  0.0266],
       requires_grad=True))

Layer: encoder.2.1.block.0.weight
Parameters: tensor([0.9604, 0.9702, 0.9625, 0.9899, 0.9430, 0.9265, 1.0174, 0.9771, 0.9863,
        0.9299, 0.9082, 0.9838, 0.9573, 0.9714, 0.9392, 0.9898, 0.9705, 0.9983,
        0.9842, 0.9486, 0.9837, 0.9831, 0.9413, 0.9383, 0.9921, 0.9690, 1.0003,
        0.9383, 0.9739, 0.9805, 0.9284, 0.9039, 0.9759, 0.9463, 0.9862, 0.9301,
        0.9799, 0.9204, 0.9651, 1.0066, 0.9678, 0.9733, 0.9133, 0.9989, 0.8608,
        0.9607, 0.9630, 0.9296, 0.9742, 0.9874, 0.9096, 0.9727, 0.9306, 1.0065,
        0.9193, 0.9623, 0.9913, 0.9112, 0.9507, 0.9679, 0.9672, 0.9972, 0.9646,
        0.9902, 0.9552, 0.9271, 1.0274, 1.0129, 0.9819, 0.9967, 0.9749, 0.9530,
        0.9745, 1.0167, 0.9811, 0.9867, 0.9521, 1.0276, 0.9553, 0.9515, 0.9395,
        1.0144, 0.9751, 0.9910, 1.0008, 0.9806, 0.9329, 0.9616, 0.9435, 0.9108,
        0.9094, 0.9640, 0.9496, 0.9441, 0.9270, 0.9559, 0.9212, 0.9691, 0.9573,
        0.9845, 0.9953, 0.9698, 0.9845, 0.9763, 0.9631, 0.9563, 0.9386, 0.9853,
        0.9887, 1.0121, 1.0006, 0.9401, 0.9388, 0.9855, 1.0013, 0.9376, 0.9829,
        0.9425, 1.0040, 0.9210, 0.9428, 0.9573, 0.9649, 0.9723, 0.8837, 0.9805,
        0.9405, 0.9876, 0.9552, 0.8952, 0.9806, 0.9425, 1.0286, 0.9928, 0.9902,
        0.9660, 0.9686, 0.9436, 0.9856, 0.9319, 0.9292, 0.9356, 0.9363, 0.9427,
        0.9449, 0.9452, 1.0231, 0.9625, 0.9426, 0.9657, 0.9919, 0.9494, 0.9340,
        0.9809, 0.9802, 0.9616, 0.9872, 0.9853, 0.9651, 0.9915, 0.9223, 0.9705,
        1.0202, 0.9494, 0.9347, 0.9520, 0.9805, 0.9438, 0.9169, 0.9269, 0.9484,
        0.9435, 0.9931, 0.9784, 0.9132, 0.9528, 0.9932, 0.9208, 0.9833, 1.0210,
        0.9582, 0.9616, 0.9301, 0.9771, 0.9686, 0.9762, 0.9568, 0.9246, 0.9793,
        0.9890, 0.9467, 0.9512])

Layer: encoder.2.1.block.0.bias
Parameters: tensor([ 6.5064e-02, -4.2360e-02, -3.3914e-02,  7.6034e-03, -3.8546e-02,
        -1.5970e-02, -1.9620e-02, -6.5303e-03, -4.4471e-02,  3.9039e-02,
        -2.0819e-02,  6.0189e-02, -5.4702e-02,  5.5413e-05,  4.0514e-02,
        -6.4140e-02, -7.0102e-02, -7.3142e-02,  3.4783e-02,  5.9185e-02,
        -3.8584e-02,  5.7760e-02, -7.4662e-03, -3.7117e-02,  3.9000e-02,
        -4.1652e-02, -7.7203e-02,  1.2661e-03,  2.7334e-02, -5.6674e-02,
        -3.3540e-02,  6.5584e-03,  3.9006e-02, -1.3795e-02, -5.4448e-02,
         4.7286e-02, -3.2024e-02, -3.5763e-02,  6.1744e-03, -4.1948e-02,
        -2.3556e-02, -5.3704e-02, -3.3588e-02,  3.6380e-02, -1.5646e-02,
        -1.1528e-03,  5.0696e-02,  7.3024e-03,  5.1372e-02, -3.7693e-02,
         8.9251e-02,  1.0023e-02,  6.2679e-02, -4.9637e-02, -2.6189e-02,
         2.9024e-02,  3.0482e-02,  8.2182e-03, -2.6155e-02, -6.5898e-02,
        -6.1311e-02, -7.1712e-02, -6.7864e-03,  4.7098e-02, -6.4554e-03,
        -3.8496e-02, -3.8159e-02,  6.1236e-02, -3.2810e-02, -4.5818e-02,
        -4.8381e-02, -3.5158e-02, -3.1957e-02, -3.2896e-02,  3.7845e-02,
        -5.5424e-02, -2.0304e-03,  5.8466e-02, -3.2628e-02, -1.9251e-02,
        -4.4554e-02, -3.3253e-02,  4.0440e-02,  5.0707e-02, -6.4182e-02,
        -5.0139e-02,  2.2951e-02,  1.5118e-02, -6.9297e-02,  4.6552e-02,
        -3.2610e-02, -1.1273e-02,  6.2132e-02, -3.7434e-02, -1.7403e-02,
         7.0505e-02, -7.3490e-03, -3.7903e-02, -4.9994e-02, -1.1032e-02,
        -4.2502e-02, -2.4060e-03, -1.1470e-03, -5.1540e-02,  5.5797e-02,
         4.8778e-02,  5.4378e-02,  4.4549e-02,  6.1894e-02, -7.8829e-03,
         7.5731e-03, -3.0439e-03, -3.8247e-02, -3.5189e-02, -2.3481e-02,
        -5.7438e-02,  3.5470e-02, -2.3416e-02, -3.0580e-02, -2.6936e-02,
         5.3203e-03, -1.9079e-02, -1.4839e-02,  6.4990e-02, -7.8995e-05,
        -3.3238e-02,  3.4295e-02,  2.3954e-02,  4.6344e-02, -3.8128e-02,
         6.1788e-02,  2.6278e-02, -2.4347e-02, -5.8374e-02, -4.1738e-02,
         3.7929e-02, -3.0199e-02,  2.3979e-02, -4.9691e-02, -2.2863e-02,
        -2.7803e-02,  2.9465e-02, -6.2360e-02, -1.8745e-03, -1.2799e-02,
         7.1732e-02,  2.1108e-02,  3.7094e-02, -1.6215e-02,  3.5928e-02,
        -3.6588e-02, -1.9868e-02, -6.8886e-03, -4.2518e-02,  4.7942e-02,
         2.5029e-03,  4.4559e-02, -4.5821e-02, -5.5000e-02, -6.0584e-02,
         2.9116e-02, -3.5638e-02, -7.9312e-02, -9.0149e-02, -2.0068e-02,
        -9.8251e-03, -5.0091e-02,  6.5205e-02, -8.5833e-02,  2.2896e-02,
         2.1697e-02, -1.8905e-02, -2.9528e-02, -8.7866e-03,  2.6421e-02,
         2.2958e-02,  8.4655e-02,  2.2694e-02, -6.0124e-02,  4.6190e-02,
         6.1033e-03,  5.2477e-02, -1.4667e-02,  5.0129e-02, -1.6060e-02,
        -5.3808e-02, -4.0469e-02, -2.7371e-02,  4.5355e-02,  5.5676e-02,
         3.9939e-02,  4.6023e-02])

Layer: encoder.2.1.block.1.0._packed_params._packed_params
Parameters: (tensor([[-0.0229, -0.0491, -0.0458,  ...,  0.0164, -0.0900, -0.0638],
        [ 0.0393,  0.0573, -0.0180,  ..., -0.0458, -0.0589, -0.0393],
        [-0.0295,  0.0278, -0.0720,  ..., -0.0049, -0.0098, -0.0753],
        ...,
        [-0.0638, -0.0327,  0.0671,  ..., -0.0573, -0.0246,  0.0327],
        [ 0.0458, -0.0360,  0.0295,  ...,  0.0098, -0.0687,  0.0589],
        [-0.0278,  0.0622,  0.0540,  ...,  0.0098,  0.0000, -0.0393]],
       size=(768, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0016368464566767216,
       zero_point=0), Parameter containing:
tensor([-6.2825e-02, -5.9925e-02,  1.9689e-02, -1.0500e-01, -1.2624e-02,
        -4.6200e-04, -6.0403e-02,  4.3060e-02, -6.0316e-02, -1.1787e-02,
        -1.2164e-02, -8.8890e-03,  1.6618e-03, -3.5662e-02, -7.2256e-02,
        -6.7720e-02, -5.9831e-02,  1.0516e-04, -1.6534e-04,  6.9006e-04,
        -4.8849e-02, -2.1132e-02, -9.7015e-02, -3.1489e-03,  8.0167e-03,
        -7.5050e-02, -5.2075e-02, -6.4045e-02, -1.6519e-02, -1.0034e-01,
        -1.9744e-02,  1.4580e-02, -9.6054e-02, -7.9813e-02, -1.5511e-02,
        -5.1413e-02,  3.5885e-02, -3.9163e-02, -1.8044e-02, -4.7575e-02,
        -6.0403e-02, -5.1688e-02, -1.2840e-02, -5.0485e-02, -5.0426e-02,
        -5.3754e-02,  1.5582e-02, -8.2961e-02, -3.9504e-02, -5.3855e-02,
        -3.2479e-02, -5.0422e-02, -5.4641e-02, -6.5602e-03, -9.4862e-03,
        -5.4123e-02, -3.7985e-02,  1.9307e-02,  1.9761e-03, -7.8746e-02,
        -4.0072e-04, -9.4103e-02, -5.1214e-02, -5.2107e-03,  4.3129e-02,
        -6.1322e-02, -7.6879e-02, -4.3715e-03, -1.8587e-02, -1.1619e-02,
        -2.2741e-02, -2.7308e-04, -7.4833e-02, -3.7731e-02, -7.1029e-02,
        -3.4070e-02, -5.7208e-02, -5.8213e-02,  2.4690e-02, -4.6770e-02,
        -7.3090e-02, -5.4261e-02, -1.5676e-02,  1.1495e-02, -6.8473e-02,
        -6.4094e-02, -8.7307e-02, -5.9911e-02, -3.5490e-02, -3.6739e-03,
        -2.7189e-02, -1.1225e-01, -1.1838e-02, -2.4918e-02,  6.4507e-03,
        -3.8314e-02, -3.9589e-02, -9.5506e-03, -1.8436e-02,  1.5015e-02,
         3.4851e-02, -5.9090e-03, -2.1473e-02, -9.3484e-02, -7.7877e-02,
        -4.1228e-02, -1.3902e-02,  1.9434e-02, -9.2169e-02, -2.1916e-03,
        -8.8754e-02, -7.6131e-02, -1.2583e-02, -3.5828e-02, -5.3109e-02,
        -1.0296e-01, -9.6197e-02,  7.6389e-03,  5.6291e-05, -1.1361e-03,
         2.9365e-02, -6.8705e-02, -9.8236e-02, -7.9652e-02, -7.1043e-02,
         2.0137e-02,  1.0857e-02, -9.0371e-02, -9.2813e-02, -2.2223e-02,
        -4.4116e-02, -1.0564e-02, -1.3713e-02, -2.0586e-02, -2.0304e-02,
        -5.5083e-02, -3.5181e-02, -5.6896e-02, -4.7451e-04, -2.8464e-02,
        -6.2986e-02,  2.9243e-02, -5.0625e-02, -2.5361e-02, -2.5950e-02,
        -3.5454e-02,  5.8305e-03,  1.6140e-02, -1.8010e-02, -2.5505e-02,
        -4.5037e-02, -7.0028e-02, -8.8873e-03,  4.4776e-03, -8.4056e-02,
        -9.8921e-02, -9.5272e-02,  1.4174e-02,  6.0814e-03, -5.9052e-02,
        -4.6415e-02, -5.0804e-02,  1.9727e-03, -7.1533e-03, -6.5197e-02,
        -8.8603e-02, -9.0326e-02, -2.0231e-02, -7.1921e-02, -7.9490e-02,
        -3.1049e-02,  1.5224e-02, -4.0253e-02, -2.6942e-02,  1.5572e-02,
        -6.9262e-02,  1.1639e-02,  4.1420e-03,  9.0298e-03, -2.2344e-03,
        -1.0722e-01,  3.0865e-03, -1.0992e-02, -6.8640e-02, -8.1689e-02,
        -1.1236e-01,  3.4494e-02, -1.2312e-02, -6.6043e-02, -7.6974e-03,
         3.7515e-02, -2.5202e-03, -7.2661e-02, -7.8695e-02,  1.6326e-02,
        -2.7705e-03,  2.0764e-02, -1.0687e-01, -1.8905e-03, -9.5563e-02,
         3.6841e-03, -3.7069e-02, -5.5153e-03, -6.0769e-02, -2.0950e-02,
        -6.0330e-02, -2.1936e-02, -1.7735e-02, -1.1540e-01,  4.4541e-02,
         2.7743e-02, -7.6732e-04, -5.5392e-02, -7.4367e-03, -4.6239e-02,
        -5.0124e-02,  3.1346e-02, -2.4499e-03, -1.7387e-02, -3.2708e-02,
        -1.6484e-02, -6.4176e-02, -1.3629e-02, -4.8212e-02,  1.0296e-04,
        -3.1171e-02, -2.2158e-02, -4.3269e-03,  1.3180e-02, -1.6560e-02,
         2.0188e-02, -3.8777e-02, -1.6199e-02, -3.2312e-02,  2.5805e-02,
        -5.7137e-02, -2.0137e-02, -3.0557e-02, -6.6463e-02,  3.1983e-02,
        -2.2998e-02,  1.7146e-02, -9.4896e-02, -1.1201e-01,  2.3858e-02,
        -2.1190e-02, -1.6471e-02, -8.0851e-02,  5.4213e-03, -1.3440e-03,
         9.3585e-03, -8.7035e-02, -1.0217e-02, -2.6127e-02, -5.7664e-02,
        -3.4274e-02,  1.6571e-02,  4.0440e-03, -4.1139e-02,  1.8818e-03,
        -2.1799e-02,  4.0418e-02,  2.6436e-03, -9.2305e-02, -9.5224e-02,
        -4.7590e-02, -5.5037e-03, -7.1984e-03, -1.0678e-01, -4.6851e-02,
        -2.6137e-02, -2.2595e-03, -8.0420e-03, -4.4855e-02, -8.9241e-02,
        -6.2174e-02, -8.9936e-02, -6.1660e-02, -5.0714e-02,  3.3178e-03,
        -7.2544e-02,  1.9639e-02, -7.9092e-02, -9.2734e-02,  1.0351e-02,
        -6.9643e-02, -3.4151e-02, -4.4597e-02, -6.1513e-04,  4.1661e-02,
        -2.6298e-02, -1.0017e-01, -3.2174e-02, -2.8658e-02, -1.1477e-01,
         1.5543e-03,  1.2301e-02,  2.1011e-02, -6.2491e-02, -5.5843e-02,
        -8.6496e-02,  3.2853e-03,  1.1083e-02, -4.3909e-03, -5.6554e-02,
         2.1305e-02,  2.0746e-02,  3.3695e-03, -6.1452e-02, -5.0776e-02,
        -4.7318e-02, -3.6800e-02,  1.4742e-02, -1.0251e-01,  7.2439e-03,
         1.3907e-03, -2.9886e-02, -8.2529e-02, -1.1928e-02, -1.0092e-01,
        -1.0533e-02, -9.5069e-02, -3.3560e-02, -8.7856e-02, -7.8063e-02,
        -5.2378e-03, -8.1579e-02, -7.9801e-02,  9.7008e-03, -5.3339e-02,
        -8.9517e-02, -7.7386e-02, -2.4451e-02,  3.4241e-02, -1.2690e-03,
         4.0956e-02,  3.4203e-03, -9.8111e-02, -5.2920e-02,  9.9523e-03,
         3.5432e-02,  4.3526e-02, -9.8196e-02, -5.1930e-02, -4.6710e-02,
        -1.1480e-02, -7.6065e-02, -4.6784e-02, -9.6453e-02, -2.3366e-02,
        -3.9636e-02, -2.8950e-02, -7.4791e-02, -4.8498e-02, -2.8773e-02,
        -2.0264e-02, -1.0134e-01, -9.4735e-02,  1.8693e-03, -9.1980e-02,
        -1.4861e-02, -2.2730e-02,  4.6826e-03, -8.3180e-02, -6.7549e-02,
        -3.3244e-02, -5.7670e-02, -5.4141e-03, -2.0881e-02, -2.6163e-03,
         2.5686e-02,  4.4869e-03, -1.9351e-02, -1.6964e-02, -9.4010e-02,
        -5.2025e-03, -5.6243e-02, -9.4005e-02,  1.9325e-02, -8.4582e-02,
        -2.1012e-02, -2.1088e-02, -6.6500e-02,  3.8251e-02,  2.7949e-02,
         1.3616e-02, -2.2105e-02, -7.0978e-02,  3.1185e-02,  1.6650e-02,
         1.3369e-02, -9.4901e-02, -4.1913e-02, -8.1575e-02, -8.3472e-03,
        -3.1526e-02,  1.4368e-02, -3.3094e-02,  2.8762e-02,  1.6968e-02,
        -9.0383e-02, -8.5282e-03,  9.1166e-03, -4.5345e-02,  1.8647e-02,
         2.8758e-02, -8.0407e-02, -5.9288e-02, -7.9068e-02, -2.0545e-02,
         1.7127e-04, -4.6787e-02,  2.0879e-02, -1.8992e-02, -9.4204e-02,
        -6.8703e-02, -4.9041e-02, -5.2482e-02, -5.8339e-02, -4.1749e-02,
         1.1890e-02,  1.0353e-02, -5.0819e-02, -7.3530e-02, -8.8908e-02,
        -8.9334e-02,  1.6425e-02, -3.7677e-02, -2.3369e-02, -8.1217e-02,
        -9.5448e-02, -7.2363e-02,  1.4757e-02, -1.4471e-02,  2.4515e-02,
        -3.1313e-02,  3.7496e-02, -2.6144e-03,  8.3677e-04, -7.5205e-02,
        -3.9007e-02,  1.2713e-02, -6.1551e-02,  6.9904e-03, -2.9246e-03,
         9.3714e-03, -8.3196e-02, -6.4955e-02, -4.9139e-02, -3.1840e-02,
        -2.1862e-02,  1.6531e-02, -6.2276e-03, -8.5279e-02, -2.2457e-02,
        -9.0360e-02, -8.6761e-02, -1.0425e-01, -1.0278e-01,  1.8212e-02,
        -9.5732e-02, -8.6527e-02,  1.3754e-02, -4.7328e-02,  8.8141e-03,
        -7.5525e-02, -5.0557e-02,  6.6861e-03, -5.6322e-02,  1.8771e-02,
         9.1603e-03, -8.1352e-02, -3.2598e-02,  3.6195e-02,  1.5765e-02,
        -7.0108e-03, -7.2404e-02, -1.1497e-02, -7.2817e-02, -1.1946e-02,
        -5.1923e-03, -1.0919e-02, -6.8661e-02, -1.0259e-02,  1.0043e-02,
        -2.0018e-02,  2.1078e-02, -6.1074e-02,  3.4750e-02, -5.1055e-02,
        -1.0279e-01, -2.0405e-02,  5.5936e-03, -7.2191e-02, -8.2432e-02,
        -1.0333e-01, -5.9830e-02, -1.1741e-02, -6.2885e-04,  1.2781e-02,
        -3.1420e-03, -2.7209e-03, -3.2933e-02,  2.3147e-02, -3.0728e-02,
        -8.9733e-02, -5.3949e-02, -3.6737e-02, -2.0360e-02, -2.2143e-02,
        -6.2328e-02, -2.3810e-02, -9.1121e-02, -4.9425e-02, -8.9072e-02,
        -7.5524e-03, -4.4672e-02, -1.0353e-01, -8.1059e-02,  3.7141e-02,
         6.0836e-03, -5.0727e-02, -8.5370e-02, -2.9111e-02, -6.4260e-02,
        -1.2768e-02, -4.7028e-02,  1.1098e-02,  1.0640e-02, -3.5751e-04,
        -1.3744e-03, -2.3780e-02, -9.4255e-02, -1.0797e-01, -6.0007e-02,
        -1.6748e-03, -9.2900e-02, -7.3527e-02, -9.3293e-02, -5.3835e-02,
         4.0053e-02, -4.9636e-02, -1.3589e-03, -9.8251e-02, -8.5069e-02,
        -1.2862e-01, -4.0287e-02, -6.4913e-02, -7.6112e-02, -4.5021e-03,
        -1.8141e-02, -9.0785e-02, -6.7719e-03, -8.4915e-02, -4.0020e-02,
        -9.6746e-03, -1.1889e-02, -7.1309e-03, -3.4110e-02, -6.5799e-02,
        -7.4375e-03, -8.7582e-02, -3.6930e-02,  8.0791e-03,  2.8562e-02,
         3.8928e-03, -8.1794e-02, -1.0181e-02, -6.2707e-03, -1.4808e-02,
        -5.6316e-02,  1.6669e-02,  4.2177e-02,  1.2637e-02, -8.4925e-02,
        -6.2359e-02,  6.9673e-03, -8.7173e-02,  3.5685e-02, -7.4361e-02,
        -2.7928e-02, -8.0826e-02, -9.5858e-02,  6.3934e-03, -4.8087e-02,
        -4.5292e-02, -5.0708e-02, -1.9293e-02, -1.1107e-01, -9.8397e-03,
         1.4028e-02, -2.4430e-02,  1.9622e-02, -6.3337e-02, -1.0807e-01,
        -1.9839e-02, -1.0578e-01, -4.2978e-02, -1.9559e-02, -1.7847e-03,
        -9.2881e-02,  1.7043e-02, -6.3358e-02, -6.9912e-02, -1.1856e-01,
        -1.7057e-02, -5.4020e-02, -3.2552e-03, -1.7955e-02, -7.7893e-02,
        -4.3621e-03, -1.8432e-02,  2.4195e-02, -7.3187e-02, -4.0473e-02,
        -6.3496e-03, -3.9492e-03, -7.6702e-02,  1.2021e-02, -6.2026e-02,
         1.5652e-02,  9.0797e-03,  3.3137e-02, -8.1672e-02, -5.7117e-02,
         2.4732e-02, -5.9668e-02, -8.0016e-02, -8.4890e-02,  2.2408e-02,
        -4.4575e-02,  3.1215e-02, -3.1240e-02, -1.6516e-02, -2.2390e-02,
        -6.0180e-02,  2.4719e-02, -1.0407e-01,  2.3522e-02, -3.4266e-02,
        -2.7226e-02,  8.7234e-03,  3.5024e-02, -4.8315e-02, -5.8108e-02,
         1.2335e-02, -1.8068e-02, -7.4040e-02, -7.4793e-02,  1.3554e-02,
         2.9603e-02, -2.5704e-02,  1.3551e-02,  7.5116e-03,  4.0533e-02,
        -9.4732e-02, -5.1760e-03, -7.9522e-03, -6.9637e-02, -2.0136e-02,
        -1.0274e-01, -5.9871e-02, -1.7819e-04, -9.9077e-02, -9.3987e-02,
        -1.1878e-02, -2.0909e-02, -7.2264e-02, -6.8333e-02, -1.9597e-02,
        -7.3894e-02, -6.3600e-02, -1.4603e-02,  2.1491e-02,  1.9091e-02,
        -7.8177e-02, -9.5765e-02, -6.8314e-02, -6.8076e-02, -9.3592e-02,
        -3.1090e-02, -6.1442e-02, -8.3919e-02, -3.9596e-02,  8.2747e-03,
         2.4785e-02, -1.2931e-02, -2.7780e-02, -6.9410e-03,  1.6213e-02,
        -4.4674e-02, -5.3168e-02,  1.2015e-02, -1.5813e-02, -1.0346e-01,
        -1.8457e-02, -5.1871e-02, -3.3592e-02, -3.7241e-02,  2.4677e-02,
        -3.4543e-02, -1.7719e-02, -5.7534e-02, -3.0769e-02, -3.2744e-02,
        -3.0560e-02, -5.8522e-02, -6.1040e-02,  1.4603e-02, -1.9459e-02,
        -5.9728e-02, -7.2351e-02, -5.7231e-02, -2.3530e-02,  1.8453e-02,
         1.9623e-03, -4.6914e-02, -3.3175e-02, -8.8304e-02,  3.0096e-02,
         3.1331e-03, -8.2430e-02,  2.1705e-02, -7.2449e-02, -9.8618e-02,
         3.5610e-02, -6.9230e-02, -1.1074e-01,  2.0970e-02, -9.1771e-02,
        -7.8779e-02,  4.2274e-03, -3.4298e-02, -7.7311e-02, -9.4822e-02,
        -3.4031e-02, -6.6181e-02, -5.7138e-03,  4.6508e-02, -4.0292e-02,
        -9.0512e-02, -7.6827e-02, -3.8805e-02,  2.3373e-02, -1.8574e-03,
        -8.7478e-02, -2.2341e-02,  1.1120e-02, -1.9421e-02, -2.5370e-02,
         9.5575e-03, -4.2593e-02, -5.3775e-02,  2.1899e-02, -4.4056e-02,
         2.8894e-02,  9.3918e-03, -4.5204e-02, -7.1903e-02, -2.4300e-03,
        -1.0341e-01, -8.6810e-02, -2.5628e-02, -2.5381e-02,  3.1013e-02,
         2.6985e-02, -1.6544e-02, -8.4687e-03], requires_grad=True))

Layer: encoder.2.1.block.1.2._packed_params._packed_params
Parameters: (tensor([[-0.0196,  0.0141,  0.0685,  ...,  0.0087,  0.0435,  0.0120],
        [-0.0457, -0.0598,  0.0130,  ...,  0.0272,  0.0478, -0.0402],
        [ 0.0152, -0.0098, -0.0380,  ...,  0.0261,  0.0239,  0.0022],
        ...,
        [ 0.0174,  0.0152, -0.0076,  ..., -0.0033, -0.0544,  0.0163],
        [ 0.0163,  0.0380,  0.0315,  ..., -0.0272, -0.0370, -0.0370],
        [ 0.0239,  0.0043, -0.0109,  ...,  0.0098,  0.0228, -0.0359]],
       size=(192, 768), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.001087119453586638,
       zero_point=0), Parameter containing:
tensor([-4.1336e-02,  4.5801e-03,  2.2126e-02,  2.3822e-02, -2.6815e-02,
         2.7287e-02,  1.5227e-02, -1.6175e-02,  7.2780e-04,  3.4957e-02,
        -2.7929e-02, -4.1313e-02,  2.3468e-02,  1.5889e-02, -6.5835e-02,
         3.0434e-02,  2.6640e-02, -6.9037e-03, -1.6055e-02, -2.9718e-02,
         5.6194e-03,  9.1998e-03,  2.7723e-02,  2.6039e-02, -2.2929e-02,
         2.8780e-02, -2.3998e-03, -1.4049e-02, -3.6893e-03, -1.3857e-02,
         7.7030e-03, -2.7831e-02, -2.0406e-02, -3.2954e-02, -2.5833e-03,
        -2.2123e-02, -2.5976e-02,  1.1305e-02, -5.1825e-02,  1.0536e-02,
         2.0024e-02,  5.2821e-03,  4.5590e-02,  1.6092e-02,  1.7242e-02,
        -3.5606e-02, -4.8611e-02, -1.4111e-02,  3.5987e-03, -1.3671e-02,
         2.7544e-02, -1.8263e-02,  2.0302e-02, -6.7009e-03, -2.6875e-02,
        -2.4101e-02, -4.6095e-02, -4.1305e-02,  1.4058e-02, -1.3969e-02,
        -2.4202e-02, -1.3085e-02, -6.3109e-03, -1.4449e-02,  3.0289e-02,
        -7.5053e-04,  2.9985e-02,  9.7298e-03, -2.1443e-02,  4.3011e-03,
        -2.3308e-02,  1.3980e-02,  8.3623e-03, -2.0629e-02,  2.2714e-02,
        -1.7243e-03, -1.6768e-02, -1.7893e-02, -1.8564e-02,  3.4308e-02,
         1.0617e-02,  8.7901e-03, -2.5478e-02, -3.1424e-02, -2.0903e-02,
         1.1776e-02, -6.1900e-02,  1.5049e-03,  5.6944e-02, -5.6714e-02,
         2.5937e-02, -5.6062e-03, -7.7449e-03,  2.0190e-02, -1.8714e-02,
         1.0769e-02, -6.6137e-02,  1.9843e-02,  2.4148e-02, -8.8280e-03,
        -3.1424e-02, -1.6539e-02,  3.6630e-02,  1.3717e-02, -9.5428e-03,
         4.8574e-04, -2.1357e-02, -5.4897e-02, -2.0686e-02, -2.2666e-02,
        -1.9598e-02, -7.1080e-03, -1.4985e-02, -8.9427e-03, -1.0009e-02,
        -3.9445e-03,  8.1100e-03,  2.4291e-02, -1.1446e-02,  6.4249e-03,
        -3.8435e-02, -3.8145e-02,  1.2886e-02,  2.4926e-02, -2.3288e-02,
        -3.3482e-03, -2.6558e-02, -2.2742e-02, -3.3689e-02, -4.9741e-02,
        -5.8191e-02, -6.3206e-03, -1.7045e-02, -1.3661e-02,  2.8246e-02,
        -3.2521e-02, -1.6637e-02, -2.2820e-02,  1.5036e-02, -3.8246e-03,
        -7.7782e-05, -1.0405e-02,  8.3125e-03, -2.1578e-02,  1.4777e-02,
        -4.3269e-02,  1.9868e-02, -7.4834e-03,  1.0887e-02,  9.2601e-03,
        -1.2346e-02, -4.3302e-03, -2.5231e-02,  2.4491e-02, -2.0670e-03,
         3.0971e-02,  2.1516e-02,  1.9835e-02, -3.1372e-02, -7.6633e-03,
         4.3476e-03,  2.5469e-02,  3.2322e-02,  2.9501e-03, -1.8306e-02,
        -1.7891e-02,  2.1939e-02, -5.1756e-02,  1.7926e-02,  1.5656e-02,
        -2.7191e-02, -9.5799e-03,  2.2232e-02, -3.4753e-02, -4.2954e-02,
        -3.7147e-02, -4.3021e-02, -3.4651e-03, -1.0416e-02, -1.5006e-02,
        -2.0783e-02,  6.8024e-03,  2.6789e-02, -2.5690e-02, -1.0627e-02,
         3.7415e-02,  4.5537e-03, -1.3005e-02,  1.3236e-02,  1.9604e-03,
         9.2773e-03, -5.9155e-02], requires_grad=True))

