{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "9f24395e3322cf24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:08.854678Z",
     "start_time": "2024-05-06T09:24:08.839267Z"
    }
   },
   "source": [
    "import itertools # 是 Python 的內建模組，提供了一組用於處理迭代器的函數和工具。\n",
    "                 # 它包含了各種用於高效處理迭代器的函數，可以幫助我們編寫更簡潔、高效的代碼。\n",
    "import sys # 是 Python 的內建模組，提供了與 Python 解釋器和運行環境相關的功能。\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# sys.path 是一個列表，包含了 Python 解釋器在導入模組時會搜尋的路徑。\n",
    "# 當你使用 import 語句導入模組時 Python 會依次在 sys.path 中的路徑下尋找對應的模組文件。\n",
    "sys.path.append(\"../ecg-classification/\")\n",
    "# sys.path.append(\"C:\\\\Users\\\\Chen_Lab01\\\\Documents\\\\GitHub/ecg-classification\")\n",
    "# from IPython.display import Video\n",
    "# import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(\"ggplot\") #  是 Matplotlib 庫中用於設置繪圖樣式的函數。它使用了一種名為 \"ggplot\" 的預定義樣式\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "                        #  該樣式模仿了 R 語言的 ggplot2 繪圖包的外觀。\n",
    "# print(sys.path)\n",
    "import torch\n",
    "from ecg_tools.config import EcgConfig, Mode\n",
    "from ecg_tools.data_loader import DatasetConfig, get_data_loaders\n",
    "from ecg_tools.model import ECGformer\n",
    "from ecg_tools.train import ECGClassifierTrainer\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "adbb91b9dbfa94e3",
   "metadata": {},
   "source": "## Load model"
  },
  {
   "cell_type": "code",
   "id": "29bb12472624e7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:08.931581Z",
     "start_time": "2024-05-06T09:24:08.901160Z"
    }
   },
   "source": [
    "import torch\n",
    "config = EcgConfig()    \n",
    "model_quantized = torch.load(\"..\\\\..\\\\model_save\\\\model_quantized_598.pth\")\n",
    "model = torch.load(\"..\\\\..\\\\model_save\\\\model_epoch_598.pth\")\n",
    "model_quantized.eval()\n",
    "model_quantized.to('cpu')\n",
    "model.eval()\n",
    "model.to('cpu')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECGformer(\n",
       "  (encoder): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (0): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): MultiHeadAttention(\n",
       "            (queries_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (values_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (final_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (0): Reduce('b n e -> b e', 'mean')\n",
       "    (1): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       "  (embedding): LinearEmbedding(\n",
       "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "fdc775d31a626cf5",
   "metadata": {},
   "source": "## 量化模型"
  },
  {
   "cell_type": "code",
   "id": "fe171c96df432930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.055529Z",
     "start_time": "2024-05-06T09:24:09.040493Z"
    }
   },
   "source": [
    "# import torch.quantization\n",
    "# \n",
    "# # 使用 Eager Mode Quantization\n",
    "# # 將 torch.nn.Linear 的參數映射到 -127~127 之間\n",
    "# \n",
    "# model_quantized = torch.quantization.quantize_dynamic(\n",
    "#     model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "# )\n",
    "# \n",
    "# torch.save(model_quantized, \"..\\\\..\\\\model_save\\\\model_quantized_598.pth\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 準確度測試",
   "id": "a041d1c91993f194"
  },
  {
   "cell_type": "code",
   "id": "780d3550eb5bc030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.024466Z",
     "start_time": "2024-05-06T09:24:08.933673Z"
    }
   },
   "source": [
    "import einops\n",
    "loader = get_data_loaders(DatasetConfig())\n",
    "accuracy = 0\n",
    "for signal, label in loader[Mode.eval]:\n",
    "    signal.to('cpu')\n",
    "    label.to('cpu')\n",
    "    signal = einops.rearrange(signal, \"b c e -> b e c\")\n",
    "    # print(signal)\n",
    "    p = model_quantized(signal)\n",
    "    # print(p)\n",
    "    print(label)\n",
    "    # print(signal.shape, label.shape)\n",
    "    # print(p.argmax(1) == label)\n",
    "    accuracy += torch.sum(p.argmax(1) == label)\n",
    "    print(f\"accuracy: {accuracy / config.dataset.batch_size}\")\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## View Prameter",
   "id": "a8b7f2cf319ea7bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model layer",
   "id": "a64d994b3dd451d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.071044Z",
     "start_time": "2024-05-06T09:24:09.057529Z"
    }
   },
   "cell_type": "code",
   "source": "print(model_quantized)",
   "id": "c5ba9693145ab0eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGformer(\n",
      "  (encoder): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (0): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): MultiHeadAttention(\n",
      "            (queries_projection): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (values_projection): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (keys_projection): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (final_projection): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (0): Reduce('b n e -> b e', 'mean')\n",
      "    (1): DynamicQuantizedLinear(in_features=64, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (embedding): LinearEmbedding(\n",
      "    (0): DynamicQuantizedLinear(in_features=1, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): GELU(approximate='none')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "字典形式的量化模型參數",
   "id": "a724a3db99f7bd99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.086339Z",
     "start_time": "2024-05-06T09:24:09.072545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 量化\n",
    "for param_name, param_tensor in model_quantized.state_dict().items():\n",
    "    print(f\"{param_name}\")"
   ],
   "id": "f3513f473313dc22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding\n",
      "encoder.0.0.block.0.queries_projection.scale\n",
      "encoder.0.0.block.0.queries_projection.zero_point\n",
      "encoder.0.0.block.0.queries_projection._packed_params.dtype\n",
      "encoder.0.0.block.0.queries_projection._packed_params._packed_params\n",
      "encoder.0.0.block.0.values_projection.scale\n",
      "encoder.0.0.block.0.values_projection.zero_point\n",
      "encoder.0.0.block.0.values_projection._packed_params.dtype\n",
      "encoder.0.0.block.0.values_projection._packed_params._packed_params\n",
      "encoder.0.0.block.0.keys_projection.scale\n",
      "encoder.0.0.block.0.keys_projection.zero_point\n",
      "encoder.0.0.block.0.keys_projection._packed_params.dtype\n",
      "encoder.0.0.block.0.keys_projection._packed_params._packed_params\n",
      "encoder.0.0.block.0.final_projection.scale\n",
      "encoder.0.0.block.0.final_projection.zero_point\n",
      "encoder.0.0.block.0.final_projection._packed_params.dtype\n",
      "encoder.0.0.block.0.final_projection._packed_params._packed_params\n",
      "classifier.1.scale\n",
      "classifier.1.zero_point\n",
      "classifier.1._packed_params.dtype\n",
      "classifier.1._packed_params._packed_params\n",
      "embedding.cls_token\n",
      "embedding.0.scale\n",
      "embedding.0.zero_point\n",
      "embedding.0._packed_params.dtype\n",
      "embedding.0._packed_params._packed_params\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "字典形式的非量化模型參數",
   "id": "d308ce0f9be7ec25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.101538Z",
     "start_time": "2024-05-06T09:24:09.087839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 未量化\n",
    "for param_name, param_tensor in model.state_dict().items():\n",
    "    print(f\"{param_name}\")"
   ],
   "id": "f4dd095dd96a7ee0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding\n",
      "encoder.0.0.block.0.queries_projection.weight\n",
      "encoder.0.0.block.0.queries_projection.bias\n",
      "encoder.0.0.block.0.values_projection.weight\n",
      "encoder.0.0.block.0.values_projection.bias\n",
      "encoder.0.0.block.0.keys_projection.weight\n",
      "encoder.0.0.block.0.keys_projection.bias\n",
      "encoder.0.0.block.0.final_projection.weight\n",
      "encoder.0.0.block.0.final_projection.bias\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "embedding.cls_token\n",
      "embedding.0.weight\n",
      "embedding.0.bias\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packed_params",
   "id": "4ab7fe9f2a642014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.132685Z",
     "start_time": "2024-05-06T09:24:09.103038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "weight_tensor_after = model_quantized.state_dict()['encoder.0.0.block.0.final_projection._packed_params._packed_params']\n",
    "\n",
    "# packed_params = model_quantized.encoder[0][0].block[0].queries_projection._packed_params._packed_params\n",
    "\n",
    "# Unpack the quantized weights and biases\n",
    "int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "int8_bias_nd = int8_bias.detach().numpy()\n",
    "# # Dequantize the weights and biases\n",
    "# weights = int8_weights.dequantize()\n",
    "# bias = int8_bias.dequantize()\n",
    "\n",
    "print(int8_weights_nd)#　將量化後的權重轉換為整數表示並轉化為numpy# array\n",
    "print(int8_weights.q_scale()) #　獲取量化的scale\n",
    "print(int8_bias_nd)"
   ],
   "id": "d95afa5238bac0f7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'packed_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 7\u001B[0m\n\u001B[0;32m      2\u001B[0m weight_tensor_after \u001B[38;5;241m=\u001B[39m model_quantized\u001B[38;5;241m.\u001B[39mstate_dict()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoder.0.0.block.0.final_projection._packed_params._packed_params\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# packed_params = model_quantized.encoder[0][0].block[0].queries_projection._packed_params._packed_params\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Unpack the quantized weights and biases\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m int8_weights, int8_bias \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mquantized\u001B[38;5;241m.\u001B[39mlinear_unpack(\u001B[43mpacked_params\u001B[49m)\n\u001B[0;32m      8\u001B[0m int8_weights_nd \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(int8_weights\u001B[38;5;241m.\u001B[39mint_repr())\n\u001B[0;32m      9\u001B[0m int8_bias_nd \u001B[38;5;241m=\u001B[39m int8_bias\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'packed_params' is not defined"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.134685Z",
     "start_time": "2024-05-06T09:24:09.134185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param_name, param_tensor in model_quantized.state_dict().items():\n",
    "    if isinstance(param_tensor, torch.Tensor):\n",
    "        if not any(special_param in param_name for special_param in ['scale', 'zero_point', 'dtype', '_packed_params']):\n",
    "            print(f\"{param_name}\\t{param_tensor.size()}\")"
   ],
   "id": "93d323b07c1341c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "印出模型中所有的weights和bias",
   "id": "9ec3ae45807205c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.135685Z",
     "start_time": "2024-05-06T09:24:09.135685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model_quantized.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Weight: {param.data}\")\n",
    "    elif 'bias' in name:\n",
    "        print(f\"Layer: {name}\") \n",
    "        print(f\"Bias: {param.data}\")\n"
   ],
   "id": "42b88a0cb1bf2fa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for var_name in model_quantized.state_dict():\n",
    "    print(var_name, \"\\t\", model_quantized.state_dict()[var_name])"
   ],
   "id": "306fd264326966ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18593e383e1153c3",
   "metadata": {},
   "source": "## Parameter Extraction"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    'scale',\n",
    "    'zero_point',\n",
    "    '_packed_params.dtype'\n",
    "]\n",
    "\n",
    "with open('model_layers_params.txt', 'w') as f:\n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if not any(ignore_key in layer_name for ignore_key in ignore_keys):\n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                # Dynamically get the corresponding layer\n",
    "                layer_parts = layer_name.split('.')\n",
    "                layer = model_quantized\n",
    "                for part in layer_parts[:-1]:\n",
    "                    layer = getattr(layer, part)\n",
    "                \n",
    "                packed_params = getattr(layer, '_packed_params')\n",
    "                \n",
    "                # Unpack the quantized weights and biases\n",
    "                int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "                int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "                int8_bias_nd = int8_bias.detach().numpy()\n",
    "                \n",
    "                f.write(f\"Layer: {layer_name}\\n\")\n",
    "                f.write(f\"Quantized Weights:\\n{int8_weights_nd}\\n\")\n",
    "                f.write(f\"Quantization Scale: {int8_weights.q_scale()}\\n\")\n",
    "                f.write(f\"Quantized Bias:\\n{int8_bias_nd}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                # Convert param_tensor to ndarray\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                \n",
    "                f.write(f\"Layer: {layer_name}\\n\") \n",
    "                f.write(f\"Parameters:\\n{param_ndarray}\\n\")\n",
    "                f.write(\"\\n\")\n"
   ],
   "id": "dcb42f8ab80da6cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b8d3a51dac9641d",
   "metadata": {},
   "source": "### 提取六層 Encoder, Classifier, Embedding 參數"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 32-bit 浮點數(Unquantized model)",
   "id": "fe292a94fbec233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "def extract_params(layer_name, param_tensor, folder_path):\n",
    "    param_ndarray = param_tensor.detach().numpy()\n",
    "    \n",
    "    # Save parameters\n",
    "    param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "    with open(param_file, 'w') as f:\n",
    "        np.savetxt(f, param_ndarray.flatten(), fmt='%.8f')\n",
    "\n",
    "# Create the \"32float\" folder in the current directory\n",
    "float32_folder = '32float'\n",
    "os.makedirs(float32_folder, exist_ok=True)\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(1):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}'):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ce7d0fd7fa7900a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 16-bit IEEE 754 binary format(Unquantized model)",
   "id": "179952e3621e781b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:28:44.912203Z",
     "start_time": "2024-05-06T09:28:44.820058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "def float32_to_binary16(value):\n",
    "    # Convert float32 to float16\n",
    "    float16_value = np.float16(value)\n",
    "    \n",
    "    # Get the binary representation of float16\n",
    "    binary_string = np.binary_repr(float16_value.view('H'), width=16)\n",
    "    \n",
    "    return binary_string\n",
    "\n",
    "\n",
    "def extract_params(layer_name, param_tensor, folder_path):\n",
    "    param_ndarray = param_tensor.detach().numpy()\n",
    "    \n",
    "    # Save parameters as binary representations\n",
    "    param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "    with open(param_file, 'w') as f:\n",
    "        for value in param_ndarray.flatten():\n",
    "            binary_string = float32_to_binary16(value)\n",
    "            f.write(binary_string + '\\n')\n",
    "\n",
    "# Create the \"16bit_binary\" folder in the current directory\n",
    "binary_folder = '16bit_binary'\n",
    "os.makedirs(binary_folder, exist_ok=True)\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(1):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(binary_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}'):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(binary_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "bbc0ca433c25d4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 32-bit 浮點數(Quantized model)",
   "id": "20e7df3fed31e5a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    '.scale',\n",
    "    '.zero_point',\n",
    "    '._packed_params.dtype'\n",
    "]\n",
    "\n",
    "def extract_packed_params(layer_name, param_tensor, folder_path):\n",
    "    if '_packed_params._packed_params' in layer_name:\n",
    "        layer_parts = layer_name.split('.')\n",
    "        layer = model_quantized\n",
    "        for part in layer_parts[:-1]:\n",
    "            layer = getattr(layer, part)\n",
    "        \n",
    "        packed_params = getattr(layer, '_packed_params')\n",
    "        \n",
    "        int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "        int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "        int8_bias_nd = int8_bias.detach().numpy()\n",
    "        \n",
    "        # Save quantized weights\n",
    "        weights_file = os.path.join(folder_path, 'quantized_weights.txt')\n",
    "        with open(weights_file, 'w') as f:\n",
    "            for weight in int8_weights_nd.flatten():\n",
    "                f.write(f\"{weight}\\n\")\n",
    "        \n",
    "        # Save quantized biases\n",
    "        bias_file = os.path.join(folder_path, 'quantized_bias.txt')\n",
    "        with open(bias_file, 'w') as f:\n",
    "            np.savetxt(f, int8_bias_nd, fmt='%.8f')\n",
    "        \n",
    "        # Save quantization scale\n",
    "        scale_file = os.path.join(folder_path, 'quantization_scale.txt')\n",
    "        with open(scale_file, 'w') as f:\n",
    "            f.write(str(int8_weights.q_scale()))\n",
    "        \n",
    "        # Save quantization zero_point\n",
    "        zero_point_file = os.path.join(folder_path, 'quantization_zero_point.txt')\n",
    "        with open(zero_point_file, 'w') as f:\n",
    "            f.write(str(int8_weights.q_zero_point()))\n",
    "    else:\n",
    "        param_ndarray = param_tensor.detach().numpy()\n",
    "        \n",
    "        # Save regular parameters\n",
    "        param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "        with open(param_file, 'w') as f:\n",
    "            np.savetxt(f, param_ndarray.flatten(), fmt='%.8f')\n",
    "\n",
    "# Create the \"32float\" folder in the current directory\n",
    "float32_folder = '32float'\n",
    "os.makedirs(float32_folder, exist_ok=True)\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(1):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}') and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                extract_packed_params(layer_name, param_tensor, layer_folder)\n",
    "            else:\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                param_file = os.path.join(layer_folder, 'parameters.txt')\n",
    "                with open(param_file, 'w') as f:\n",
    "                    np.savetxt(f, param_ndarray.flatten(), fmt='%.8f')\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer) and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_packed_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ee1229d64b4c3fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16-bit IEEE 754 binary format(Quantized model)\n",
    "\n",
    "- 整數轉成8位元的二進制\n",
    "- 浮點數轉成16位元的IEEE 754二進制"
   ],
   "id": "d72a43aa9b0c05c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:09.180426Z",
     "start_time": "2024-05-06T09:24:09.180426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import struct\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    '.scale',\n",
    "    '.zero_point',\n",
    "    '._packed_params.dtype'\n",
    "]\n",
    "\n",
    "def float_to_bin_16bit(num):\n",
    "    # Convert float to 16-bit IEEE 754 binary format\n",
    "    binary = struct.pack('>e', num)\n",
    "    return ''.join('{:08b}'.format(b) for b in binary)\n",
    "\n",
    "def int_to_bin_8bit(num):\n",
    "    # Convert integer to 8-bit signed binary format\n",
    "    return '{:08b}'.format(num & 0xff)\n",
    "\n",
    "def extract_packed_params(layer_name, param_tensor, folder_path):\n",
    "    if '_packed_params._packed_params' in layer_name:\n",
    "        layer_parts = layer_name.split('.')\n",
    "        layer = model_quantized\n",
    "        for part in layer_parts[:-1]:\n",
    "            layer = getattr(layer, part)\n",
    "        \n",
    "        packed_params = getattr(layer, '_packed_params')\n",
    "        \n",
    "        int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "        int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "        int8_bias_nd = int8_bias.detach().numpy()\n",
    "        \n",
    "        # Save quantized weights as binary in text format\n",
    "        weights_file = os.path.join(folder_path, 'quantized_weights.txt')\n",
    "        with open(weights_file, 'w') as f:\n",
    "            for weight in int8_weights_nd.flatten():\n",
    "                f.write(int_to_bin_8bit(weight) + '\\n')\n",
    "        \n",
    "        # Save quantized biases as binary in text format\n",
    "        bias_file = os.path.join(folder_path, 'quantized_bias.txt')\n",
    "        with open(bias_file, 'w') as f:\n",
    "            for bias in int8_bias_nd.flatten():\n",
    "                f.write(float_to_bin_16bit(bias) + '\\n')\n",
    "        \n",
    "        # Save quantization scale as binary in text format\n",
    "        scale_file = os.path.join(folder_path, 'quantization_scale.txt')\n",
    "        with open(scale_file, 'w') as f:\n",
    "            f.write(float_to_bin_16bit(int8_weights.q_scale()))\n",
    "    else:\n",
    "        param_ndarray = param_tensor.detach().numpy()\n",
    "        \n",
    "        # Save regular parameters as binary in text format\n",
    "        param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "        with open(param_file, 'w') as f:\n",
    "            for param in param_ndarray.flatten():\n",
    "                if isinstance(param, np.integer):\n",
    "                    f.write(int_to_bin_8bit(param) + '\\n')\n",
    "                else:\n",
    "                    f.write(float_to_bin_16bit(param) + '\\n')\n",
    "\n",
    "# Extract encoder.0 to encoder.5 \n",
    "for i in range(1):\n",
    "    folder_name = f'binary/encoder_{i}_params'\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}') and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_name, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                extract_packed_params(layer_name, param_tensor, layer_folder)\n",
    "            else:\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                param_file = os.path.join(layer_folder, 'parameters.txt')\n",
    "                with open(param_file, 'w') as f:\n",
    "                    for param in param_ndarray.flatten():\n",
    "                        if isinstance(param, np.integer):\n",
    "                            f.write(int_to_bin_8bit(param) + '\\n')\n",
    "                        else:\n",
    "                            f.write(float_to_bin_16bit(param) + '\\n')\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'binary/{layer}_params'\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer) and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_name, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_packed_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ad9f3ef3fd02811a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
