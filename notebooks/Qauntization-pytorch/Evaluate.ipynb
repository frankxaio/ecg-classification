{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "9f24395e3322cf24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:51.313860Z",
     "start_time": "2024-05-04T16:33:51.299628Z"
    }
   },
   "source": [
    "import itertools # 是 Python 的內建模組，提供了一組用於處理迭代器的函數和工具。\n",
    "                 # 它包含了各種用於高效處理迭代器的函數，可以幫助我們編寫更簡潔、高效的代碼。\n",
    "import sys # 是 Python 的內建模組，提供了與 Python 解釋器和運行環境相關的功能。\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# sys.path 是一個列表，包含了 Python 解釋器在導入模組時會搜尋的路徑。\n",
    "# 當你使用 import 語句導入模組時 Python 會依次在 sys.path 中的路徑下尋找對應的模組文件。\n",
    "sys.path.append(\"../ecg-classification/\")\n",
    "# sys.path.append(\"C:\\\\Users\\\\Chen_Lab01\\\\Documents\\\\GitHub/ecg-classification\")\n",
    "# from IPython.display import Video\n",
    "# import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(\"ggplot\") #  是 Matplotlib 庫中用於設置繪圖樣式的函數。它使用了一種名為 \"ggplot\" 的預定義樣式\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "                        #  該樣式模仿了 R 語言的 ggplot2 繪圖包的外觀。\n",
    "# print(sys.path)\n",
    "import torch\n",
    "from ecg_tools.config import EcgConfig, Mode\n",
    "from ecg_tools.data_loader import DatasetConfig, get_data_loaders\n",
    "from ecg_tools.model import ECGformer\n",
    "from ecg_tools.train import ECGClassifierTrainer\n"
   ],
   "outputs": [],
   "execution_count": 277
  },
  {
   "cell_type": "markdown",
   "id": "adbb91b9dbfa94e3",
   "metadata": {},
   "source": "## Load model"
  },
  {
   "cell_type": "code",
   "id": "29bb12472624e7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:51.492995Z",
     "start_time": "2024-05-04T16:33:51.389590Z"
    }
   },
   "source": [
    "import torch\n",
    "config = EcgConfig()    \n",
    "model_quantized = torch.load(\"..\\\\..\\\\model_save\\\\model_quantized_98_torch.pth\")\n",
    "model = torch.load(\"..\\\\..\\\\model_save\\\\model_epoch_98.pth\")\n",
    "model_quantized.eval()\n",
    "model_quantized.to('cpu')\n",
    "model.eval()\n",
    "model.to('cpu')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECGformer(\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (0): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (queries_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (values_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (keys_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (final_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (0): Reduce('b n e -> b e', 'mean')\n",
       "    (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=192, out_features=6, bias=True)\n",
       "  )\n",
       "  (embedding): LinearEmbedding(\n",
       "    (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "    (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 279
  },
  {
   "cell_type": "markdown",
   "id": "fdc775d31a626cf5",
   "metadata": {},
   "source": "## 量化模型"
  },
  {
   "cell_type": "code",
   "id": "fe171c96df432930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:51.374099Z",
     "start_time": "2024-05-04T16:33:51.359694Z"
    }
   },
   "source": [
    "# import torch.quantization\n",
    "# \n",
    "# # 使用 Eager Mode Quantization\n",
    "# # 將 torch.nn.Linear 的參數映射到 -127~127 之間\n",
    "\n",
    "# quantized_model = torch.quantization.quantize_dynamic(\n",
    "#     model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "# )\n",
    "# \n",
    "# torch.save(quantized_model, \"..\\\\model_save\\\\model_quantized_98.pth\")"
   ],
   "outputs": [],
   "execution_count": 278
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 準確度測試",
   "id": "a041d1c91993f194"
  },
  {
   "cell_type": "code",
   "id": "780d3550eb5bc030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:55.174703Z",
     "start_time": "2024-05-04T16:33:51.508995Z"
    }
   },
   "source": [
    "import einops\n",
    "loader = get_data_loaders(DatasetConfig())\n",
    "accuracy = 0\n",
    "for signal, label in loader[Mode.train]:\n",
    "    signal.to('cpu')\n",
    "    label.to('cpu')\n",
    "    signal = einops.rearrange(signal, \"b c e -> b e c\")\n",
    "    # print(signal)\n",
    "    p = model_quantized(signal)\n",
    "    print(p)\n",
    "    print(label)\n",
    "    print(signal.shape, label.shape)\n",
    "    print(p.argmax(1) == label)\n",
    "    accuracy += torch.sum(p.argmax(1) == label)\n",
    "    print(f\"accuracy: {accuracy / config.dataset.batch_size}\")\n",
    "    break\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.7121e+00,  8.1317e-01, -2.1535e+00, -4.5101e+00, -2.4372e-01,\n",
      "         -4.4972e+00],\n",
      "        [ 5.5329e+00,  1.2125e+00, -6.6878e+00, -2.1944e+00, -1.3575e+01,\n",
      "          5.9528e+00],\n",
      "        [ 8.6056e+00,  1.4416e+00, -5.1934e+00, -3.1483e+00, -1.6442e+01,\n",
      "          2.4495e+00],\n",
      "        [ 5.2470e+00, -1.2009e+00, -4.3494e+00, -2.4378e+00, -3.9618e+00,\n",
      "          1.3853e-01],\n",
      "        [ 6.2940e+00, -1.9455e-01, -3.6662e+00, -4.9141e+00, -3.8721e+00,\n",
      "         -9.5588e-02],\n",
      "        [ 5.2444e+00, -5.3966e-01, -4.4465e+00, -3.9307e+00, -5.1170e+00,\n",
      "          2.3582e+00],\n",
      "        [ 6.3309e+00, -3.3984e-01, -4.0722e+00, -4.1203e+00, -5.1247e+00,\n",
      "         -1.3886e-01],\n",
      "        [ 3.6990e+00, -2.2362e+00, -1.7755e+00, -2.9430e+00, -3.9126e+00,\n",
      "          1.6458e+00],\n",
      "        [ 7.4732e+00, -1.4786e+00, -7.2334e+00, -1.5093e+00, -5.7579e+00,\n",
      "          7.7000e-01],\n",
      "        [ 4.2240e+00, -6.6191e-01, -2.4108e+00, -1.6537e+00,  5.4606e-01,\n",
      "         -4.0633e+00],\n",
      "        [ 6.7336e+00,  2.9788e+00, -5.1943e-01, -5.3454e+00, -1.2079e+01,\n",
      "         -2.6639e+00],\n",
      "        [ 4.1280e+00, -2.2211e+00, -1.0246e+00, -2.4348e+00, -1.0140e+00,\n",
      "         -2.1984e+00],\n",
      "        [-6.3126e-01, -2.6422e+00,  3.9572e+00, -6.8942e-01, -5.5113e+00,\n",
      "          1.5088e+00],\n",
      "        [ 7.1837e+00,  2.9442e+00, -5.4771e+00, -6.4777e+00, -8.9587e+00,\n",
      "          1.1531e+00],\n",
      "        [ 8.8054e+00, -3.7825e-02, -4.9819e+00, -3.5465e+00, -1.2440e+01,\n",
      "          6.2928e-01],\n",
      "        [ 7.9342e+00,  9.5142e-01, -3.6179e+00,  1.4520e+00, -1.4354e+01,\n",
      "         -2.7671e+00],\n",
      "        [ 3.1013e+00, -9.2171e-01,  2.9849e-01, -1.9536e+00, -3.3432e+00,\n",
      "         -2.5272e+00],\n",
      "        [ 8.0570e+00, -3.0273e-01, -3.8967e+00, -1.2081e+00, -1.3567e+01,\n",
      "         -3.8635e-01],\n",
      "        [ 5.9367e+00, -2.3577e+00, -5.9640e+00, -2.1414e+00, -2.4753e+00,\n",
      "          9.5329e-01],\n",
      "        [ 1.7792e+00, -1.9931e+00, -1.8068e+00, -1.8241e+00,  4.8348e+00,\n",
      "         -1.5508e+00],\n",
      "        [ 4.1067e+00, -1.6146e+00, -1.4501e+00, -3.7912e+00, -2.8207e-01,\n",
      "         -1.3077e+00],\n",
      "        [ 7.9546e+00,  3.2540e-01, -8.0181e+00,  2.8276e-01, -1.4313e+01,\n",
      "          2.7087e+00],\n",
      "        [ 6.8315e+00,  1.1075e-02, -3.8546e+00, -3.5993e+00, -5.9829e+00,\n",
      "         -1.9794e+00],\n",
      "        [ 8.2841e+00, -1.0362e+00, -7.5203e+00, -7.6593e-01, -9.5056e+00,\n",
      "          1.5320e+00],\n",
      "        [ 5.0398e+00, -2.1669e+00, -2.5322e+00, -3.9173e+00,  1.3805e+00,\n",
      "         -1.7619e+00],\n",
      "        [ 1.3773e+00, -1.8828e+00, -3.8729e+00, -1.8690e+00, -5.6341e+00,\n",
      "          8.2423e+00],\n",
      "        [ 2.3557e+00, -1.2074e+00, -2.0615e+00, -5.3602e+00,  9.0414e+00,\n",
      "         -1.4759e+00],\n",
      "        [ 5.7269e+00,  3.4669e-01, -2.7324e+00, -3.9690e-01, -7.6641e+00,\n",
      "         -3.0786e+00],\n",
      "        [-9.6124e-01, -2.3614e+00,  8.1282e+00,  5.0082e+00, -1.0778e+01,\n",
      "         -2.9861e+00],\n",
      "        [ 8.6526e+00, -1.2327e+00, -6.6250e+00, -1.9429e+00, -1.3285e+01,\n",
      "          3.5261e+00],\n",
      "        [ 5.6265e+00, -1.3551e-02, -2.7630e+00, -2.9464e+00, -4.3567e+00,\n",
      "         -2.0806e+00],\n",
      "        [ 8.7719e+00, -6.7633e-01, -5.4711e+00, -2.3448e+00, -1.3724e+01,\n",
      "          1.7446e+00],\n",
      "        [ 7.3585e+00, -1.2995e+00, -2.2988e+00, -2.0945e+00, -9.3413e+00,\n",
      "         -2.2837e+00],\n",
      "        [-4.0018e+00, -7.3614e-01,  1.1031e+01,  4.0220e-01, -4.6999e+00,\n",
      "         -2.3471e+00],\n",
      "        [ 6.6305e+00,  1.0950e+00, -3.9079e+00, -4.6325e+00, -9.6412e+00,\n",
      "          1.3323e+00],\n",
      "        [-3.1025e-01,  1.0105e+00, -4.8673e+00, -8.0111e-02, -7.2149e+00,\n",
      "          9.8352e+00],\n",
      "        [ 4.5118e+00,  1.7255e-01, -3.3445e+00, -5.8260e+00, -4.4417e+00,\n",
      "          3.1882e+00],\n",
      "        [ 1.2287e+00,  4.4653e+00, -4.5730e+00, -5.7132e+00,  6.8228e+00,\n",
      "         -1.3010e+00],\n",
      "        [ 4.6071e+00,  9.8044e-01, -5.4372e+00, -2.9777e+00, -6.5414e+00,\n",
      "          2.2057e+00],\n",
      "        [ 4.6866e+00,  8.8282e-01,  2.8741e-01, -2.7831e+00, -8.8358e+00,\n",
      "         -1.9604e+00],\n",
      "        [ 7.5063e+00,  1.6374e+00, -4.2375e+00, -3.0999e+00, -1.2208e+01,\n",
      "         -9.4341e-01],\n",
      "        [ 3.1493e+00,  6.5380e-01, -2.9945e+00, -2.4547e+00, -5.4086e+00,\n",
      "          1.5403e+00],\n",
      "        [ 7.5214e+00,  4.7228e-01, -3.7127e+00, -4.0111e+00, -5.1994e+00,\n",
      "         -3.0401e+00],\n",
      "        [ 4.7980e+00,  1.9964e-01, -2.6006e+00,  1.0757e-01, -9.6920e+00,\n",
      "         -9.5625e-01],\n",
      "        [ 4.0056e+00,  1.3185e+00, -3.5373e+00, -4.3463e+00, -6.1458e+00,\n",
      "          2.1404e+00],\n",
      "        [ 1.6246e+00, -1.9064e+00, -4.3682e+00, -2.8463e+00, -2.6201e-01,\n",
      "          5.2230e+00],\n",
      "        [ 2.8344e+00, -3.0096e+00, -3.9392e+00, -3.3084e+00, -5.3877e+00,\n",
      "          7.5019e+00],\n",
      "        [-3.0958e+00, -4.3869e-01,  6.8320e+00, -1.3605e+00, -3.0687e-01,\n",
      "         -1.3031e+00],\n",
      "        [-3.1473e+00,  3.4638e+00,  1.1021e+01, -1.6998e+00, -1.5279e+00,\n",
      "         -8.0745e+00],\n",
      "        [ 8.7479e+00,  2.2373e-01, -3.4917e+00, -1.0961e+00, -1.1844e+01,\n",
      "         -3.6800e+00],\n",
      "        [ 6.4417e+00,  1.2814e+00, -6.9342e+00, -3.3295e+00, -3.5339e+00,\n",
      "         -7.4183e-01],\n",
      "        [ 2.2619e+00,  5.7271e-01, -2.3113e+00, -3.5001e+00, -4.0382e+00,\n",
      "          2.2187e+00],\n",
      "        [ 3.6393e+00,  8.3656e-01, -4.7272e+00, -1.1761e+00, -9.6215e+00,\n",
      "          3.9256e+00],\n",
      "        [ 4.7714e+00, -1.5990e-01, -3.4105e+00,  2.3897e-01, -8.0270e+00,\n",
      "         -7.2336e-01],\n",
      "        [ 9.1752e-01, -1.4916e+00, -1.3972e+00, -4.1523e+00,  9.7789e+00,\n",
      "         -1.1323e+00],\n",
      "        [ 8.4695e+00,  1.3593e+00, -8.4512e+00, -8.2556e-01, -1.0401e+01,\n",
      "          6.1137e-02],\n",
      "        [ 7.7777e+00, -4.2304e-01, -4.5587e+00, -9.3251e-01, -1.2047e+01,\n",
      "         -6.2328e-01],\n",
      "        [-5.5949e-01,  1.2058e+00,  9.9276e-01, -6.0859e+00,  1.3511e+01,\n",
      "         -3.5722e+00],\n",
      "        [ 6.3724e+00, -2.2295e+00,  2.2400e+00, -4.6460e+00, -8.0472e+00,\n",
      "         -2.8681e+00],\n",
      "        [ 4.8274e+00,  6.4371e-02, -5.4607e+00, -3.3625e+00, -3.6658e+00,\n",
      "          1.7156e+00],\n",
      "        [ 3.2139e+00, -7.4001e-01, -2.7841e+00, -2.0292e+00, -3.2046e+00,\n",
      "          1.0346e+00],\n",
      "        [ 6.3552e+00,  1.6497e+00, -3.3125e+00, -3.3337e+00, -8.5385e+00,\n",
      "         -1.9772e+00],\n",
      "        [ 6.4810e+00, -1.1831e+00, -2.9778e+00, -4.2552e+00, -6.4770e+00,\n",
      "         -1.9180e-01],\n",
      "        [ 2.8823e+00, -5.2647e-01, -4.3538e+00, -2.5676e+00,  3.7210e-01,\n",
      "          1.3176e+00],\n",
      "        [ 6.9793e+00,  5.1045e-01, -2.1586e+00, -4.8812e+00, -5.8230e+00,\n",
      "         -3.0922e+00],\n",
      "        [ 5.9017e+00, -4.3461e+00, -1.9124e+00, -1.8322e+00, -1.3142e+00,\n",
      "         -2.3274e+00],\n",
      "        [ 2.5986e+00, -1.4297e+00, -1.7372e+00, -2.4815e+00, -1.7165e-02,\n",
      "         -3.8494e-01],\n",
      "        [ 6.5351e+00, -3.3280e+00, -3.7566e+00, -1.3554e+00, -3.2048e+00,\n",
      "         -1.5552e+00],\n",
      "        [ 8.3782e+00,  1.8902e+00, -5.9634e+00, -3.3251e+00, -1.2119e+01,\n",
      "          4.0273e-01],\n",
      "        [ 4.4483e+00,  4.1255e+00, -2.4518e+00, -3.3171e+00, -1.2927e+01,\n",
      "          1.0951e+00],\n",
      "        [ 6.6294e+00,  1.0884e+00, -4.1359e+00, -3.6084e+00, -1.1945e+01,\n",
      "          1.1309e+00],\n",
      "        [ 1.1858e+00, -2.0703e+00, -1.6837e+00, -3.8176e+00,  4.1103e+00,\n",
      "          2.1124e+00],\n",
      "        [ 3.0763e+00, -1.9834e+00, -4.2655e-01, -2.4625e+00,  1.0484e+00,\n",
      "         -2.5114e+00],\n",
      "        [ 4.6064e+00,  3.9230e+00, -1.8717e+00, -7.5207e+00, -2.2618e+00,\n",
      "         -2.0312e+00],\n",
      "        [ 5.9315e+00, -1.4378e+00, -1.0250e+00, -2.7694e+00, -2.1457e+00,\n",
      "         -4.3794e+00],\n",
      "        [ 2.5365e+00,  3.5441e+00, -3.4323e+00, -5.0605e+00, -9.2954e+00,\n",
      "          5.6000e+00],\n",
      "        [ 1.1742e+00,  7.9118e-01,  7.0323e-01, -6.0027e+00,  1.3156e+01,\n",
      "         -5.8133e+00],\n",
      "        [-3.1464e-01, -1.0394e+00,  6.4517e+00, -2.7931e+00, -6.8374e+00,\n",
      "         -2.6744e-01],\n",
      "        [ 8.2708e-02,  4.3964e+00,  2.4019e+00, -2.1520e+00, -3.2392e+00,\n",
      "         -4.5565e+00],\n",
      "        [ 1.7857e-01, -1.8095e+00,  1.4548e+00, -2.1028e+00,  7.4467e+00,\n",
      "         -3.9205e+00],\n",
      "        [ 2.3504e+00, -4.9322e-01,  4.9110e-01, -2.1838e+00, -1.0110e+00,\n",
      "         -3.2097e+00],\n",
      "        [ 9.0274e+00,  1.9312e+00, -5.2426e+00, -2.7339e+00, -1.3820e+01,\n",
      "         -1.2240e+00],\n",
      "        [-3.6379e+00, -1.3979e+00,  9.1690e+00,  5.7898e-01, -6.2546e-02,\n",
      "         -4.0608e+00],\n",
      "        [ 6.4314e+00,  4.1105e+00, -4.5740e+00, -6.3820e+00, -4.5356e+00,\n",
      "         -2.6931e+00],\n",
      "        [ 6.8465e+00,  2.4572e-01, -7.5229e+00, -4.0494e+00, -5.4666e+00,\n",
      "          2.0528e+00],\n",
      "        [ 7.8746e+00, -9.1473e-02, -3.6631e+00, -4.0162e+00, -7.6747e+00,\n",
      "         -2.2160e+00],\n",
      "        [ 5.6826e+00, -5.0554e-01, -2.1766e+00, -2.2829e+00, -6.6626e+00,\n",
      "         -1.9511e+00],\n",
      "        [ 3.5744e+00, -9.0922e-01, -3.3269e+00, -1.8248e+00, -6.2019e+00,\n",
      "          3.0225e+00],\n",
      "        [ 1.2375e+00,  1.9471e-01, -4.1037e-01, -4.9491e+00,  9.8452e+00,\n",
      "         -3.8677e+00],\n",
      "        [ 9.3593e+00,  5.5178e-01, -5.4396e+00, -3.9914e+00, -1.0003e+01,\n",
      "         -1.3694e+00],\n",
      "        [ 3.3213e+00,  1.1688e+00, -3.4432e+00, -4.5643e+00, -1.9201e-01,\n",
      "         -3.5152e-01],\n",
      "        [ 6.6331e+00, -2.4415e-01, -6.2672e+00, -1.2896e+00, -1.2717e+01,\n",
      "          3.4199e+00],\n",
      "        [ 5.9223e+00,  2.5561e+00, -2.8981e+00, -5.7088e+00, -9.6730e+00,\n",
      "          9.2972e-01],\n",
      "        [ 2.0496e-01,  4.6430e-02,  7.0781e-01, -6.1324e+00,  1.2072e+01,\n",
      "         -3.2938e+00],\n",
      "        [ 8.2765e+00,  5.2039e+00, -7.3824e+00, -4.0965e+00, -1.0340e+01,\n",
      "         -1.5939e+00],\n",
      "        [ 2.2204e+00, -6.7440e-01,  6.6779e+00, -1.1564e-01, -9.5030e-01,\n",
      "         -1.0167e+01],\n",
      "        [ 5.3072e+00,  2.4675e+00, -5.1935e+00, -1.3689e+00, -9.8507e+00,\n",
      "          2.9332e-01],\n",
      "        [ 5.2998e+00, -2.0021e+00,  2.6524e+00, -3.8760e+00, -9.1183e+00,\n",
      "         -2.0609e+00],\n",
      "        [ 4.7570e+00, -3.5327e+00,  3.9172e-01, -1.7518e+00, -3.8382e+00,\n",
      "         -2.1960e+00],\n",
      "        [ 4.8080e+00, -8.9304e-01, -2.2888e+00, -3.3653e+00, -5.5532e+00,\n",
      "          4.4829e-01],\n",
      "        [-3.5141e-01,  7.3718e-01,  1.4832e+00, -6.3653e+00,  1.2279e+01,\n",
      "         -2.7852e+00],\n",
      "        [ 3.9813e+00, -2.7904e+00, -3.8733e+00, -1.3504e+00, -1.8256e+00,\n",
      "          1.8843e+00],\n",
      "        [ 6.9580e+00, -9.5003e-01, -2.3614e+00, -3.2642e+00, -8.4929e+00,\n",
      "         -1.5733e+00],\n",
      "        [ 7.5295e+00, -7.8457e-02, -2.5438e+00, -3.9027e+00, -7.0407e+00,\n",
      "         -2.2982e+00],\n",
      "        [ 2.7268e-01, -9.1573e-01, -1.1198e+00, -4.6383e+00,  1.0461e+01,\n",
      "         -9.7384e-01],\n",
      "        [ 7.6097e-01, -1.0971e+00, -2.7335e-01, -6.2460e+00,  1.1853e+01,\n",
      "         -6.6321e-01],\n",
      "        [ 5.0374e+00, -1.8350e+00, -2.3753e+00, -1.8672e+00, -7.2606e+00,\n",
      "          6.9296e-01],\n",
      "        [ 5.9886e+00, -1.0577e+00, -1.5421e+00, -2.2333e+00, -6.9962e+00,\n",
      "         -2.4081e+00],\n",
      "        [ 8.5801e+00,  1.9254e+00, -5.0785e+00, -3.9309e+00, -1.1317e+01,\n",
      "         -1.6481e+00],\n",
      "        [ 4.5725e+00, -2.0793e+00, -1.2689e+00, -2.8691e+00, -2.3580e+00,\n",
      "         -1.5103e+00],\n",
      "        [ 4.2705e+00, -3.4307e-02, -1.4661e+00, -2.7432e+00, -6.0415e+00,\n",
      "         -1.3158e+00],\n",
      "        [-3.1462e+00, -1.1889e+00,  1.0060e+01,  3.1766e+00, -6.8279e+00,\n",
      "         -4.1945e+00],\n",
      "        [ 1.1196e+00, -5.9108e-02, -1.9641e+00, -4.8708e+00,  7.9844e+00,\n",
      "         -6.5318e-01],\n",
      "        [ 1.1882e+00,  1.2862e+00, -3.0888e-01, -4.3460e+00, -6.6365e+00,\n",
      "          3.9720e+00],\n",
      "        [ 1.9052e+00,  1.5305e+00,  1.1368e+00, -2.0876e+00, -5.7957e+00,\n",
      "         -1.9999e+00],\n",
      "        [ 3.7678e+00, -8.4572e-01, -1.7796e+00, -2.2633e+00, -3.2954e+00,\n",
      "         -1.0118e+00],\n",
      "        [ 6.9058e+00,  2.4875e+00, -4.7415e+00, -1.2672e+00, -1.5089e+01,\n",
      "          7.5716e-01],\n",
      "        [ 3.5788e+00, -1.6202e+00, -1.7363e+00, -4.3880e+00,  7.8439e+00,\n",
      "         -4.2235e+00],\n",
      "        [ 7.0992e+00,  1.3880e+00, -4.1234e+00, -5.0000e+00, -8.7290e+00,\n",
      "         -2.1889e-01],\n",
      "        [ 4.9032e+00,  2.1449e+00, -3.0163e+00, -2.8218e+00, -9.2745e+00,\n",
      "         -5.4342e-01],\n",
      "        [ 6.7511e+00, -1.0660e-01, -5.5163e+00,  1.0673e+00, -2.9861e+00,\n",
      "         -4.3632e+00],\n",
      "        [ 5.3234e+00, -3.3984e-01, -5.3905e+00, -1.9564e+00, -5.2644e+00,\n",
      "          6.2594e-01],\n",
      "        [ 5.4245e+00,  2.9107e+00, -2.0357e+00, -3.2725e+00, -1.3215e+01,\n",
      "         -6.0604e-01],\n",
      "        [ 3.6432e+00, -2.3815e+00, -1.5125e+00,  2.8118e-01, -4.7539e+00,\n",
      "         -7.7860e-01],\n",
      "        [ 4.1808e+00, -3.7069e+00, -2.3350e+00, -5.4466e-01, -1.5255e+00,\n",
      "         -3.3006e-01],\n",
      "        [ 5.4059e+00, -1.8459e+00, -4.1332e+00, -3.4779e+00,  1.3678e+00,\n",
      "         -1.2662e+00],\n",
      "        [ 5.4180e+00, -3.4722e+00, -2.2211e+00, -2.3905e+00, -2.8213e+00,\n",
      "         -7.5995e-01],\n",
      "        [ 5.9088e+00,  3.9670e+00, -6.1230e+00, -5.5845e+00, -4.5875e+00,\n",
      "         -8.8730e-01]], grad_fn=<WarnNotImplemented>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0,\n",
      "        0, 5, 4, 0, 2, 0, 0, 0, 0, 2, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 5, 5, 5, 2,\n",
      "        2, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,\n",
      "        0, 0, 0, 5, 4, 2, 1, 4, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 2,\n",
      "        0, 0, 0, 0, 4, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 2, 4, 5, 0, 0, 0, 4, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([128, 187, 1]) torch.Size([128])\n",
      "tensor([ True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True])\n",
      "accuracy: 0.9765625\n"
     ]
    }
   ],
   "execution_count": 281
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## View Prameter",
   "id": "a8b7f2cf319ea7bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model layer",
   "id": "a64d994b3dd451d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:34:56.542357Z",
     "start_time": "2024-05-04T16:34:56.528056Z"
    }
   },
   "cell_type": "code",
   "source": "print(model_quantized)",
   "id": "c5ba9693145ab0eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGformer(\n",
      "  (encoder): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (0): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): MultiHeadAttention(\n",
      "            (queries_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (values_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (keys_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (final_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): MLP(\n",
      "            (0): DynamicQuantizedLinear(in_features=192, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): DynamicQuantizedLinear(in_features=768, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (0): Reduce('b n e -> b e', 'mean')\n",
      "    (1): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): DynamicQuantizedLinear(in_features=192, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (embedding): LinearEmbedding(\n",
      "    (0): DynamicQuantizedLinear(in_features=1, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 292
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "字典形式的量化模型參數",
   "id": "a724a3db99f7bd99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:55.234075Z",
     "start_time": "2024-05-04T16:33:55.205332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 量化\n",
    "for param_name, param_tensor in model_quantized.state_dict().items():\n",
    "    print(f\"{param_name}\")"
   ],
   "id": "f3513f473313dc22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding\n",
      "encoder.0.0.block.0.weight\n",
      "encoder.0.0.block.0.bias\n",
      "encoder.0.0.block.1.queries_projection.scale\n",
      "encoder.0.0.block.1.queries_projection.zero_point\n",
      "encoder.0.0.block.1.queries_projection._packed_params.dtype\n",
      "encoder.0.0.block.1.queries_projection._packed_params._packed_params\n",
      "encoder.0.0.block.1.values_projection.scale\n",
      "encoder.0.0.block.1.values_projection.zero_point\n",
      "encoder.0.0.block.1.values_projection._packed_params.dtype\n",
      "encoder.0.0.block.1.values_projection._packed_params._packed_params\n",
      "encoder.0.0.block.1.keys_projection.scale\n",
      "encoder.0.0.block.1.keys_projection.zero_point\n",
      "encoder.0.0.block.1.keys_projection._packed_params.dtype\n",
      "encoder.0.0.block.1.keys_projection._packed_params._packed_params\n",
      "encoder.0.0.block.1.final_projection.scale\n",
      "encoder.0.0.block.1.final_projection.zero_point\n",
      "encoder.0.0.block.1.final_projection._packed_params.dtype\n",
      "encoder.0.0.block.1.final_projection._packed_params._packed_params\n",
      "encoder.0.1.block.0.weight\n",
      "encoder.0.1.block.0.bias\n",
      "encoder.0.1.block.1.0.scale\n",
      "encoder.0.1.block.1.0.zero_point\n",
      "encoder.0.1.block.1.0._packed_params.dtype\n",
      "encoder.0.1.block.1.0._packed_params._packed_params\n",
      "encoder.0.1.block.1.2.scale\n",
      "encoder.0.1.block.1.2.zero_point\n",
      "encoder.0.1.block.1.2._packed_params.dtype\n",
      "encoder.0.1.block.1.2._packed_params._packed_params\n",
      "encoder.1.0.block.0.weight\n",
      "encoder.1.0.block.0.bias\n",
      "encoder.1.0.block.1.queries_projection.scale\n",
      "encoder.1.0.block.1.queries_projection.zero_point\n",
      "encoder.1.0.block.1.queries_projection._packed_params.dtype\n",
      "encoder.1.0.block.1.queries_projection._packed_params._packed_params\n",
      "encoder.1.0.block.1.values_projection.scale\n",
      "encoder.1.0.block.1.values_projection.zero_point\n",
      "encoder.1.0.block.1.values_projection._packed_params.dtype\n",
      "encoder.1.0.block.1.values_projection._packed_params._packed_params\n",
      "encoder.1.0.block.1.keys_projection.scale\n",
      "encoder.1.0.block.1.keys_projection.zero_point\n",
      "encoder.1.0.block.1.keys_projection._packed_params.dtype\n",
      "encoder.1.0.block.1.keys_projection._packed_params._packed_params\n",
      "encoder.1.0.block.1.final_projection.scale\n",
      "encoder.1.0.block.1.final_projection.zero_point\n",
      "encoder.1.0.block.1.final_projection._packed_params.dtype\n",
      "encoder.1.0.block.1.final_projection._packed_params._packed_params\n",
      "encoder.1.1.block.0.weight\n",
      "encoder.1.1.block.0.bias\n",
      "encoder.1.1.block.1.0.scale\n",
      "encoder.1.1.block.1.0.zero_point\n",
      "encoder.1.1.block.1.0._packed_params.dtype\n",
      "encoder.1.1.block.1.0._packed_params._packed_params\n",
      "encoder.1.1.block.1.2.scale\n",
      "encoder.1.1.block.1.2.zero_point\n",
      "encoder.1.1.block.1.2._packed_params.dtype\n",
      "encoder.1.1.block.1.2._packed_params._packed_params\n",
      "encoder.2.0.block.0.weight\n",
      "encoder.2.0.block.0.bias\n",
      "encoder.2.0.block.1.queries_projection.scale\n",
      "encoder.2.0.block.1.queries_projection.zero_point\n",
      "encoder.2.0.block.1.queries_projection._packed_params.dtype\n",
      "encoder.2.0.block.1.queries_projection._packed_params._packed_params\n",
      "encoder.2.0.block.1.values_projection.scale\n",
      "encoder.2.0.block.1.values_projection.zero_point\n",
      "encoder.2.0.block.1.values_projection._packed_params.dtype\n",
      "encoder.2.0.block.1.values_projection._packed_params._packed_params\n",
      "encoder.2.0.block.1.keys_projection.scale\n",
      "encoder.2.0.block.1.keys_projection.zero_point\n",
      "encoder.2.0.block.1.keys_projection._packed_params.dtype\n",
      "encoder.2.0.block.1.keys_projection._packed_params._packed_params\n",
      "encoder.2.0.block.1.final_projection.scale\n",
      "encoder.2.0.block.1.final_projection.zero_point\n",
      "encoder.2.0.block.1.final_projection._packed_params.dtype\n",
      "encoder.2.0.block.1.final_projection._packed_params._packed_params\n",
      "encoder.2.1.block.0.weight\n",
      "encoder.2.1.block.0.bias\n",
      "encoder.2.1.block.1.0.scale\n",
      "encoder.2.1.block.1.0.zero_point\n",
      "encoder.2.1.block.1.0._packed_params.dtype\n",
      "encoder.2.1.block.1.0._packed_params._packed_params\n",
      "encoder.2.1.block.1.2.scale\n",
      "encoder.2.1.block.1.2.zero_point\n",
      "encoder.2.1.block.1.2._packed_params.dtype\n",
      "encoder.2.1.block.1.2._packed_params._packed_params\n",
      "encoder.3.0.block.0.weight\n",
      "encoder.3.0.block.0.bias\n",
      "encoder.3.0.block.1.queries_projection.scale\n",
      "encoder.3.0.block.1.queries_projection.zero_point\n",
      "encoder.3.0.block.1.queries_projection._packed_params.dtype\n",
      "encoder.3.0.block.1.queries_projection._packed_params._packed_params\n",
      "encoder.3.0.block.1.values_projection.scale\n",
      "encoder.3.0.block.1.values_projection.zero_point\n",
      "encoder.3.0.block.1.values_projection._packed_params.dtype\n",
      "encoder.3.0.block.1.values_projection._packed_params._packed_params\n",
      "encoder.3.0.block.1.keys_projection.scale\n",
      "encoder.3.0.block.1.keys_projection.zero_point\n",
      "encoder.3.0.block.1.keys_projection._packed_params.dtype\n",
      "encoder.3.0.block.1.keys_projection._packed_params._packed_params\n",
      "encoder.3.0.block.1.final_projection.scale\n",
      "encoder.3.0.block.1.final_projection.zero_point\n",
      "encoder.3.0.block.1.final_projection._packed_params.dtype\n",
      "encoder.3.0.block.1.final_projection._packed_params._packed_params\n",
      "encoder.3.1.block.0.weight\n",
      "encoder.3.1.block.0.bias\n",
      "encoder.3.1.block.1.0.scale\n",
      "encoder.3.1.block.1.0.zero_point\n",
      "encoder.3.1.block.1.0._packed_params.dtype\n",
      "encoder.3.1.block.1.0._packed_params._packed_params\n",
      "encoder.3.1.block.1.2.scale\n",
      "encoder.3.1.block.1.2.zero_point\n",
      "encoder.3.1.block.1.2._packed_params.dtype\n",
      "encoder.3.1.block.1.2._packed_params._packed_params\n",
      "encoder.4.0.block.0.weight\n",
      "encoder.4.0.block.0.bias\n",
      "encoder.4.0.block.1.queries_projection.scale\n",
      "encoder.4.0.block.1.queries_projection.zero_point\n",
      "encoder.4.0.block.1.queries_projection._packed_params.dtype\n",
      "encoder.4.0.block.1.queries_projection._packed_params._packed_params\n",
      "encoder.4.0.block.1.values_projection.scale\n",
      "encoder.4.0.block.1.values_projection.zero_point\n",
      "encoder.4.0.block.1.values_projection._packed_params.dtype\n",
      "encoder.4.0.block.1.values_projection._packed_params._packed_params\n",
      "encoder.4.0.block.1.keys_projection.scale\n",
      "encoder.4.0.block.1.keys_projection.zero_point\n",
      "encoder.4.0.block.1.keys_projection._packed_params.dtype\n",
      "encoder.4.0.block.1.keys_projection._packed_params._packed_params\n",
      "encoder.4.0.block.1.final_projection.scale\n",
      "encoder.4.0.block.1.final_projection.zero_point\n",
      "encoder.4.0.block.1.final_projection._packed_params.dtype\n",
      "encoder.4.0.block.1.final_projection._packed_params._packed_params\n",
      "encoder.4.1.block.0.weight\n",
      "encoder.4.1.block.0.bias\n",
      "encoder.4.1.block.1.0.scale\n",
      "encoder.4.1.block.1.0.zero_point\n",
      "encoder.4.1.block.1.0._packed_params.dtype\n",
      "encoder.4.1.block.1.0._packed_params._packed_params\n",
      "encoder.4.1.block.1.2.scale\n",
      "encoder.4.1.block.1.2.zero_point\n",
      "encoder.4.1.block.1.2._packed_params.dtype\n",
      "encoder.4.1.block.1.2._packed_params._packed_params\n",
      "encoder.5.0.block.0.weight\n",
      "encoder.5.0.block.0.bias\n",
      "encoder.5.0.block.1.queries_projection.scale\n",
      "encoder.5.0.block.1.queries_projection.zero_point\n",
      "encoder.5.0.block.1.queries_projection._packed_params.dtype\n",
      "encoder.5.0.block.1.queries_projection._packed_params._packed_params\n",
      "encoder.5.0.block.1.values_projection.scale\n",
      "encoder.5.0.block.1.values_projection.zero_point\n",
      "encoder.5.0.block.1.values_projection._packed_params.dtype\n",
      "encoder.5.0.block.1.values_projection._packed_params._packed_params\n",
      "encoder.5.0.block.1.keys_projection.scale\n",
      "encoder.5.0.block.1.keys_projection.zero_point\n",
      "encoder.5.0.block.1.keys_projection._packed_params.dtype\n",
      "encoder.5.0.block.1.keys_projection._packed_params._packed_params\n",
      "encoder.5.0.block.1.final_projection.scale\n",
      "encoder.5.0.block.1.final_projection.zero_point\n",
      "encoder.5.0.block.1.final_projection._packed_params.dtype\n",
      "encoder.5.0.block.1.final_projection._packed_params._packed_params\n",
      "encoder.5.1.block.0.weight\n",
      "encoder.5.1.block.0.bias\n",
      "encoder.5.1.block.1.0.scale\n",
      "encoder.5.1.block.1.0.zero_point\n",
      "encoder.5.1.block.1.0._packed_params.dtype\n",
      "encoder.5.1.block.1.0._packed_params._packed_params\n",
      "encoder.5.1.block.1.2.scale\n",
      "encoder.5.1.block.1.2.zero_point\n",
      "encoder.5.1.block.1.2._packed_params.dtype\n",
      "encoder.5.1.block.1.2._packed_params._packed_params\n",
      "classifier.1.scale\n",
      "classifier.1.zero_point\n",
      "classifier.1._packed_params.dtype\n",
      "classifier.1._packed_params._packed_params\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n",
      "classifier.3.scale\n",
      "classifier.3.zero_point\n",
      "classifier.3._packed_params.dtype\n",
      "classifier.3._packed_params._packed_params\n",
      "embedding.cls_token\n",
      "embedding.0.scale\n",
      "embedding.0.zero_point\n",
      "embedding.0._packed_params.dtype\n",
      "embedding.0._packed_params._packed_params\n",
      "embedding.1.weight\n",
      "embedding.1.bias\n"
     ]
    }
   ],
   "execution_count": 283
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "字典形式的非量化模型參數",
   "id": "d308ce0f9be7ec25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:55.189420Z",
     "start_time": "2024-05-04T16:33:55.177322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 未量化\n",
    "for param_name, param_tensor in model.state_dict().items():\n",
    "    print(f\"{param_name}\")"
   ],
   "id": "f4dd095dd96a7ee0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding\n",
      "encoder.0.0.block.0.weight\n",
      "encoder.0.0.block.0.bias\n",
      "encoder.0.0.block.1.queries_projection.weight\n",
      "encoder.0.0.block.1.queries_projection.bias\n",
      "encoder.0.0.block.1.values_projection.weight\n",
      "encoder.0.0.block.1.values_projection.bias\n",
      "encoder.0.0.block.1.keys_projection.weight\n",
      "encoder.0.0.block.1.keys_projection.bias\n",
      "encoder.0.0.block.1.final_projection.weight\n",
      "encoder.0.0.block.1.final_projection.bias\n",
      "encoder.0.1.block.0.weight\n",
      "encoder.0.1.block.0.bias\n",
      "encoder.0.1.block.1.0.weight\n",
      "encoder.0.1.block.1.0.bias\n",
      "encoder.0.1.block.1.2.weight\n",
      "encoder.0.1.block.1.2.bias\n",
      "encoder.1.0.block.0.weight\n",
      "encoder.1.0.block.0.bias\n",
      "encoder.1.0.block.1.queries_projection.weight\n",
      "encoder.1.0.block.1.queries_projection.bias\n",
      "encoder.1.0.block.1.values_projection.weight\n",
      "encoder.1.0.block.1.values_projection.bias\n",
      "encoder.1.0.block.1.keys_projection.weight\n",
      "encoder.1.0.block.1.keys_projection.bias\n",
      "encoder.1.0.block.1.final_projection.weight\n",
      "encoder.1.0.block.1.final_projection.bias\n",
      "encoder.1.1.block.0.weight\n",
      "encoder.1.1.block.0.bias\n",
      "encoder.1.1.block.1.0.weight\n",
      "encoder.1.1.block.1.0.bias\n",
      "encoder.1.1.block.1.2.weight\n",
      "encoder.1.1.block.1.2.bias\n",
      "encoder.2.0.block.0.weight\n",
      "encoder.2.0.block.0.bias\n",
      "encoder.2.0.block.1.queries_projection.weight\n",
      "encoder.2.0.block.1.queries_projection.bias\n",
      "encoder.2.0.block.1.values_projection.weight\n",
      "encoder.2.0.block.1.values_projection.bias\n",
      "encoder.2.0.block.1.keys_projection.weight\n",
      "encoder.2.0.block.1.keys_projection.bias\n",
      "encoder.2.0.block.1.final_projection.weight\n",
      "encoder.2.0.block.1.final_projection.bias\n",
      "encoder.2.1.block.0.weight\n",
      "encoder.2.1.block.0.bias\n",
      "encoder.2.1.block.1.0.weight\n",
      "encoder.2.1.block.1.0.bias\n",
      "encoder.2.1.block.1.2.weight\n",
      "encoder.2.1.block.1.2.bias\n",
      "encoder.3.0.block.0.weight\n",
      "encoder.3.0.block.0.bias\n",
      "encoder.3.0.block.1.queries_projection.weight\n",
      "encoder.3.0.block.1.queries_projection.bias\n",
      "encoder.3.0.block.1.values_projection.weight\n",
      "encoder.3.0.block.1.values_projection.bias\n",
      "encoder.3.0.block.1.keys_projection.weight\n",
      "encoder.3.0.block.1.keys_projection.bias\n",
      "encoder.3.0.block.1.final_projection.weight\n",
      "encoder.3.0.block.1.final_projection.bias\n",
      "encoder.3.1.block.0.weight\n",
      "encoder.3.1.block.0.bias\n",
      "encoder.3.1.block.1.0.weight\n",
      "encoder.3.1.block.1.0.bias\n",
      "encoder.3.1.block.1.2.weight\n",
      "encoder.3.1.block.1.2.bias\n",
      "encoder.4.0.block.0.weight\n",
      "encoder.4.0.block.0.bias\n",
      "encoder.4.0.block.1.queries_projection.weight\n",
      "encoder.4.0.block.1.queries_projection.bias\n",
      "encoder.4.0.block.1.values_projection.weight\n",
      "encoder.4.0.block.1.values_projection.bias\n",
      "encoder.4.0.block.1.keys_projection.weight\n",
      "encoder.4.0.block.1.keys_projection.bias\n",
      "encoder.4.0.block.1.final_projection.weight\n",
      "encoder.4.0.block.1.final_projection.bias\n",
      "encoder.4.1.block.0.weight\n",
      "encoder.4.1.block.0.bias\n",
      "encoder.4.1.block.1.0.weight\n",
      "encoder.4.1.block.1.0.bias\n",
      "encoder.4.1.block.1.2.weight\n",
      "encoder.4.1.block.1.2.bias\n",
      "encoder.5.0.block.0.weight\n",
      "encoder.5.0.block.0.bias\n",
      "encoder.5.0.block.1.queries_projection.weight\n",
      "encoder.5.0.block.1.queries_projection.bias\n",
      "encoder.5.0.block.1.values_projection.weight\n",
      "encoder.5.0.block.1.values_projection.bias\n",
      "encoder.5.0.block.1.keys_projection.weight\n",
      "encoder.5.0.block.1.keys_projection.bias\n",
      "encoder.5.0.block.1.final_projection.weight\n",
      "encoder.5.0.block.1.final_projection.bias\n",
      "encoder.5.1.block.0.weight\n",
      "encoder.5.1.block.0.bias\n",
      "encoder.5.1.block.1.0.weight\n",
      "encoder.5.1.block.1.0.bias\n",
      "encoder.5.1.block.1.2.weight\n",
      "encoder.5.1.block.1.2.bias\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n",
      "embedding.cls_token\n",
      "embedding.0.weight\n",
      "embedding.0.bias\n",
      "embedding.1.weight\n",
      "embedding.1.bias\n"
     ]
    }
   ],
   "execution_count": 282
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packed_params",
   "id": "4ab7fe9f2a642014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:55.264008Z",
     "start_time": "2024-05-04T16:33:55.235632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "weight_tensor_after = model_quantized.state_dict()['encoder.0.0.block.1.queries_projection._packed_params._packed_params']\n",
    "\n",
    "packed_params = model_quantized.encoder[0][0].block[1].queries_projection._packed_params._packed_params\n",
    "\n",
    "# Unpack the quantized weights and biases\n",
    "int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "int8_bias_nd = int8_bias.detach().numpy()\n",
    "# # Dequantize the weights and biases\n",
    "# weights = int8_weights.dequantize()\n",
    "# bias = int8_bias.dequantize()\n",
    "\n",
    "print(int8_weights_nd)#　將量化後的權重轉換為整數表示並轉化為numpy# array\n",
    "print(int8_weights.q_scale()) #　獲取量化的scale\n",
    "print(int8_bias_nd)"
   ],
   "id": "d95afa5238bac0f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-70  25  45 ...   9 -11  41]\n",
      " [ -1  -2 -10 ...  -6 -11 -24]\n",
      " [-69  75  15 ...  -9   9 -46]\n",
      " ...\n",
      " [ 42 -16  -7 ... -34 -45  27]\n",
      " [  3 -18  19 ...  11 -29  11]\n",
      " [ 33 -54 -41 ...  43 -23  35]]\n",
      "0.0016688834875822067\n",
      "[ 0.06134006  0.05202604 -0.03778487 -0.00852696 -0.04655216  0.09476199\n",
      " -0.01202581  0.02501333  0.00824877  0.00398409 -0.05725901 -0.02867818\n",
      "  0.04962902 -0.0563751  -0.03398313  0.02636591 -0.04214004  0.02425917\n",
      " -0.03342133 -0.04976596  0.07020013  0.06184619  0.00831161  0.01872784\n",
      "  0.04020224 -0.05760437 -0.00331612 -0.01521585 -0.02899045  0.07367223\n",
      " -0.00373852 -0.05436532 -0.07119824 -0.00342908  0.04767367  0.0072046\n",
      " -0.0268723   0.08635192 -0.059508   -0.04140051 -0.01219196  0.04398649\n",
      " -0.00661933  0.09846511 -0.06277379  0.05841304 -0.00993187  0.00212757\n",
      " -0.00176864 -0.01117148  0.01954278 -0.03350052 -0.06176341  0.0046997\n",
      " -0.0649123   0.00070314 -0.04784982 -0.00160643 -0.02818445 -0.05547581\n",
      "  0.02812169 -0.03888802 -0.07212877  0.02438743  0.00807508 -0.03862466\n",
      "  0.02303939  0.02343759  0.05854193 -0.01020232 -0.07036472  0.02743189\n",
      "  0.04304015  0.06840157 -0.01941688 -0.0304034  -0.00137582 -0.00900388\n",
      " -0.01233665  0.00291946 -0.05158512 -0.02442968 -0.05387415 -0.03928642\n",
      "  0.04577171  0.02957063  0.08381347  0.03623725 -0.0227721   0.07215102\n",
      "  0.01925179 -0.0115608   0.0273394   0.06647399 -0.04293759  0.01916825\n",
      "  0.04337128 -0.04333074 -0.06528896 -0.01609893 -0.02871842  0.07259724\n",
      " -0.02966251  0.03296401  0.07282845 -0.03629882 -0.04961355 -0.04133865\n",
      "  0.03049797 -0.00543388 -0.01880797 -0.00051831  0.07393742 -0.04405031\n",
      " -0.03261995  0.00458383  0.03699398 -0.0763021   0.02480612  0.06995831\n",
      "  0.00876886 -0.00659454 -0.04503281 -0.06733263 -0.05475987 -0.07913084\n",
      " -0.04287952  0.04827432 -0.07141711  0.04529127  0.03156482 -0.04063607\n",
      "  0.0506044  -0.03009742 -0.01292468  0.02221986 -0.01890882 -0.06406441\n",
      "  0.04170186  0.02139968 -0.01395813  0.09046028  0.06999844 -0.06286401\n",
      "  0.02948192  0.05491566 -0.01102406  0.0551344   0.01187254  0.03332254\n",
      "  0.06817374  0.04613569  0.03609907 -0.05669223 -0.09149918  0.10000239\n",
      " -0.05498323  0.02187265  0.05888537  0.02671108  0.06151502 -0.03718671\n",
      "  0.02367043 -0.06470986 -0.01685396  0.08590618 -0.05091292 -0.04596566\n",
      "  0.05085964  0.00573283  0.01736598 -0.04413607 -0.02628079 -0.00931518\n",
      " -0.03043624 -0.02786605  0.00992709 -0.01497354 -0.01484699  0.06916173\n",
      "  0.01569596 -0.0040952  -0.01142544  0.05837364  0.01851606  0.03273023\n",
      "  0.06663238  0.01794928  0.03488636  0.04557784 -0.03868137 -0.00595846]\n"
     ]
    }
   ],
   "execution_count": 284
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:55.293801Z",
     "start_time": "2024-05-04T16:33:55.265610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param_name, param_tensor in model_quantized.state_dict().items():\n",
    "    if isinstance(param_tensor, torch.Tensor):\n",
    "        if not any(special_param in param_name for special_param in ['scale', 'zero_point', 'dtype', '_packed_params']):\n",
    "            print(f\"{param_name}\\t{param_tensor.size()}\")"
   ],
   "id": "93d323b07c1341c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding\ttorch.Size([188, 192])\n",
      "encoder.0.0.block.0.weight\ttorch.Size([192])\n",
      "encoder.0.0.block.0.bias\ttorch.Size([192])\n",
      "encoder.0.1.block.0.weight\ttorch.Size([192])\n",
      "encoder.0.1.block.0.bias\ttorch.Size([192])\n",
      "encoder.1.0.block.0.weight\ttorch.Size([192])\n",
      "encoder.1.0.block.0.bias\ttorch.Size([192])\n",
      "encoder.1.1.block.0.weight\ttorch.Size([192])\n",
      "encoder.1.1.block.0.bias\ttorch.Size([192])\n",
      "encoder.2.0.block.0.weight\ttorch.Size([192])\n",
      "encoder.2.0.block.0.bias\ttorch.Size([192])\n",
      "encoder.2.1.block.0.weight\ttorch.Size([192])\n",
      "encoder.2.1.block.0.bias\ttorch.Size([192])\n",
      "encoder.3.0.block.0.weight\ttorch.Size([192])\n",
      "encoder.3.0.block.0.bias\ttorch.Size([192])\n",
      "encoder.3.1.block.0.weight\ttorch.Size([192])\n",
      "encoder.3.1.block.0.bias\ttorch.Size([192])\n",
      "encoder.4.0.block.0.weight\ttorch.Size([192])\n",
      "encoder.4.0.block.0.bias\ttorch.Size([192])\n",
      "encoder.4.1.block.0.weight\ttorch.Size([192])\n",
      "encoder.4.1.block.0.bias\ttorch.Size([192])\n",
      "encoder.5.0.block.0.weight\ttorch.Size([192])\n",
      "encoder.5.0.block.0.bias\ttorch.Size([192])\n",
      "encoder.5.1.block.0.weight\ttorch.Size([192])\n",
      "encoder.5.1.block.0.bias\ttorch.Size([192])\n",
      "classifier.2.weight\ttorch.Size([192])\n",
      "classifier.2.bias\ttorch.Size([192])\n",
      "embedding.cls_token\ttorch.Size([1, 192])\n",
      "embedding.1.weight\ttorch.Size([192])\n",
      "embedding.1.bias\ttorch.Size([192])\n"
     ]
    }
   ],
   "execution_count": 285
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "印出模型中所有的weights和bias",
   "id": "9ec3ae45807205c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:55.354001Z",
     "start_time": "2024-05-04T16:33:55.295466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model_quantized.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Weight: {param.data}\")\n",
    "    elif 'bias' in name:\n",
    "        print(f\"Layer: {name}\") \n",
    "        print(f\"Bias: {param.data}\")\n"
   ],
   "id": "42b88a0cb1bf2fa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: encoder.0.0.block.0.weight\n",
      "Weight: tensor([0.9750, 1.0008, 1.0394, 0.9690, 1.0381, 0.9551, 1.0361, 0.9804, 1.0251,\n",
      "        0.9770, 0.9904, 1.0317, 1.0246, 0.9874, 1.0160, 0.9927, 1.0636, 0.9876,\n",
      "        0.9794, 1.0233, 1.0472, 1.0000, 1.0550, 1.0249, 0.9639, 1.0036, 1.0061,\n",
      "        1.0274, 1.0092, 1.0298, 1.0008, 1.0026, 1.0025, 1.0162, 0.9898, 1.0281,\n",
      "        0.9993, 1.0172, 1.0028, 0.9791, 1.0002, 0.9492, 0.9808, 1.0047, 1.0121,\n",
      "        1.0756, 1.0484, 1.0530, 1.0308, 1.0548, 1.0228, 0.9818, 1.0365, 1.0565,\n",
      "        1.0072, 0.9981, 1.0169, 1.0323, 0.9796, 1.0250, 0.9712, 0.9878, 1.0699,\n",
      "        1.0638, 1.0120, 0.9914, 0.9969, 1.0180, 0.9557, 0.9869, 0.9812, 1.0278,\n",
      "        0.9913, 0.9831, 0.9790, 0.9776, 1.0698, 0.9919, 0.9821, 1.0128, 1.0208,\n",
      "        0.9713, 1.0205, 0.9782, 1.0168, 0.9990, 1.0669, 0.9685, 0.9903, 1.0564,\n",
      "        1.0028, 0.9732, 0.9900, 1.0016, 0.9764, 1.0538, 0.9700, 1.0321, 0.9853,\n",
      "        1.0275, 0.9730, 1.0576, 1.0065, 0.9865, 0.9718, 1.0318, 0.9698, 1.0009,\n",
      "        0.9997, 0.9998, 1.0457, 1.0856, 1.0419, 1.0456, 1.0426, 1.0052, 0.9880,\n",
      "        1.0245, 0.9877, 1.0324, 1.0372, 1.0039, 1.0107, 1.0121, 1.0386, 1.0081,\n",
      "        1.0086, 1.0957, 1.0536, 1.0189, 1.0088, 1.0196, 0.9778, 1.0053, 0.9693,\n",
      "        1.0646, 0.9503, 1.0302, 0.9749, 1.0263, 1.0327, 1.0324, 1.0163, 1.0140,\n",
      "        1.0167, 1.0796, 1.0235, 1.0249, 0.9832, 0.9742, 1.0106, 0.9565, 0.9845,\n",
      "        1.0191, 1.0266, 1.0431, 1.0238, 0.9800, 0.9822, 1.0213, 1.0231, 0.9399,\n",
      "        1.0111, 0.9937, 1.0118, 1.0086, 0.9801, 1.1129, 1.0025, 1.0467, 1.0208,\n",
      "        1.0143, 0.9412, 0.9968, 1.0069, 1.0380, 0.9438, 1.0153, 0.9957, 0.9649,\n",
      "        0.9793, 1.0316, 1.0387, 1.0581, 0.9927, 1.0166, 0.9882, 1.0216, 1.0175,\n",
      "        0.9699, 1.0435, 1.0296])\n",
      "Layer: encoder.0.0.block.0.bias\n",
      "Bias: tensor([-0.0099,  0.0175,  0.0103,  0.0544,  0.0166,  0.0551, -0.0056, -0.0475,\n",
      "         0.0165, -0.0376,  0.0286, -0.0341,  0.0089, -0.0023, -0.0366,  0.0168,\n",
      "         0.0178,  0.0113, -0.0455, -0.0371,  0.0378, -0.0489,  0.0080,  0.0226,\n",
      "        -0.0426,  0.0028,  0.0143, -0.0218, -0.0121,  0.0311,  0.0449, -0.0221,\n",
      "        -0.0491, -0.0084,  0.0581, -0.0353,  0.0214,  0.0234,  0.0338,  0.0184,\n",
      "        -0.0098,  0.0452,  0.0186, -0.0239, -0.0029, -0.0277, -0.0265, -0.0053,\n",
      "        -0.0056, -0.0203,  0.0013,  0.0084, -0.0211, -0.0017,  0.0089, -0.0236,\n",
      "        -0.0526,  0.0114,  0.0090,  0.0126,  0.0481, -0.0087, -0.0158,  0.0018,\n",
      "         0.0072,  0.0218,  0.0349, -0.0650,  0.0131,  0.0209,  0.0152,  0.0220,\n",
      "         0.0230,  0.0175, -0.0170,  0.0128,  0.0038, -0.0601,  0.0150,  0.0149,\n",
      "        -0.0081,  0.0282, -0.0190, -0.0743,  0.0226,  0.0216, -0.0114,  0.0103,\n",
      "         0.0239, -0.0071,  0.0069,  0.0270, -0.0171, -0.0114,  0.0192,  0.0134,\n",
      "        -0.0758, -0.0113,  0.0405,  0.0059,  0.0075,  0.0124,  0.0117,  0.0107,\n",
      "        -0.0002,  0.0083, -0.0284, -0.0610, -0.0523,  0.0102, -0.0296, -0.0267,\n",
      "        -0.0023,  0.0212, -0.0020,  0.0172, -0.0306,  0.0098,  0.0205,  0.0220,\n",
      "         0.0179,  0.0124,  0.0054, -0.0029,  0.0211,  0.0134, -0.0328,  0.0132,\n",
      "        -0.0132,  0.0205, -0.0649, -0.0160,  0.0219,  0.0268,  0.0212,  0.0089,\n",
      "         0.0598,  0.0410,  0.0189, -0.0208, -0.0115,  0.0057,  0.0106, -0.0285,\n",
      "        -0.0001, -0.0121,  0.0220,  0.0150,  0.0228, -0.0639,  0.0042,  0.0202,\n",
      "        -0.0070,  0.0355,  0.0124,  0.0082, -0.0410,  0.0208,  0.0175,  0.0074,\n",
      "        -0.0190,  0.0273,  0.0243,  0.0354,  0.0238,  0.0323,  0.0510, -0.0106,\n",
      "         0.0108, -0.0516, -0.0069, -0.0005,  0.0417,  0.0186, -0.0255,  0.0017,\n",
      "        -0.0525, -0.0008,  0.0209, -0.0207, -0.0056,  0.0190,  0.0051,  0.0082,\n",
      "         0.0186,  0.0309,  0.0127,  0.0068, -0.0250, -0.0340, -0.0517, -0.0038])\n",
      "Layer: encoder.0.1.block.0.weight\n",
      "Weight: tensor([1.0418, 1.0060, 0.9843, 1.0244, 0.9766, 0.9731, 0.9900, 1.0045, 0.9934,\n",
      "        0.9246, 0.9790, 1.0001, 1.0164, 1.0122, 1.0299, 1.0022, 0.9819, 0.9828,\n",
      "        0.9876, 1.0061, 1.0116, 0.9758, 1.0600, 1.0011, 1.0177, 0.9773, 0.9597,\n",
      "        1.0402, 1.0084, 1.0014, 0.9844, 1.0189, 0.9999, 1.0244, 0.9638, 1.0114,\n",
      "        0.9992, 0.9415, 1.0154, 1.0409, 0.9924, 0.9905, 0.9760, 1.0082, 1.0073,\n",
      "        1.0096, 1.0062, 1.0026, 1.0364, 1.0243, 1.0242, 0.9666, 1.0254, 1.0063,\n",
      "        1.0002, 0.9557, 0.9972, 1.0193, 0.9928, 1.0067, 1.0291, 1.0142, 1.0217,\n",
      "        1.0477, 1.0090, 0.9806, 0.9889, 1.0190, 0.9976, 0.9598, 0.9704, 1.0153,\n",
      "        0.9871, 0.9514, 0.9977, 1.0173, 1.0370, 0.9797, 1.0120, 0.9704, 1.0046,\n",
      "        0.9635, 1.0448, 1.0170, 0.9863, 0.9857, 1.0390, 0.9805, 1.0160, 1.0095,\n",
      "        0.9687, 0.9715, 0.9976, 1.0055, 0.9779, 1.0118, 0.9559, 0.9797, 0.9794,\n",
      "        1.0075, 0.9688, 0.9967, 1.0082, 0.9840, 0.9996, 1.0197, 0.9926, 0.9955,\n",
      "        1.0229, 0.9760, 1.0045, 1.0288, 0.9705, 0.9852, 0.9655, 0.9944, 0.9620,\n",
      "        1.0057, 0.9649, 1.0342, 1.0620, 0.9784, 1.0075, 0.9833, 1.0068, 0.9686,\n",
      "        1.0129, 1.0176, 1.0270, 1.0250, 1.0131, 1.0113, 0.9810, 1.0118, 0.9920,\n",
      "        1.0119, 1.0422, 0.9986, 1.0011, 0.9944, 1.0000, 0.9705, 0.9171, 1.0277,\n",
      "        1.0099, 1.0078, 1.0219, 1.0379, 0.9346, 0.9725, 0.9968, 0.9953, 0.9346,\n",
      "        0.9512, 1.0464, 0.9894, 1.0040, 0.9928, 1.0144, 1.0170, 1.0321, 0.9679,\n",
      "        1.0077, 1.0281, 1.0248, 1.0251, 0.9380, 1.0445, 0.9918, 1.0437, 1.0214,\n",
      "        0.9882, 1.0295, 0.9880, 1.0508, 1.0101, 0.9888, 0.9767, 0.9874, 0.9685,\n",
      "        1.0250, 1.0143, 0.9962, 1.0165, 0.9456, 0.9749, 1.0488, 1.0022, 1.0098,\n",
      "        0.9623, 1.0359, 0.9558])\n",
      "Layer: encoder.0.1.block.0.bias\n",
      "Bias: tensor([ 0.0205, -0.0077, -0.0075, -0.0099,  0.0061,  0.0484,  0.0246, -0.0096,\n",
      "        -0.0252, -0.0579,  0.0010, -0.0087, -0.0341, -0.0034,  0.0226, -0.0206,\n",
      "         0.0332, -0.0106, -0.0256, -0.0182,  0.0377, -0.0070, -0.0336,  0.0022,\n",
      "        -0.0037, -0.0141,  0.0237, -0.0114, -0.0150, -0.0226,  0.0205, -0.0101,\n",
      "        -0.0109,  0.0186, -0.0110,  0.0039,  0.0059,  0.0316,  0.0525, -0.0080,\n",
      "         0.0172,  0.0259, -0.0392, -0.0233, -0.0301, -0.0019,  0.0143,  0.0109,\n",
      "        -0.0022, -0.0114,  0.0370,  0.0198,  0.0043,  0.0061,  0.0246, -0.0153,\n",
      "        -0.0208,  0.0226,  0.0059,  0.0013,  0.0440, -0.0301,  0.0181, -0.0015,\n",
      "        -0.0197, -0.0087,  0.0260,  0.0063,  0.0023,  0.0603,  0.0093,  0.0124,\n",
      "         0.0101,  0.0285, -0.0021, -0.0150, -0.0005, -0.0056, -0.0203, -0.0255,\n",
      "        -0.0393,  0.0072,  0.0196,  0.0170,  0.0136, -0.0124,  0.0059,  0.0412,\n",
      "        -0.0156,  0.0155,  0.0103, -0.0026,  0.0015, -0.0004, -0.0462,  0.0190,\n",
      "        -0.0288, -0.0055, -0.0130, -0.0194,  0.0107,  0.0117, -0.0022, -0.0116,\n",
      "         0.0200, -0.0204, -0.0221, -0.0108, -0.0146,  0.0037,  0.0034, -0.0085,\n",
      "        -0.0329,  0.0159, -0.0194, -0.0018, -0.0502, -0.0068,  0.0003,  0.0074,\n",
      "         0.0035,  0.0130, -0.0103, -0.0241,  0.0200,  0.0086,  0.0152,  0.0023,\n",
      "         0.0123, -0.0278,  0.0173,  0.0167,  0.0618,  0.0232,  0.0274,  0.0083,\n",
      "         0.0339, -0.0020,  0.0419, -0.0569,  0.0228,  0.0303,  0.0323, -0.0174,\n",
      "         0.0016,  0.0124,  0.0039,  0.0118,  0.0254, -0.0257,  0.0132,  0.0182,\n",
      "         0.0128,  0.0383,  0.0093, -0.0090,  0.0110, -0.0200, -0.0369, -0.0105,\n",
      "         0.0148,  0.0083, -0.0299, -0.0094,  0.0043, -0.0145,  0.0160,  0.0201,\n",
      "        -0.0085, -0.0011,  0.0121,  0.0072, -0.0082,  0.0357,  0.0118,  0.0290,\n",
      "         0.0075, -0.0462,  0.0007, -0.0310, -0.0229,  0.0222,  0.0081, -0.0074,\n",
      "         0.0065,  0.0386, -0.0168, -0.0166, -0.0256, -0.0556,  0.0115, -0.0077])\n",
      "Layer: encoder.1.0.block.0.weight\n",
      "Weight: tensor([1.0161, 1.0231, 0.9826, 1.0039, 1.0581, 0.9974, 1.0265, 1.0120, 1.0098,\n",
      "        1.0004, 1.0070, 1.0387, 1.0114, 1.0275, 1.0338, 1.0315, 1.0734, 1.0083,\n",
      "        0.9813, 1.0265, 1.0171, 1.0316, 1.0424, 1.0061, 1.0116, 1.0042, 1.0110,\n",
      "        1.0697, 0.9837, 1.0320, 1.0239, 1.0531, 0.9996, 1.0433, 1.0123, 1.0268,\n",
      "        1.0513, 1.0079, 1.0205, 1.0160, 1.0132, 0.9690, 1.0056, 1.0217, 1.0385,\n",
      "        1.0267, 0.9913, 1.0171, 1.0294, 1.0621, 1.0289, 1.0332, 1.0093, 1.0363,\n",
      "        1.0138, 1.0071, 1.0193, 1.0710, 1.0434, 1.0243, 1.0262, 1.0821, 1.0374,\n",
      "        1.0362, 1.0669, 1.0127, 1.0232, 0.9810, 1.0262, 1.0304, 1.0018, 1.0526,\n",
      "        1.0157, 0.9959, 1.0096, 1.0217, 1.0842, 1.0062, 1.0342, 1.0534, 1.0193,\n",
      "        0.9937, 1.0154, 1.0441, 1.0103, 1.0114, 1.1301, 1.0217, 1.0187, 1.0270,\n",
      "        0.9955, 1.0262, 1.0033, 0.9940, 1.0163, 1.0469, 1.0049, 1.0180, 1.0043,\n",
      "        1.0170, 1.0243, 1.0135, 1.0374, 1.0354, 1.0450, 1.0780, 1.0485, 1.0240,\n",
      "        1.0112, 1.0181, 0.9876, 1.0595, 1.0302, 1.0117, 1.0392, 1.0132, 1.0485,\n",
      "        1.0010, 1.0135, 0.9909, 1.0728, 1.0114, 1.0322, 1.0227, 1.0695, 1.0364,\n",
      "        1.0500, 1.0235, 1.0362, 1.0110, 1.0441, 1.0424, 0.9942, 1.0456, 1.0169,\n",
      "        1.0465, 1.0282, 1.0208, 1.0075, 1.0244, 1.0217, 1.0301, 1.0334, 1.0507,\n",
      "        1.0406, 1.0415, 1.0365, 1.0162, 1.0004, 1.0512, 1.0222, 1.0560, 1.0208,\n",
      "        1.0022, 1.0629, 0.9957, 1.0324, 1.0552, 1.0035, 1.0346, 1.0233, 0.9962,\n",
      "        1.0264, 1.0202, 1.0412, 1.0259, 1.0064, 1.0739, 1.0398, 0.9889, 1.0388,\n",
      "        1.0368, 1.0002, 0.9850, 1.0199, 1.0288, 0.9936, 1.0549, 1.0265, 1.0428,\n",
      "        1.0554, 1.0233, 1.0740, 1.0567, 1.0159, 0.9935, 0.9823, 0.9785, 1.0223,\n",
      "        1.0236, 1.0577, 1.0401])\n",
      "Layer: encoder.1.0.block.0.bias\n",
      "Bias: tensor([ 1.9074e-02,  2.0685e-03, -1.6077e-02,  4.1630e-03, -3.4105e-03,\n",
      "        -4.0206e-03,  1.0803e-02, -1.9882e-02, -7.9523e-03, -1.3674e-02,\n",
      "         1.0176e-02, -1.0876e-02,  2.9409e-02,  2.6015e-02, -1.1198e-02,\n",
      "        -8.0522e-03, -2.2917e-02,  7.9198e-03, -3.0215e-02, -1.9141e-02,\n",
      "        -1.5296e-02, -2.5309e-03,  5.2448e-03, -2.5735e-02, -1.9260e-02,\n",
      "         6.6864e-03, -3.9883e-03, -2.3682e-03,  1.0864e-02,  1.1410e-02,\n",
      "        -2.2547e-02,  3.0410e-03, -2.3581e-02, -2.0890e-02, -1.1495e-02,\n",
      "        -2.0054e-02,  4.5477e-03,  6.6608e-03, -3.6667e-03,  2.0926e-02,\n",
      "        -2.7839e-02,  7.0908e-03, -3.6578e-03, -2.1765e-02,  3.6649e-03,\n",
      "        -1.4733e-03,  1.6454e-02, -1.2484e-02, -1.8236e-02, -1.4967e-02,\n",
      "         1.0232e-02,  9.5959e-03, -8.7438e-03, -1.6214e-02,  5.0850e-03,\n",
      "         1.1155e-02, -1.0752e-03,  2.2741e-03, -2.2039e-02, -1.9147e-02,\n",
      "         2.1197e-02, -1.2477e-03, -8.2279e-03, -3.5126e-02,  5.9926e-04,\n",
      "         2.1498e-03, -6.5927e-03, -1.9038e-02,  1.1821e-02,  3.2151e-02,\n",
      "        -3.9834e-03, -7.0652e-03, -8.7383e-03, -7.6366e-03, -1.7619e-02,\n",
      "        -6.8003e-03, -1.8757e-02, -1.7504e-02,  6.5936e-03,  2.0987e-02,\n",
      "        -1.1828e-02, -9.4542e-03, -4.7940e-02, -1.2574e-02,  1.1707e-02,\n",
      "         1.4162e-02, -8.1208e-03, -8.1660e-03,  8.0668e-03, -1.8352e-02,\n",
      "        -2.3183e-02, -2.9567e-02,  2.5228e-02,  2.2482e-02, -7.9362e-03,\n",
      "         3.1878e-03,  8.6810e-03, -1.5530e-02,  1.3472e-02, -2.9333e-03,\n",
      "         3.1686e-02, -1.3530e-02, -3.1270e-03,  8.4381e-03, -4.8111e-03,\n",
      "         3.6472e-02,  2.7102e-02, -5.8864e-03, -1.3934e-02, -1.7258e-04,\n",
      "         2.9685e-03,  1.2392e-02,  3.4103e-03, -1.7109e-03,  9.7698e-04,\n",
      "        -3.7045e-03,  5.0796e-03, -3.5148e-03,  1.1446e-02,  2.1964e-02,\n",
      "         1.1731e-03, -2.9656e-02,  9.1646e-03,  3.1780e-02, -2.0218e-03,\n",
      "        -2.3785e-02, -1.5282e-02, -2.0057e-03, -1.3830e-02, -2.9385e-02,\n",
      "         6.0658e-03, -9.9083e-03, -1.2375e-02, -3.8539e-02,  1.4985e-02,\n",
      "         4.5107e-04,  2.4175e-02, -5.4305e-03, -1.7981e-02, -5.0235e-03,\n",
      "        -8.1290e-04,  1.3264e-02, -3.4122e-02, -6.6395e-04, -1.7322e-02,\n",
      "        -1.7638e-02,  2.2514e-02,  3.1183e-02,  9.3279e-03, -1.2283e-02,\n",
      "        -5.1916e-03,  1.3756e-02, -6.4683e-03,  1.1527e-02, -1.1520e-02,\n",
      "        -8.4035e-04,  1.7670e-02, -1.9797e-02, -2.3503e-02, -4.3949e-03,\n",
      "        -4.1185e-02,  7.1069e-04,  1.6029e-02,  3.2590e-02,  1.4538e-02,\n",
      "         2.8918e-02, -2.9300e-03,  8.2022e-03,  2.3143e-03, -1.6289e-03,\n",
      "         3.1038e-02, -7.4599e-05,  1.8657e-02, -6.9727e-04, -2.9207e-02,\n",
      "         3.6911e-03, -2.5156e-02, -9.7597e-03, -7.4264e-03,  1.8907e-02,\n",
      "        -1.3512e-02,  1.1529e-02,  1.8028e-03,  9.9850e-03,  4.9829e-03,\n",
      "        -1.7746e-03,  2.7619e-02, -8.0883e-03, -7.3363e-03,  1.2097e-02,\n",
      "        -1.1326e-02, -9.5146e-03])\n",
      "Layer: encoder.1.1.block.0.weight\n",
      "Weight: tensor([0.9937, 0.9714, 0.9593, 1.0001, 0.9654, 0.9882, 1.0192, 1.0262, 0.9958,\n",
      "        0.9649, 0.9616, 1.0572, 1.0028, 1.0241, 0.9845, 0.9521, 0.9181, 1.0075,\n",
      "        0.9892, 0.9921, 0.9717, 0.9962, 0.9611, 0.9773, 0.9953, 0.9913, 1.0107,\n",
      "        0.9917, 1.0318, 0.9331, 0.9890, 0.9947, 0.9410, 0.9994, 0.9932, 0.9796,\n",
      "        0.9983, 0.9620, 1.0082, 1.0188, 0.9757, 1.0089, 0.9730, 1.0271, 0.9559,\n",
      "        0.9663, 0.9945, 0.9812, 0.9947, 1.0281, 0.9631, 0.9874, 0.9615, 0.9778,\n",
      "        0.9866, 0.9812, 1.0167, 0.9659, 1.0027, 1.0020, 1.0021, 0.9653, 0.9868,\n",
      "        0.9906, 1.0175, 0.9818, 1.0886, 0.9776, 0.9650, 1.0003, 0.9981, 0.9346,\n",
      "        0.9766, 1.0103, 1.0005, 0.9716, 0.9810, 0.9807, 1.0277, 0.9830, 0.9632,\n",
      "        1.0075, 1.0407, 0.9863, 0.9853, 0.9445, 0.9828, 0.9778, 0.9768, 0.9744,\n",
      "        0.9685, 0.9772, 0.9851, 1.0019, 0.9003, 0.9585, 0.9774, 1.0165, 0.9708,\n",
      "        1.0047, 1.0300, 0.9570, 0.9828, 0.9602, 0.9993, 0.9666, 0.9802, 1.0011,\n",
      "        0.9903, 1.0057, 1.0067, 0.9951, 0.9702, 0.9724, 0.9725, 0.9783, 1.0173,\n",
      "        0.9145, 0.9807, 0.9719, 1.0214, 0.9698, 0.9521, 0.9868, 0.9648, 0.9992,\n",
      "        0.9718, 1.0061, 0.9806, 0.9610, 0.9677, 0.9595, 1.0387, 0.9973, 0.9922,\n",
      "        0.9530, 1.0113, 0.9896, 0.9858, 0.9875, 0.9518, 1.0137, 0.9432, 0.9480,\n",
      "        0.9465, 0.9908, 1.0353, 0.9774, 0.9881, 1.0240, 0.9728, 0.9582, 0.9858,\n",
      "        1.0119, 1.0194, 0.9795, 0.9401, 0.9596, 1.0130, 0.9825, 1.0257, 0.8709,\n",
      "        0.9738, 0.9818, 0.9568, 1.0064, 0.9707, 1.0130, 0.9886, 0.9547, 0.9714,\n",
      "        0.9901, 1.0310, 1.0049, 1.0027, 0.9570, 0.9742, 0.9477, 1.0104, 0.9760,\n",
      "        1.0082, 0.9400, 0.9876, 1.0089, 0.9767, 0.9772, 0.9799, 0.9866, 1.0075,\n",
      "        1.0061, 0.9923, 1.0012])\n",
      "Layer: encoder.1.1.block.0.bias\n",
      "Bias: tensor([ 0.0325, -0.0406, -0.0538, -0.0492, -0.0241, -0.0227, -0.0540,  0.0465,\n",
      "        -0.0445,  0.0665, -0.0278,  0.0831, -0.0207, -0.0281,  0.0558, -0.0350,\n",
      "        -0.0202, -0.0456,  0.0222,  0.0348, -0.0292,  0.0649, -0.0198, -0.0437,\n",
      "         0.0517, -0.0372, -0.0576, -0.0188, -0.0257, -0.0104, -0.0399, -0.0589,\n",
      "         0.0535,  0.0003, -0.0499,  0.0144, -0.0225,  0.0088,  0.0080, -0.0745,\n",
      "         0.0080, -0.0024, -0.0391,  0.0528, -0.0358, -0.0389,  0.0713,  0.0869,\n",
      "         0.0288, -0.0359,  0.0539, -0.0002,  0.0251, -0.0218, -0.0636,  0.0564,\n",
      "         0.0429,  0.0358, -0.0485, -0.0075, -0.0772, -0.0341,  0.0062, -0.0044,\n",
      "        -0.0114, -0.0581, -0.0392,  0.0384, -0.0218, -0.0359, -0.0529,  0.0006,\n",
      "        -0.0394, -0.0295,  0.0661, -0.0599,  0.0139,  0.0441, -0.0629, -0.0657,\n",
      "        -0.0282, -0.0390,  0.0638,  0.0337, -0.0104, -0.0502,  0.0383,  0.0498,\n",
      "        -0.0545,  0.0611, -0.0640,  0.0071,  0.0135, -0.0678, -0.0383,  0.0161,\n",
      "         0.0477, -0.0725, -0.0784, -0.0270, -0.0377,  0.0010, -0.0007, -0.0784,\n",
      "         0.0678,  0.0375, -0.0256,  0.0658,  0.0337, -0.0336,  0.0309, -0.0393,\n",
      "        -0.0385, -0.0314,  0.0030, -0.0211,  0.0408, -0.0220, -0.0095, -0.0109,\n",
      "         0.0264, -0.0452, -0.0269,  0.0332,  0.0258, -0.0139,  0.0988,  0.0328,\n",
      "         0.0569, -0.0179,  0.0247, -0.0376, -0.0473, -0.0297, -0.0384,  0.0016,\n",
      "        -0.0758, -0.0801, -0.0372, -0.0294, -0.0059, -0.0217, -0.0427,  0.0141,\n",
      "        -0.0277,  0.0538, -0.0129,  0.0496, -0.0295,  0.0625, -0.0141, -0.0573,\n",
      "         0.0036, -0.0743,  0.0385, -0.0593,  0.0006, -0.0495, -0.0633, -0.0841,\n",
      "         0.0769, -0.0081, -0.0590, -0.0636, -0.0461, -0.0404, -0.0495,  0.0827,\n",
      "        -0.0563,  0.0650, -0.0034, -0.0189, -0.0211, -0.0722,  0.0438,  0.0745,\n",
      "         0.0355, -0.0529, -0.0147,  0.0226, -0.0306,  0.0597, -0.0596,  0.0245,\n",
      "        -0.0314, -0.0195, -0.0300, -0.0419,  0.0179,  0.0376,  0.0331,  0.0795])\n",
      "Layer: encoder.2.0.block.0.weight\n",
      "Weight: tensor([1.0683, 1.1038, 1.0350, 1.0554, 1.0545, 1.0968, 1.0435, 1.0464, 1.0424,\n",
      "        1.0330, 1.0687, 1.0465, 1.0285, 1.0409, 1.0376, 1.0527, 1.0340, 1.0383,\n",
      "        1.0693, 1.0516, 1.0870, 1.0546, 1.0515, 1.0133, 1.0666, 1.0618, 1.0167,\n",
      "        1.0883, 1.0738, 1.0407, 1.0547, 1.0548, 1.0306, 1.0189, 1.0361, 1.0457,\n",
      "        1.0166, 1.0672, 1.0638, 1.0604, 1.0296, 1.0497, 1.0919, 1.0701, 1.0854,\n",
      "        1.0413, 1.0159, 1.0239, 1.0166, 1.0809, 1.0415, 1.0555, 1.0513, 1.0283,\n",
      "        1.0021, 1.0298, 1.0211, 1.0502, 1.0596, 1.0428, 1.0692, 1.0472, 1.0645,\n",
      "        1.0871, 1.0801, 1.0582, 1.0663, 1.0357, 1.0764, 1.0415, 1.0337, 1.0298,\n",
      "        1.0500, 1.0724, 1.1178, 1.0711, 1.0815, 1.0112, 1.0684, 1.0699, 1.0131,\n",
      "        1.0372, 1.0205, 0.9901, 1.0182, 1.0343, 1.0692, 1.0715, 1.0384, 1.0586,\n",
      "        1.0698, 1.0338, 1.0445, 1.0453, 1.0465, 1.0413, 1.0643, 1.0003, 1.0596,\n",
      "        1.0190, 1.0605, 1.0160, 0.9936, 1.0770, 1.0673, 1.0238, 1.0453, 1.0252,\n",
      "        1.0623, 1.0521, 1.0450, 1.0459, 1.1072, 1.0517, 1.0281, 1.0441, 1.0597,\n",
      "        1.0749, 1.0324, 1.0577, 1.1065, 1.0629, 1.0858, 1.0406, 1.0875, 1.0768,\n",
      "        1.0579, 1.0378, 1.0388, 1.0467, 1.0440, 1.0442, 1.0870, 1.0864, 1.0348,\n",
      "        1.0399, 1.0827, 1.0150, 1.0478, 1.0101, 1.0656, 1.0410, 1.0152, 1.0353,\n",
      "        1.0281, 1.0667, 1.0512, 1.0563, 1.0402, 1.0478, 1.0174, 1.0094, 1.0656,\n",
      "        1.0313, 1.0459, 1.0306, 1.0728, 1.0465, 1.0714, 1.0757, 1.0682, 1.0778,\n",
      "        1.0566, 1.0521, 1.0196, 1.0586, 1.0045, 1.0909, 1.0369, 1.0134, 0.9898,\n",
      "        1.0396, 1.0366, 1.0565, 1.0528, 1.0106, 1.0321, 1.0342, 1.1135, 0.9966,\n",
      "        1.0753, 1.0765, 1.0382, 1.0453, 1.0385, 1.0471, 1.0371, 1.0347, 1.0597,\n",
      "        1.0563, 1.0756, 1.0350])\n",
      "Layer: encoder.2.0.block.0.bias\n",
      "Bias: tensor([-3.6446e-03, -6.5881e-03, -1.1986e-02, -2.4973e-03, -9.9924e-03,\n",
      "        -1.4291e-02,  1.7567e-02, -1.6188e-02,  3.1986e-03,  1.4056e-02,\n",
      "        -5.9809e-03, -1.7776e-02, -1.0477e-03, -2.1115e-02, -6.9671e-04,\n",
      "        -2.7420e-03, -1.9178e-02, -6.8525e-03,  1.4848e-02,  1.5895e-02,\n",
      "         1.1005e-02,  5.6878e-03, -6.9563e-03,  5.9341e-03, -1.0367e-02,\n",
      "        -1.3444e-02,  3.8690e-03, -3.0317e-02,  1.3401e-02,  5.8541e-04,\n",
      "         1.1199e-02,  8.0511e-03,  2.4858e-03, -1.8745e-02,  2.1255e-02,\n",
      "         5.9271e-03,  1.1017e-02,  1.4185e-03,  1.5176e-03,  1.5505e-02,\n",
      "        -8.5912e-03, -1.6486e-02,  2.2144e-02,  5.1069e-03, -3.1707e-02,\n",
      "         2.0572e-02, -2.2695e-02,  1.0978e-02, -6.9593e-03,  2.7257e-03,\n",
      "         2.3423e-02,  1.3376e-02,  9.1066e-03, -1.3712e-03, -1.5862e-02,\n",
      "         2.0420e-02,  2.2255e-03, -7.1601e-03,  7.5766e-04,  1.2090e-02,\n",
      "         9.9160e-03, -9.5357e-03, -2.3562e-03, -5.7854e-03, -1.1888e-03,\n",
      "        -1.3136e-02,  1.2679e-02,  1.9396e-03,  2.3740e-02, -3.2642e-03,\n",
      "        -1.4836e-02, -6.9739e-03, -1.7619e-02, -1.8351e-02,  2.7037e-03,\n",
      "         2.7729e-03, -7.6364e-03, -9.2561e-03, -1.8954e-02,  8.0507e-03,\n",
      "         1.1746e-02,  1.1925e-02, -3.1888e-02, -7.4455e-03,  9.7302e-03,\n",
      "        -1.3232e-02, -1.4125e-02,  2.1757e-02, -1.0573e-02,  3.5075e-03,\n",
      "        -3.2732e-02, -7.2386e-03,  2.0204e-02, -1.0977e-02,  1.5115e-02,\n",
      "         2.5450e-03, -4.1520e-04, -1.0603e-02,  6.1987e-03, -1.3471e-02,\n",
      "        -2.6834e-02, -1.6710e-02,  1.6658e-03, -1.3414e-02,  1.3979e-02,\n",
      "         2.4177e-02,  6.1661e-03, -7.4873e-03,  1.7036e-02, -7.8963e-03,\n",
      "        -7.3729e-04,  2.2395e-02, -1.7012e-03,  1.1685e-02,  2.2202e-02,\n",
      "        -2.0007e-02,  1.0618e-02, -1.7621e-02,  6.3320e-03,  2.0332e-02,\n",
      "        -1.7218e-02, -5.4499e-03, -1.0615e-02,  6.4871e-03, -2.2855e-02,\n",
      "         5.4921e-03,  5.4272e-03, -2.9475e-02, -2.2723e-02, -2.8847e-03,\n",
      "        -9.6929e-03, -5.8019e-03, -5.4211e-03, -9.9635e-03,  1.9887e-02,\n",
      "        -1.0792e-02, -1.3309e-02, -9.6228e-03, -8.7096e-03, -1.8253e-02,\n",
      "         4.4729e-03, -5.8649e-03,  1.6258e-02, -1.6976e-02, -1.8517e-03,\n",
      "        -4.0630e-02, -8.2244e-03,  1.8420e-02, -4.0826e-03, -6.9102e-03,\n",
      "        -2.2173e-02, -6.3609e-03,  2.6126e-02, -9.1581e-03,  2.0242e-03,\n",
      "        -1.8304e-03, -1.4936e-02, -2.7712e-02, -2.2822e-03, -2.7136e-03,\n",
      "        -6.8565e-03, -9.4793e-03,  5.0700e-03, -1.9538e-02, -2.4476e-02,\n",
      "         1.3336e-02, -1.2966e-02,  1.7717e-03,  2.4539e-02, -3.3361e-04,\n",
      "         2.6973e-02, -6.5365e-04,  1.4755e-02,  1.8234e-03,  7.7648e-03,\n",
      "         7.7296e-03, -1.3725e-02,  4.2852e-03,  2.4533e-03,  1.0880e-02,\n",
      "         5.0256e-03,  2.1404e-02,  2.1154e-03, -1.7917e-02, -1.5392e-02,\n",
      "         2.0912e-02, -8.4101e-04, -3.7363e-02,  9.3017e-03, -5.0554e-03,\n",
      "         6.3814e-05,  1.8658e-02])\n",
      "Layer: encoder.2.1.block.0.weight\n",
      "Weight: tensor([0.9604, 0.9702, 0.9625, 0.9899, 0.9430, 0.9265, 1.0174, 0.9771, 0.9863,\n",
      "        0.9299, 0.9082, 0.9838, 0.9573, 0.9714, 0.9392, 0.9898, 0.9705, 0.9983,\n",
      "        0.9842, 0.9486, 0.9837, 0.9831, 0.9413, 0.9383, 0.9921, 0.9690, 1.0003,\n",
      "        0.9383, 0.9739, 0.9805, 0.9284, 0.9039, 0.9759, 0.9463, 0.9862, 0.9301,\n",
      "        0.9799, 0.9204, 0.9651, 1.0066, 0.9678, 0.9733, 0.9133, 0.9989, 0.8608,\n",
      "        0.9607, 0.9630, 0.9296, 0.9742, 0.9874, 0.9096, 0.9727, 0.9306, 1.0065,\n",
      "        0.9193, 0.9623, 0.9913, 0.9112, 0.9507, 0.9679, 0.9672, 0.9972, 0.9646,\n",
      "        0.9902, 0.9552, 0.9271, 1.0274, 1.0129, 0.9819, 0.9967, 0.9749, 0.9530,\n",
      "        0.9745, 1.0167, 0.9811, 0.9867, 0.9521, 1.0276, 0.9553, 0.9515, 0.9395,\n",
      "        1.0144, 0.9751, 0.9910, 1.0008, 0.9806, 0.9329, 0.9616, 0.9435, 0.9108,\n",
      "        0.9094, 0.9640, 0.9496, 0.9441, 0.9270, 0.9559, 0.9212, 0.9691, 0.9573,\n",
      "        0.9845, 0.9953, 0.9698, 0.9845, 0.9763, 0.9631, 0.9563, 0.9386, 0.9853,\n",
      "        0.9887, 1.0121, 1.0006, 0.9401, 0.9388, 0.9855, 1.0013, 0.9376, 0.9829,\n",
      "        0.9425, 1.0040, 0.9210, 0.9428, 0.9573, 0.9649, 0.9723, 0.8837, 0.9805,\n",
      "        0.9405, 0.9876, 0.9552, 0.8952, 0.9806, 0.9425, 1.0286, 0.9928, 0.9902,\n",
      "        0.9660, 0.9686, 0.9436, 0.9856, 0.9319, 0.9292, 0.9356, 0.9363, 0.9427,\n",
      "        0.9449, 0.9452, 1.0231, 0.9625, 0.9426, 0.9657, 0.9919, 0.9494, 0.9340,\n",
      "        0.9809, 0.9802, 0.9616, 0.9872, 0.9853, 0.9651, 0.9915, 0.9223, 0.9705,\n",
      "        1.0202, 0.9494, 0.9347, 0.9520, 0.9805, 0.9438, 0.9169, 0.9269, 0.9484,\n",
      "        0.9435, 0.9931, 0.9784, 0.9132, 0.9528, 0.9932, 0.9208, 0.9833, 1.0210,\n",
      "        0.9582, 0.9616, 0.9301, 0.9771, 0.9686, 0.9762, 0.9568, 0.9246, 0.9793,\n",
      "        0.9890, 0.9467, 0.9512])\n",
      "Layer: encoder.2.1.block.0.bias\n",
      "Bias: tensor([ 6.5064e-02, -4.2360e-02, -3.3914e-02,  7.6034e-03, -3.8546e-02,\n",
      "        -1.5970e-02, -1.9620e-02, -6.5303e-03, -4.4471e-02,  3.9039e-02,\n",
      "        -2.0819e-02,  6.0189e-02, -5.4702e-02,  5.5413e-05,  4.0514e-02,\n",
      "        -6.4140e-02, -7.0102e-02, -7.3142e-02,  3.4783e-02,  5.9185e-02,\n",
      "        -3.8584e-02,  5.7760e-02, -7.4662e-03, -3.7117e-02,  3.9000e-02,\n",
      "        -4.1652e-02, -7.7203e-02,  1.2661e-03,  2.7334e-02, -5.6674e-02,\n",
      "        -3.3540e-02,  6.5584e-03,  3.9006e-02, -1.3795e-02, -5.4448e-02,\n",
      "         4.7286e-02, -3.2024e-02, -3.5763e-02,  6.1744e-03, -4.1948e-02,\n",
      "        -2.3556e-02, -5.3704e-02, -3.3588e-02,  3.6380e-02, -1.5646e-02,\n",
      "        -1.1528e-03,  5.0696e-02,  7.3024e-03,  5.1372e-02, -3.7693e-02,\n",
      "         8.9251e-02,  1.0023e-02,  6.2679e-02, -4.9637e-02, -2.6189e-02,\n",
      "         2.9024e-02,  3.0482e-02,  8.2182e-03, -2.6155e-02, -6.5898e-02,\n",
      "        -6.1311e-02, -7.1712e-02, -6.7864e-03,  4.7098e-02, -6.4554e-03,\n",
      "        -3.8496e-02, -3.8159e-02,  6.1236e-02, -3.2810e-02, -4.5818e-02,\n",
      "        -4.8381e-02, -3.5158e-02, -3.1957e-02, -3.2896e-02,  3.7845e-02,\n",
      "        -5.5424e-02, -2.0304e-03,  5.8466e-02, -3.2628e-02, -1.9251e-02,\n",
      "        -4.4554e-02, -3.3253e-02,  4.0440e-02,  5.0707e-02, -6.4182e-02,\n",
      "        -5.0139e-02,  2.2951e-02,  1.5118e-02, -6.9297e-02,  4.6552e-02,\n",
      "        -3.2610e-02, -1.1273e-02,  6.2132e-02, -3.7434e-02, -1.7403e-02,\n",
      "         7.0505e-02, -7.3490e-03, -3.7903e-02, -4.9994e-02, -1.1032e-02,\n",
      "        -4.2502e-02, -2.4060e-03, -1.1470e-03, -5.1540e-02,  5.5797e-02,\n",
      "         4.8778e-02,  5.4378e-02,  4.4549e-02,  6.1894e-02, -7.8829e-03,\n",
      "         7.5731e-03, -3.0439e-03, -3.8247e-02, -3.5189e-02, -2.3481e-02,\n",
      "        -5.7438e-02,  3.5470e-02, -2.3416e-02, -3.0580e-02, -2.6936e-02,\n",
      "         5.3203e-03, -1.9079e-02, -1.4839e-02,  6.4990e-02, -7.8995e-05,\n",
      "        -3.3238e-02,  3.4295e-02,  2.3954e-02,  4.6344e-02, -3.8128e-02,\n",
      "         6.1788e-02,  2.6278e-02, -2.4347e-02, -5.8374e-02, -4.1738e-02,\n",
      "         3.7929e-02, -3.0199e-02,  2.3979e-02, -4.9691e-02, -2.2863e-02,\n",
      "        -2.7803e-02,  2.9465e-02, -6.2360e-02, -1.8745e-03, -1.2799e-02,\n",
      "         7.1732e-02,  2.1108e-02,  3.7094e-02, -1.6215e-02,  3.5928e-02,\n",
      "        -3.6588e-02, -1.9868e-02, -6.8886e-03, -4.2518e-02,  4.7942e-02,\n",
      "         2.5029e-03,  4.4559e-02, -4.5821e-02, -5.5000e-02, -6.0584e-02,\n",
      "         2.9116e-02, -3.5638e-02, -7.9312e-02, -9.0149e-02, -2.0068e-02,\n",
      "        -9.8251e-03, -5.0091e-02,  6.5205e-02, -8.5833e-02,  2.2896e-02,\n",
      "         2.1697e-02, -1.8905e-02, -2.9528e-02, -8.7866e-03,  2.6421e-02,\n",
      "         2.2958e-02,  8.4655e-02,  2.2694e-02, -6.0124e-02,  4.6190e-02,\n",
      "         6.1033e-03,  5.2477e-02, -1.4667e-02,  5.0129e-02, -1.6060e-02,\n",
      "        -5.3808e-02, -4.0469e-02, -2.7371e-02,  4.5355e-02,  5.5676e-02,\n",
      "         3.9939e-02,  4.6023e-02])\n",
      "Layer: encoder.3.0.block.0.weight\n",
      "Weight: tensor([1.0814, 1.0765, 1.0622, 1.0329, 1.1102, 1.0746, 1.0925, 1.0411, 1.1040,\n",
      "        1.0981, 1.0950, 1.0356, 1.0815, 1.1120, 1.0693, 1.0973, 1.1254, 1.0787,\n",
      "        1.0592, 1.0606, 1.0992, 1.0615, 0.9902, 1.0858, 1.0415, 1.1419, 1.0079,\n",
      "        1.0934, 1.1100, 1.0668, 1.0658, 1.1220, 1.0822, 1.0712, 1.0245, 1.0677,\n",
      "        1.0573, 1.0459, 1.0717, 1.0428, 1.0451, 1.0981, 1.0555, 1.0663, 1.0625,\n",
      "        1.1187, 1.1033, 1.0720, 1.0314, 1.0786, 1.0697, 1.0485, 1.0711, 1.0635,\n",
      "        1.0471, 1.0872, 1.0857, 1.0699, 1.0979, 1.0730, 1.0468, 1.0317, 1.0492,\n",
      "        1.0516, 1.0897, 1.0804, 1.0888, 1.0284, 1.0744, 1.1144, 1.0221, 1.0429,\n",
      "        1.0943, 1.0632, 1.0699, 1.0654, 1.0951, 0.9977, 1.0544, 1.0982, 1.0271,\n",
      "        1.1078, 1.0240, 1.0728, 1.0496, 1.0858, 1.1356, 1.0749, 1.0588, 1.0832,\n",
      "        1.0828, 1.0805, 1.1403, 1.0709, 1.1037, 1.0706, 1.0642, 1.0828, 1.0812,\n",
      "        1.0569, 1.0600, 1.0737, 1.0908, 1.0724, 1.0601, 1.0410, 1.1175, 1.0310,\n",
      "        1.0576, 1.0817, 1.0750, 1.0618, 1.0438, 1.0588, 1.0755, 1.0793, 1.0518,\n",
      "        1.0599, 1.0509, 1.0689, 1.0683, 1.0697, 1.1012, 1.0485, 1.1046, 1.0697,\n",
      "        1.0422, 1.1182, 1.0877, 1.0309, 1.0545, 1.0816, 1.1037, 1.0711, 1.1021,\n",
      "        1.1056, 1.0422, 1.0668, 1.0743, 1.0884, 1.0687, 1.1000, 1.0329, 1.0914,\n",
      "        1.0499, 1.0940, 1.1198, 1.0762, 1.0780, 1.0616, 1.0400, 1.0842, 1.0716,\n",
      "        1.0918, 1.0975, 1.0989, 1.0515, 1.0576, 1.0828, 1.0621, 1.0996, 1.0448,\n",
      "        1.0778, 1.0919, 1.0676, 1.0589, 1.0729, 1.0909, 1.0650, 1.0993, 1.0963,\n",
      "        1.0632, 1.0514, 1.0683, 1.0946, 1.1234, 1.0735, 1.1094, 1.0514, 1.0855,\n",
      "        1.0378, 1.0669, 1.0823, 1.0403, 1.0336, 1.0844, 1.0274, 1.0631, 1.0846,\n",
      "        1.0818, 1.1047, 1.0734])\n",
      "Layer: encoder.3.0.block.0.bias\n",
      "Bias: tensor([-1.0134e-02, -2.5528e-02, -4.4422e-05,  1.1421e-02, -4.3505e-03,\n",
      "         8.3018e-03,  1.1776e-02, -2.4206e-02,  1.1184e-02, -4.4472e-03,\n",
      "        -7.8210e-03, -6.7566e-03,  2.8519e-03, -6.2728e-03, -2.5072e-02,\n",
      "         1.0841e-02, -1.6337e-02, -6.2578e-03,  9.7308e-04,  2.8110e-04,\n",
      "         5.3742e-02, -1.5228e-02,  3.3288e-02, -1.7523e-02,  1.3413e-02,\n",
      "         3.2621e-03,  1.9949e-02,  1.8062e-02, -2.8281e-03, -1.8056e-03,\n",
      "         1.5385e-03, -9.0413e-03, -3.6256e-02, -1.2111e-02,  7.1971e-03,\n",
      "         5.1344e-03,  1.4347e-02,  1.1417e-03,  1.1738e-02,  2.6959e-02,\n",
      "         2.9889e-04,  1.0288e-02,  1.6434e-02, -7.6264e-03, -2.3411e-02,\n",
      "         2.1314e-02,  2.0690e-03, -1.1223e-02, -1.8921e-02,  1.5514e-02,\n",
      "         1.5659e-02,  2.0060e-02, -1.2657e-02,  8.0964e-03,  1.5373e-02,\n",
      "         1.3688e-02,  1.7458e-03,  5.0623e-04, -1.4722e-02,  1.8194e-02,\n",
      "         1.0136e-02,  2.2101e-02,  9.5553e-05, -8.6899e-04,  5.1772e-03,\n",
      "         2.9227e-02, -9.9683e-03, -1.9101e-02, -2.7740e-03,  4.4398e-03,\n",
      "        -1.5628e-04, -8.2828e-03,  3.4074e-03,  1.1169e-02,  1.3783e-02,\n",
      "         5.8001e-03, -1.9167e-02, -3.6250e-02,  1.7682e-02, -6.9122e-03,\n",
      "         1.6996e-03,  1.4231e-02, -3.5898e-02,  9.2764e-04,  1.4663e-03,\n",
      "         1.3185e-02, -7.5674e-03,  1.1094e-02,  1.4695e-04, -7.0876e-03,\n",
      "         2.0398e-03, -4.5756e-03,  8.2286e-03,  4.5048e-03,  2.4132e-02,\n",
      "        -2.0049e-02, -3.9077e-03,  1.3582e-02,  1.2941e-02,  4.3835e-03,\n",
      "        -1.1672e-03,  3.7391e-04,  1.6218e-02,  1.2284e-02, -7.6780e-03,\n",
      "         4.5622e-03, -2.1223e-03, -1.3082e-02, -1.4154e-02,  1.3219e-03,\n",
      "        -3.0446e-03,  9.1153e-03,  8.7442e-03,  1.2832e-02,  1.9866e-02,\n",
      "        -2.8401e-03, -1.3944e-02, -6.4145e-03,  2.6456e-02, -1.0234e-02,\n",
      "         6.4360e-04, -9.5210e-03, -1.2136e-02, -1.4196e-02, -6.3746e-03,\n",
      "         9.8575e-03, -1.7399e-02,  7.2580e-03,  6.3795e-04, -1.0779e-02,\n",
      "        -2.5968e-02, -1.7997e-02, -1.4348e-02,  1.5219e-02,  1.5313e-03,\n",
      "         1.1217e-02, -9.1681e-03,  1.2280e-03,  1.5434e-03,  1.7528e-02,\n",
      "        -7.6899e-03,  2.9323e-03,  5.3500e-03,  1.0023e-02,  4.5727e-03,\n",
      "        -1.4378e-03, -6.9523e-03,  1.1435e-02,  1.8095e-02, -1.3672e-02,\n",
      "        -7.9346e-03,  2.5498e-03,  6.5080e-04,  6.5759e-05, -4.0068e-03,\n",
      "        -5.7062e-03, -2.0910e-02, -1.2364e-03,  7.8252e-04, -1.1298e-02,\n",
      "        -1.5854e-02,  2.6461e-02,  1.6140e-02,  1.7532e-02,  4.5712e-03,\n",
      "        -5.9016e-03, -1.5556e-02, -7.0299e-03,  1.3481e-02, -4.3510e-02,\n",
      "         1.4426e-02,  1.4176e-02,  2.5755e-02,  2.5410e-02,  4.2517e-03,\n",
      "        -1.6566e-02, -9.7021e-03,  1.8497e-02,  7.8582e-03,  2.0400e-02,\n",
      "        -7.4422e-03, -2.8774e-02, -3.1756e-02, -3.3795e-02,  1.2743e-02,\n",
      "         1.3058e-02,  3.4846e-02,  1.6342e-02,  1.8074e-02,  7.4222e-03,\n",
      "        -1.2218e-03, -6.3475e-03])\n",
      "Layer: encoder.3.1.block.0.weight\n",
      "Weight: tensor([0.9066, 0.9549, 0.9005, 0.9677, 0.9374, 0.9001, 0.9792, 0.9835, 0.9143,\n",
      "        0.9321, 0.9239, 0.9890, 0.9120, 0.9731, 0.9166, 0.9120, 0.8989, 0.8742,\n",
      "        0.9751, 0.9445, 0.9493, 0.9389, 0.9456, 0.9089, 0.9042, 0.9618, 0.9344,\n",
      "        0.9328, 0.9151, 0.9459, 0.9610, 0.9111, 0.9331, 0.8996, 0.9495, 0.9342,\n",
      "        0.9607, 0.9440, 0.9047, 0.9750, 0.9387, 0.9338, 0.8973, 0.9585, 0.8789,\n",
      "        0.9052, 0.9494, 0.9259, 0.9597, 0.9287, 0.9070, 0.9320, 0.9194, 0.9477,\n",
      "        0.8842, 0.9239, 0.9677, 0.8407, 0.8824, 0.9557, 0.9609, 0.9392, 0.9502,\n",
      "        0.9706, 0.9700, 0.9410, 0.9798, 0.9730, 0.9317, 0.9516, 0.9685, 0.9847,\n",
      "        0.9344, 0.9720, 0.9624, 0.9134, 0.9182, 0.8976, 0.9797, 0.9265, 0.8619,\n",
      "        0.9799, 0.9834, 0.9351, 0.9317, 0.9333, 0.8785, 0.9528, 0.8928, 0.9045,\n",
      "        0.9446, 0.9226, 0.9423, 0.9003, 0.9174, 0.9145, 0.8956, 0.9545, 0.9564,\n",
      "        0.9211, 0.9645, 0.9022, 0.9441, 0.9229, 0.9321, 0.9309, 0.8879, 1.0082,\n",
      "        0.9600, 0.9380, 0.9158, 0.9012, 0.9356, 0.9247, 0.9203, 0.9054, 0.9848,\n",
      "        0.9380, 1.0223, 0.9361, 0.8965, 0.9450, 0.9288, 0.9287, 0.8653, 0.9260,\n",
      "        0.9288, 0.9780, 0.9203, 0.9156, 0.9730, 0.9319, 0.9143, 0.9719, 0.9519,\n",
      "        0.9156, 0.9135, 0.8683, 0.9648, 0.8951, 0.8977, 0.9285, 0.9385, 0.8888,\n",
      "        0.8578, 0.9370, 0.9535, 0.9214, 0.9613, 0.9586, 0.9445, 0.9689, 0.9685,\n",
      "        0.9414, 0.9345, 0.8977, 0.9753, 0.9153, 0.9055, 0.9173, 0.9642, 0.9099,\n",
      "        0.9438, 0.9518, 0.9411, 0.9281, 0.9639, 0.8878, 0.8680, 0.9565, 0.9291,\n",
      "        0.9047, 0.9400, 0.8986, 0.9066, 0.9123, 0.9656, 0.8870, 0.9437, 0.9578,\n",
      "        0.9383, 0.9238, 0.9340, 0.8809, 0.9424, 0.9483, 0.9663, 0.9065, 0.9823,\n",
      "        0.9635, 0.9082, 0.8686])\n",
      "Layer: encoder.3.1.block.0.bias\n",
      "Bias: tensor([ 0.0640, -0.0412, -0.0450, -0.0043, -0.0797,  0.0126, -0.0350,  0.0474,\n",
      "        -0.0172,  0.0257, -0.0365,  0.0557,  0.0105, -0.0296,  0.0206, -0.0070,\n",
      "        -0.0298, -0.0201,  0.0545,  0.0808,  0.0003,  0.0237, -0.0207, -0.0193,\n",
      "         0.0203, -0.0337, -0.0438, -0.0133,  0.0446, -0.0189, -0.0452,  0.0227,\n",
      "         0.0084,  0.0260, -0.0393,  0.0334, -0.0027, -0.0431,  0.0121, -0.0265,\n",
      "         0.0178, -0.0301, -0.0138,  0.0182, -0.0073,  0.0010,  0.0332,  0.0228,\n",
      "         0.0280, -0.0229,  0.0249,  0.0012,  0.0495, -0.0369, -0.0593,  0.0225,\n",
      "         0.0312, -0.0625,  0.0130, -0.0029, -0.0475, -0.0556, -0.0030,  0.0392,\n",
      "        -0.0017, -0.0257, -0.0300,  0.0437, -0.0042, -0.0493, -0.0644, -0.0108,\n",
      "        -0.0267, -0.0232,  0.0668, -0.0216,  0.0085,  0.0396, -0.0143,  0.0099,\n",
      "        -0.0199, -0.0405,  0.0533,  0.0488, -0.0428, -0.0322, -0.0129,  0.0172,\n",
      "        -0.0054, -0.0062, -0.0377,  0.0025,  0.0131, -0.0118, -0.0477, -0.0016,\n",
      "         0.0145, -0.0300, -0.0413, -0.0050, -0.0441, -0.0002, -0.0232, -0.0096,\n",
      "         0.0375,  0.0242,  0.0130,  0.0081,  0.0194,  0.0199,  0.0289,  0.0190,\n",
      "        -0.0513, -0.0197, -0.0375, -0.0689,  0.0549, -0.0085, -0.0463,  0.0283,\n",
      "        -0.0262, -0.0158, -0.0162,  0.0567, -0.0191, -0.0368,  0.0478,  0.0231,\n",
      "         0.0231, -0.0540,  0.0366,  0.0094, -0.0408, -0.0675, -0.0067,  0.0089,\n",
      "        -0.0450, -0.0199, -0.0600,  0.0037, -0.0192,  0.0005, -0.0569, -0.0032,\n",
      "        -0.0049,  0.0150,  0.0424,  0.0389, -0.0417,  0.0022, -0.0324, -0.0135,\n",
      "        -0.0304, -0.0436,  0.0334,  0.0204,  0.0506, -0.0208, -0.0265, -0.0456,\n",
      "         0.0291, -0.0158, -0.0707, -0.0562, -0.0822, -0.0022, -0.0199,  0.0187,\n",
      "        -0.0248,  0.0216, -0.0093, -0.0033, -0.0198,  0.0036,  0.0293,  0.0505,\n",
      "         0.0038, -0.0133, -0.0625,  0.0333, -0.0020,  0.0366,  0.0278,  0.0276,\n",
      "        -0.0294, -0.0704, -0.0076, -0.0242,  0.0201,  0.0321,  0.0307,  0.0227])\n",
      "Layer: encoder.4.0.block.0.weight\n",
      "Weight: tensor([1.0638, 1.0900, 1.0632, 1.0523, 1.0345, 1.0584, 1.1711, 1.0900, 1.0929,\n",
      "        1.0388, 1.0739, 1.0645, 1.1292, 1.0302, 1.0804, 1.0459, 1.0506, 1.0580,\n",
      "        1.0772, 1.0868, 1.0476, 1.0388, 1.0627, 1.1022, 1.0526, 1.0761, 1.0509,\n",
      "        1.0698, 1.0465, 1.0165, 1.0643, 1.0420, 1.0936, 1.0807, 1.0737, 1.0721,\n",
      "        1.0657, 1.0433, 1.1175, 1.0729, 1.1148, 1.0733, 1.0650, 1.1116, 1.0535,\n",
      "        1.0558, 1.0894, 1.0698, 1.0856, 1.0981, 1.1126, 1.0848, 1.1226, 1.0476,\n",
      "        1.0584, 1.0810, 1.0570, 1.0946, 1.0703, 1.1243, 1.0933, 1.0815, 1.0935,\n",
      "        1.0443, 1.0913, 1.0623, 1.0801, 1.0687, 1.1020, 1.0818, 1.0493, 1.0772,\n",
      "        1.0247, 1.0790, 1.0959, 1.0866, 1.0842, 1.0525, 1.0348, 1.0587, 1.0980,\n",
      "        1.0704, 1.0870, 1.0229, 1.0635, 1.0514, 1.1175, 1.1091, 1.0512, 1.1043,\n",
      "        1.0645, 1.0539, 1.0839, 1.0627, 1.0805, 1.1096, 1.0489, 1.0336, 1.0841,\n",
      "        1.0873, 1.0860, 1.1055, 1.0654, 1.0862, 0.9907, 1.1054, 1.0718, 1.0442,\n",
      "        1.0447, 1.1030, 1.0714, 1.0698, 1.0567, 1.0150, 1.0586, 1.0929, 1.0627,\n",
      "        1.0240, 1.0025, 1.0877, 1.0795, 1.0358, 1.0918, 1.1030, 1.0876, 1.0521,\n",
      "        1.0518, 1.0729, 1.1203, 1.0556, 1.1076, 1.0885, 1.0346, 1.0846, 1.0358,\n",
      "        1.0822, 1.1065, 1.0706, 1.0744, 1.0541, 1.0603, 1.0676, 1.0301, 1.0764,\n",
      "        1.0823, 1.1184, 1.0908, 1.0279, 1.0856, 1.0532, 1.0906, 1.0517, 1.0927,\n",
      "        1.0738, 1.1094, 1.0904, 1.0742, 1.0900, 1.0831, 1.0111, 1.0649, 1.1051,\n",
      "        1.0290, 1.1085, 1.0452, 1.0810, 1.0873, 1.0647, 1.0183, 1.0940, 1.0649,\n",
      "        1.0491, 1.0893, 1.0781, 1.0968, 1.1113, 1.0955, 1.1329, 1.0838, 0.9928,\n",
      "        1.0456, 1.0852, 1.0841, 1.0612, 1.0525, 1.0696, 1.0469, 1.0601, 1.0289,\n",
      "        1.0276, 1.0675, 1.0702])\n",
      "Layer: encoder.4.0.block.0.bias\n",
      "Bias: tensor([ 0.0025, -0.0152,  0.0046,  0.0125, -0.0015,  0.0121,  0.0283, -0.0158,\n",
      "         0.0164,  0.0263,  0.0274, -0.0307, -0.0117,  0.0182,  0.0039,  0.0200,\n",
      "         0.0053, -0.0218,  0.0104, -0.0067,  0.0150, -0.0097,  0.0143,  0.0314,\n",
      "        -0.0007, -0.0148,  0.0081,  0.0148, -0.0125,  0.0101,  0.0104, -0.0148,\n",
      "         0.0163, -0.0266,  0.0091, -0.0031,  0.0106,  0.0089, -0.0107,  0.0475,\n",
      "        -0.0235,  0.0269,  0.0096, -0.0053,  0.0292, -0.0071, -0.0025,  0.0017,\n",
      "        -0.0283, -0.0025, -0.0066, -0.0110,  0.0058, -0.0022,  0.0201,  0.0045,\n",
      "        -0.0054, -0.0313, -0.0120,  0.0080,  0.0276,  0.0128, -0.0242, -0.0236,\n",
      "        -0.0137, -0.0013,  0.0067,  0.0024,  0.0070, -0.0086,  0.0324,  0.0346,\n",
      "         0.0277,  0.0130, -0.0298,  0.0353, -0.0138, -0.0330,  0.0064,  0.0239,\n",
      "        -0.0037,  0.0066, -0.0197, -0.0247,  0.0072,  0.0247, -0.0259, -0.0261,\n",
      "        -0.0164, -0.0276,  0.0174, -0.0101,  0.0107,  0.0164, -0.0110, -0.0117,\n",
      "        -0.0224, -0.0163,  0.0094, -0.0267, -0.0085,  0.0215,  0.0015,  0.0090,\n",
      "        -0.0002,  0.0163, -0.0230, -0.0074, -0.0210,  0.0095, -0.0136,  0.0038,\n",
      "         0.0205,  0.0355, -0.0136,  0.0118, -0.0268, -0.0109,  0.0298,  0.0109,\n",
      "        -0.0132,  0.0100,  0.0162,  0.0112,  0.0021, -0.0150, -0.0495,  0.0154,\n",
      "        -0.0328,  0.0272, -0.0167, -0.0022,  0.0394,  0.0217,  0.0035,  0.0161,\n",
      "         0.0180,  0.0041,  0.0038, -0.0017,  0.0023,  0.0054, -0.0026,  0.0165,\n",
      "         0.0287, -0.0183, -0.0040, -0.0425,  0.0114,  0.0170,  0.0181, -0.0314,\n",
      "         0.0303,  0.0187, -0.0290,  0.0218, -0.0155, -0.0539, -0.0005,  0.0468,\n",
      "        -0.0350,  0.0081, -0.0224,  0.0235,  0.0143,  0.0298,  0.0201, -0.0353,\n",
      "        -0.0196, -0.0348, -0.0185, -0.0124, -0.0164, -0.0171, -0.0029, -0.0193,\n",
      "         0.0061,  0.0183, -0.0085, -0.0054,  0.0183, -0.0386,  0.0345, -0.0417,\n",
      "         0.0327,  0.0290, -0.0041,  0.0229, -0.0410, -0.0133,  0.0100,  0.0139])\n",
      "Layer: encoder.4.1.block.0.weight\n",
      "Weight: tensor([0.8385, 0.8729, 0.8614, 0.9557, 0.9089, 0.9396, 0.9707, 0.9610, 0.8626,\n",
      "        0.8980, 0.8756, 0.9733, 0.8909, 0.9172, 0.8736, 0.9056, 0.8776, 0.9417,\n",
      "        0.9588, 0.9246, 0.9377, 0.8978, 0.8972, 0.8855, 0.8216, 0.8997, 0.9275,\n",
      "        0.8598, 0.8622, 0.8710, 0.9307, 0.8626, 0.9285, 0.8541, 0.8815, 0.9460,\n",
      "        0.9331, 0.8715, 0.8788, 0.8776, 0.8782, 0.8623, 0.8724, 0.9348, 0.9126,\n",
      "        0.8783, 0.8718, 0.8721, 1.0108, 0.8399, 0.8359, 0.9153, 0.8770, 0.9188,\n",
      "        0.8759, 0.8592, 0.8911, 0.8944, 0.8594, 0.9404, 0.9097, 0.9237, 0.9029,\n",
      "        0.9585, 0.9252, 0.9094, 0.9308, 0.9252, 0.9353, 0.9370, 0.9915, 0.9212,\n",
      "        0.8549, 0.9098, 0.9347, 0.8771, 0.8864, 0.8992, 0.9496, 0.9144, 0.8847,\n",
      "        0.9492, 0.9383, 0.8932, 0.9183, 0.9333, 0.9471, 0.9240, 0.8692, 0.8388,\n",
      "        0.9197, 0.8932, 0.8780, 0.8864, 0.9304, 0.8962, 0.8437, 0.8946, 0.9136,\n",
      "        0.8573, 0.9171, 0.8501, 0.9585, 0.8601, 0.9281, 0.9313, 0.9301, 0.9406,\n",
      "        0.9368, 0.9016, 0.9110, 0.9369, 0.9205, 0.9038, 0.9001, 0.8786, 0.9471,\n",
      "        0.9333, 0.9534, 0.9131, 0.8604, 0.9343, 0.8587, 0.9129, 0.8458, 0.9173,\n",
      "        0.8971, 0.8816, 0.9070, 0.8510, 0.9100, 0.9398, 0.9074, 0.8765, 0.9260,\n",
      "        0.8522, 0.8750, 0.8942, 0.9066, 0.8622, 0.8860, 0.8762, 0.9268, 0.8550,\n",
      "        0.9161, 0.9467, 0.9027, 0.9041, 0.9509, 0.9260, 0.8790, 0.8511, 0.8736,\n",
      "        0.9153, 0.8671, 0.8527, 0.9455, 0.8596, 0.8448, 0.9190, 0.9242, 0.8798,\n",
      "        0.9401, 0.8828, 0.8780, 0.8835, 0.9271, 0.9660, 0.9342, 0.9509, 0.9452,\n",
      "        0.9290, 0.9359, 0.8991, 0.8639, 0.8667, 0.9374, 0.9321, 0.8572, 0.9280,\n",
      "        0.9599, 0.9640, 0.9253, 0.8569, 0.9147, 0.9371, 0.9393, 0.8833, 0.9399,\n",
      "        0.8802, 0.8939, 0.8750])\n",
      "Layer: encoder.4.1.block.0.bias\n",
      "Bias: tensor([ 0.0807, -0.0234, -0.0325, -0.0062, -0.0243,  0.0284, -0.0043,  0.0654,\n",
      "        -0.0232,  0.0229, -0.0478,  0.0081, -0.0369, -0.0519, -0.0039, -0.0131,\n",
      "        -0.0307, -0.0584,  0.0418,  0.0549, -0.0339,  0.0116, -0.0144, -0.0388,\n",
      "         0.0113, -0.0276, -0.0366, -0.0021,  0.0181,  0.0022, -0.0350,  0.0040,\n",
      "        -0.0044,  0.0394, -0.0175,  0.0222,  0.0026, -0.0273,  0.0370, -0.0144,\n",
      "         0.0048, -0.0335, -0.0179,  0.0184, -0.0301,  0.0088,  0.0386, -0.0112,\n",
      "         0.0385,  0.0080,  0.0232, -0.0008,  0.0174, -0.0393, -0.0208, -0.0079,\n",
      "         0.0407, -0.0420, -0.0413, -0.0227, -0.0356, -0.0010, -0.0087,  0.0058,\n",
      "         0.0498, -0.0033, -0.0274,  0.0190,  0.0084, -0.0378, -0.0522, -0.0427,\n",
      "        -0.0280, -0.0230,  0.0264, -0.0439, -0.0116,  0.0088, -0.0210,  0.0270,\n",
      "        -0.0090, -0.0282,  0.0128,  0.0359, -0.0238, -0.0303,  0.0096,  0.0085,\n",
      "        -0.0030,  0.0396, -0.0335,  0.0319,  0.0178,  0.0436, -0.0205,  0.0330,\n",
      "         0.0577, -0.0139, -0.0467, -0.0256, -0.0259, -0.0048, -0.0350, -0.0020,\n",
      "         0.0403,  0.0376,  0.0160,  0.0209,  0.0228,  0.0648,  0.0359, -0.0138,\n",
      "        -0.0022, -0.0552, -0.0127,  0.0054,  0.0187,  0.0340, -0.0443,  0.0375,\n",
      "        -0.0437,  0.0013, -0.0228,  0.0199, -0.0192, -0.0387,  0.0107, -0.0092,\n",
      "         0.0106, -0.0133,  0.0120, -0.0340, -0.0258, -0.0424,  0.0133, -0.0023,\n",
      "        -0.0360,  0.0329, -0.0255, -0.0048, -0.0396,  0.0054,  0.0171, -0.0356,\n",
      "        -0.0440,  0.0184,  0.0517,  0.0408, -0.0132,  0.0081, -0.0655, -0.0013,\n",
      "        -0.0409, -0.0433,  0.0026,  0.0060,  0.0031, -0.0388,  0.0090, -0.0461,\n",
      "         0.0380,  0.0024, -0.0332, -0.0176, -0.0263, -0.0145, -0.0332,  0.0185,\n",
      "        -0.0048,  0.0418, -0.0362,  0.0054, -0.0210, -0.0085,  0.0601,  0.0263,\n",
      "         0.0133, -0.0051, -0.0084,  0.0340, -0.0205,  0.0466,  0.0080,  0.0484,\n",
      "        -0.0058, -0.0311, -0.0158, -0.0391,  0.0375,  0.0560, -0.0085, -0.0150])\n",
      "Layer: encoder.5.0.block.0.weight\n",
      "Weight: tensor([1.0040, 1.0196, 1.0704, 1.1298, 1.0712, 1.0603, 1.0917, 1.0490, 1.0462,\n",
      "        1.0612, 1.0613, 1.0652, 1.0652, 1.0995, 1.0740, 1.0476, 1.0782, 1.0395,\n",
      "        1.0467, 1.0482, 1.0595, 1.0212, 1.0892, 1.0564, 1.0591, 1.0401, 1.0688,\n",
      "        1.0644, 1.0795, 1.0583, 1.0961, 1.0673, 1.0423, 1.0371, 1.0704, 1.0984,\n",
      "        1.0560, 1.0819, 1.1150, 1.0513, 1.0395, 1.0705, 1.0471, 1.1171, 1.0756,\n",
      "        1.0950, 1.0425, 1.0132, 1.0667, 1.0646, 1.0535, 1.0627, 1.0746, 1.0333,\n",
      "        1.0406, 1.0506, 1.0950, 1.0659, 1.0542, 1.1152, 1.1051, 1.0496, 1.0604,\n",
      "        1.0909, 1.1683, 1.0517, 1.0722, 1.0735, 1.0947, 1.0375, 1.0749, 1.0275,\n",
      "        1.0235, 1.0754, 1.0849, 1.0203, 1.1024, 1.0624, 1.0068, 1.0479, 1.0372,\n",
      "        1.0356, 1.0895, 1.0444, 1.0799, 1.0163, 1.0800, 1.0689, 1.0467, 1.0579,\n",
      "        1.0647, 1.0638, 1.0726, 1.1013, 1.0186, 1.0575, 1.0328, 1.0457, 1.0428,\n",
      "        1.0432, 1.0654, 1.0768, 1.0599, 1.0159, 1.1010, 1.0476, 1.0689, 1.0837,\n",
      "        1.0496, 1.0630, 1.0956, 1.0832, 1.0394, 1.0422, 1.0577, 1.0625, 1.0778,\n",
      "        1.0391, 1.0598, 1.0762, 1.0847, 1.0916, 1.1236, 1.0519, 1.0955, 1.0524,\n",
      "        1.0737, 1.0769, 1.1136, 1.0508, 1.0745, 1.0555, 1.0512, 1.0406, 1.0306,\n",
      "        1.0552, 1.0989, 1.0317, 1.0444, 1.0923, 1.0401, 1.0403, 1.0319, 1.1142,\n",
      "        1.0648, 1.0799, 1.0375, 1.0953, 1.0798, 1.0414, 1.0859, 1.0929, 1.0831,\n",
      "        1.0774, 1.0578, 1.0532, 1.0767, 1.0957, 1.1136, 1.0403, 1.0819, 1.0746,\n",
      "        1.0552, 1.0697, 1.0258, 1.0947, 1.0903, 1.0740, 1.1220, 1.0647, 1.0678,\n",
      "        1.1130, 1.0924, 1.0170, 1.0599, 1.0582, 1.0503, 1.0619, 1.0062, 1.0201,\n",
      "        1.0889, 1.0654, 1.0477, 1.0637, 1.0801, 1.0709, 1.0788, 1.0638, 1.0835,\n",
      "        1.0617, 1.0874, 1.0522])\n",
      "Layer: encoder.5.0.block.0.bias\n",
      "Bias: tensor([-1.6078e-02, -1.6273e-02,  1.2383e-02, -2.3829e-02,  1.8708e-02,\n",
      "         1.6946e-02,  1.4111e-02, -1.2017e-02,  1.7149e-02, -3.2132e-02,\n",
      "         1.2410e-02, -4.0590e-02,  5.3764e-03,  6.9027e-03, -2.3430e-02,\n",
      "         8.9716e-03,  1.2777e-02,  3.0438e-02, -6.2387e-02, -2.0528e-02,\n",
      "         1.7131e-02, -2.6253e-02, -3.1117e-02,  1.5881e-02,  3.3949e-03,\n",
      "         7.9496e-03,  9.9044e-03, -1.5853e-02,  2.6235e-02, -2.7324e-03,\n",
      "        -3.7921e-02, -5.8391e-03, -1.7872e-02, -4.7602e-02,  2.9936e-02,\n",
      "        -1.5696e-02, -1.5856e-02,  2.4063e-03,  1.3723e-02,  5.0936e-03,\n",
      "        -1.6100e-02,  2.6463e-02, -1.1127e-02,  1.1221e-03, -2.4758e-02,\n",
      "         6.5538e-02,  1.0533e-02, -4.7662e-03, -1.3937e-02,  3.3054e-03,\n",
      "         2.5036e-02, -3.0782e-03, -1.5611e-02,  2.8689e-03,  1.1901e-02,\n",
      "         1.6200e-02,  3.2648e-02, -2.1268e-02,  1.2268e-02,  1.2687e-02,\n",
      "        -1.7268e-02, -2.3912e-02, -1.5457e-02,  1.9240e-02, -8.9838e-03,\n",
      "        -1.8814e-02,  3.3694e-02,  3.2360e-05, -3.4748e-04,  2.3958e-03,\n",
      "         2.7666e-02, -1.9382e-02, -1.9382e-02, -1.3705e-02,  2.1839e-03,\n",
      "        -2.3970e-02,  2.0861e-02, -1.2946e-02,  1.0923e-04, -4.0910e-03,\n",
      "        -6.1317e-03,  6.5413e-02, -2.6748e-02, -1.6659e-02,  9.2704e-03,\n",
      "         1.0450e-02,  4.4999e-03,  1.0988e-02, -1.2671e-02, -4.1485e-02,\n",
      "        -3.3754e-02,  1.4674e-03,  9.4934e-03,  2.1821e-02,  2.9225e-04,\n",
      "         3.1063e-02,  1.2220e-02, -4.4459e-03,  1.0390e-02,  1.8050e-02,\n",
      "        -2.3337e-02, -1.8241e-02,  8.2383e-03,  1.9018e-02,  2.0319e-02,\n",
      "         1.3866e-02, -4.3113e-03, -3.8966e-02,  3.4529e-02, -5.3830e-03,\n",
      "         1.3873e-02,  6.7009e-02, -1.2489e-02, -1.1384e-03, -2.8559e-03,\n",
      "        -2.1934e-02, -1.2523e-03, -2.0337e-02,  1.1227e-02,  1.4287e-02,\n",
      "         1.0162e-02,  9.7022e-03, -1.2539e-03,  4.3092e-03,  1.7934e-02,\n",
      "         3.9632e-02,  1.1429e-02,  2.0224e-02, -3.7302e-04, -7.2591e-03,\n",
      "        -1.1631e-02, -7.0199e-03, -4.1661e-03, -1.8633e-02, -9.8691e-04,\n",
      "        -3.5259e-02,  2.7380e-02,  1.1591e-02, -3.4748e-02, -3.2195e-02,\n",
      "        -1.1440e-02, -3.1906e-02, -1.8460e-02, -1.7123e-02,  3.7281e-02,\n",
      "        -9.0607e-03, -4.3940e-02, -5.0359e-02,  1.0282e-02, -1.2650e-02,\n",
      "         1.3500e-02, -1.5335e-02,  2.9392e-02, -1.1979e-02, -1.5790e-03,\n",
      "        -7.6614e-03,  1.8989e-02, -1.8019e-02, -1.4556e-02, -9.5855e-03,\n",
      "        -3.8630e-03, -1.3162e-02,  6.4234e-02, -5.3949e-03, -2.0051e-02,\n",
      "        -8.4334e-03,  3.9684e-02, -1.9391e-03,  1.6300e-02, -1.8553e-02,\n",
      "        -1.1218e-03,  1.3178e-03, -4.1242e-03,  8.2383e-03, -2.5318e-02,\n",
      "         1.8986e-02,  1.9129e-02, -6.3024e-03,  1.9254e-02, -1.8060e-02,\n",
      "         4.9744e-02, -2.9559e-03,  8.1625e-03,  1.4531e-02,  3.0216e-02,\n",
      "        -1.0660e-02,  1.0798e-03,  2.0625e-02, -1.2890e-02, -3.2998e-03,\n",
      "         4.4649e-03,  3.8908e-02])\n",
      "Layer: encoder.5.1.block.0.weight\n",
      "Weight: tensor([0.8664, 0.9132, 0.9032, 0.9271, 0.9020, 0.9413, 0.9585, 0.9339, 0.9020,\n",
      "        0.9248, 0.8638, 0.9689, 0.9115, 0.8984, 0.9297, 0.9144, 0.8346, 0.8713,\n",
      "        0.9432, 0.8894, 0.9235, 0.9130, 0.9011, 0.8911, 0.8200, 0.8763, 0.8905,\n",
      "        0.8848, 0.8987, 0.8596, 0.9266, 0.8857, 0.9264, 0.8507, 0.8829, 0.9521,\n",
      "        0.9357, 0.9031, 0.8799, 0.8703, 0.8927, 0.8524, 0.9122, 0.9331, 0.9448,\n",
      "        0.8858, 0.8946, 0.8902, 0.9670, 0.8493, 0.8737, 0.9203, 0.9080, 0.9065,\n",
      "        0.9281, 0.9128, 0.9391, 0.9202, 0.9260, 0.9437, 0.9063, 0.9039, 0.8987,\n",
      "        0.9167, 0.9048, 0.8761, 0.9341, 0.9169, 0.9346, 0.8983, 1.0289, 0.9326,\n",
      "        0.8718, 0.9185, 0.9172, 0.8949, 0.9005, 0.8274, 0.9436, 0.9162, 0.8792,\n",
      "        0.9154, 0.8992, 0.9007, 0.9355, 0.9067, 0.9022, 0.9246, 0.9331, 0.8889,\n",
      "        0.8484, 0.9155, 0.9009, 0.8668, 0.9483, 0.9378, 0.8251, 0.9083, 0.8894,\n",
      "        0.8763, 0.8766, 0.9206, 0.9368, 0.9215, 0.9547, 0.9055, 0.9199, 0.9558,\n",
      "        0.9304, 0.8626, 0.8949, 0.9278, 0.8908, 0.9164, 0.8805, 0.9210, 0.9259,\n",
      "        0.9149, 0.9556, 0.9071, 0.8401, 0.9187, 0.9245, 0.9060, 0.8543, 0.8859,\n",
      "        0.9084, 0.8214, 0.8903, 0.9322, 0.9181, 0.9280, 0.8711, 0.8934, 0.9592,\n",
      "        0.8819, 0.8914, 0.8876, 0.9338, 0.8592, 0.9110, 0.8602, 0.9126, 0.8957,\n",
      "        0.8644, 0.9632, 0.8984, 0.8686, 0.9456, 0.9124, 0.8891, 0.8787, 0.8484,\n",
      "        0.8967, 0.9004, 0.8997, 0.9228, 0.8950, 0.8711, 0.9120, 0.9054, 0.8928,\n",
      "        0.9133, 0.8336, 0.9238, 0.9033, 0.9115, 0.9243, 0.9398, 0.8959, 0.9478,\n",
      "        0.9230, 0.9206, 0.8960, 0.8733, 0.8717, 0.9129, 0.9338, 0.9034, 0.8852,\n",
      "        0.9319, 0.8701, 0.9205, 0.8776, 0.9065, 0.9195, 0.9312, 0.8910, 0.9169,\n",
      "        0.8725, 0.8933, 0.9239])\n",
      "Layer: encoder.5.1.block.0.bias\n",
      "Bias: tensor([-0.0007, -0.0246, -0.0409,  0.0090, -0.0361,  0.0551, -0.0173,  0.0387,\n",
      "        -0.0111,  0.0249, -0.0319,  0.0333, -0.0365, -0.0024,  0.0163, -0.0037,\n",
      "        -0.0197, -0.0077,  0.0500,  0.0438, -0.0317,  0.0276, -0.0085,  0.0043,\n",
      "        -0.0276,  0.0143, -0.0283,  0.0176,  0.0331,  0.0201, -0.0181,  0.0264,\n",
      "         0.0234, -0.0026,  0.0011,  0.0444, -0.0159, -0.0566, -0.0203, -0.0121,\n",
      "        -0.0200, -0.0132,  0.0164,  0.0321, -0.0249,  0.0073,  0.0169,  0.0046,\n",
      "         0.0359,  0.0331, -0.0349,  0.0156,  0.0324, -0.0090,  0.0065,  0.0046,\n",
      "         0.0180, -0.0412, -0.0098, -0.0033, -0.0171, -0.0016, -0.0725,  0.0182,\n",
      "        -0.0171,  0.0414, -0.0313, -0.0148, -0.0498, -0.0244, -0.0504, -0.0284,\n",
      "        -0.0131, -0.0131,  0.0206,  0.0276, -0.0212,  0.0004, -0.0201,  0.0135,\n",
      "        -0.0166, -0.0255,  0.0097,  0.0461, -0.0426, -0.0126, -0.0004, -0.0171,\n",
      "         0.0165, -0.0215, -0.0412,  0.0017, -0.0521,  0.0256, -0.0181,  0.0156,\n",
      "         0.0146, -0.0246, -0.0294, -0.0254, -0.0054, -0.0056, -0.0305, -0.0023,\n",
      "         0.0293,  0.0537,  0.0406,  0.0021,  0.0082, -0.0182,  0.0180, -0.0236,\n",
      "        -0.0015, -0.0462, -0.0003, -0.0170,  0.0337,  0.0386, -0.0290,  0.0458,\n",
      "         0.0080,  0.0138, -0.0399,  0.0207, -0.0432, -0.0384,  0.0279, -0.0095,\n",
      "        -0.0323, -0.0412,  0.0125, -0.0600, -0.0145, -0.0169,  0.0379, -0.0247,\n",
      "         0.0212,  0.0252, -0.0274,  0.0081, -0.0254, -0.0169, -0.0076,  0.0498,\n",
      "        -0.0179,  0.0005,  0.0179,  0.0270, -0.0202, -0.0221, -0.0142,  0.0260,\n",
      "        -0.0124,  0.0103, -0.0088,  0.0264, -0.0157, -0.0226,  0.0199, -0.0444,\n",
      "         0.0043,  0.0267, -0.0099, -0.0347, -0.0300, -0.0029,  0.0006, -0.0304,\n",
      "        -0.0180,  0.0052, -0.0171,  0.0075, -0.0289, -0.0082, -0.0144,  0.0094,\n",
      "        -0.0122, -0.0154,  0.0288,  0.0190, -0.0190, -0.0053, -0.0158,  0.0271,\n",
      "         0.0177, -0.0563, -0.0251, -0.0088,  0.0239,  0.0202, -0.0010, -0.0292])\n",
      "Layer: classifier.2.weight\n",
      "Weight: tensor([0.9869, 1.0017, 1.0445, 1.0618, 1.0261, 0.9845, 1.0502, 1.0739, 1.0209,\n",
      "        1.0552, 1.1130, 0.9620, 1.0001, 1.0461, 1.0402, 1.0090, 0.9988, 1.0399,\n",
      "        1.1220, 1.0196, 1.0275, 1.0092, 1.0113, 1.0440, 1.0050, 1.0488, 1.0299,\n",
      "        1.0490, 1.0821, 1.0643, 1.0137, 1.0469, 0.9998, 1.0563, 1.0224, 1.0165,\n",
      "        1.0169, 1.0310, 1.1327, 0.9927, 1.0409, 1.0055, 1.0865, 0.9970, 0.9912,\n",
      "        1.0918, 1.0617, 1.0404, 1.0043, 0.9907, 1.0316, 1.0464, 1.0421, 1.0968,\n",
      "        1.0833, 1.0448, 1.1120, 1.0754, 1.0049, 0.9991, 1.0836, 1.0625, 1.0337,\n",
      "        1.0181, 1.0757, 1.0204, 1.0076, 1.0276, 1.0606, 1.0179, 1.0378, 1.0282,\n",
      "        1.0505, 0.9807, 0.9817, 0.9748, 1.0157, 1.0649, 1.0531, 1.0021, 1.0816,\n",
      "        1.0403, 0.9454, 1.0773, 0.9735, 1.0431, 1.0451, 1.0531, 1.0148, 1.0311,\n",
      "        1.0865, 1.0806, 0.9906, 0.9319, 1.0350, 1.0353, 1.0686, 0.9949, 1.0253,\n",
      "        0.9975, 1.1136, 0.9987, 0.9923, 0.9983, 1.0667, 1.0589, 1.0014, 0.9779,\n",
      "        0.9920, 1.1052, 1.0173, 1.0051, 1.0214, 1.0351, 1.0635, 1.0168, 1.0959,\n",
      "        1.0390, 1.0227, 0.9165, 1.0484, 1.0200, 1.0341, 1.0542, 1.0412, 1.0196,\n",
      "        0.9724, 1.0765, 1.1242, 1.0140, 1.1116, 1.0503, 1.0084, 1.0289, 0.9866,\n",
      "        1.0158, 1.0473, 0.9861, 1.1162, 1.0454, 0.9890, 1.0933, 1.0417, 1.1444,\n",
      "        1.0366, 1.0735, 0.9711, 1.0191, 0.9735, 1.0057, 1.0309, 1.0659, 0.9955,\n",
      "        1.0462, 1.1000, 1.1156, 1.1235, 1.0316, 1.0128, 1.0709, 1.0543, 1.0088,\n",
      "        1.0747, 1.0815, 1.0378, 1.0953, 0.9911, 1.0275, 1.0463, 1.0469, 1.0702,\n",
      "        1.0340, 1.0036, 1.0018, 1.0244, 1.0388, 1.0135, 0.9978, 1.1183, 1.0231,\n",
      "        1.0744, 1.0719, 1.0091, 1.0237, 0.9744, 1.0028, 1.0637, 1.0180, 1.0187,\n",
      "        1.0806, 1.0167, 1.0144])\n",
      "Layer: classifier.2.bias\n",
      "Bias: tensor([ 0.0211,  0.0312, -0.0157, -0.0103,  0.0441, -0.0115, -0.0159, -0.0180,\n",
      "        -0.0482, -0.0108, -0.0136, -0.0374, -0.0385,  0.0342, -0.0187,  0.0440,\n",
      "        -0.0430,  0.0021, -0.0053, -0.0638, -0.0624, -0.0447, -0.0449,  0.0447,\n",
      "        -0.0587,  0.0352, -0.0064, -0.0103,  0.0438, -0.0091, -0.0622,  0.0040,\n",
      "         0.0062,  0.0429,  0.0551,  0.0272,  0.0192, -0.0076,  0.0357, -0.0351,\n",
      "         0.0168,  0.0575,  0.0036, -0.0578,  0.0576,  0.0122, -0.0006, -0.0013,\n",
      "        -0.0114,  0.0337,  0.0471,  0.0285,  0.0209,  0.0424,  0.0227, -0.0083,\n",
      "        -0.0042,  0.0059, -0.0159, -0.0361, -0.0340,  0.0399,  0.0155, -0.0009,\n",
      "         0.0218,  0.0360,  0.0483,  0.0377,  0.0258, -0.0445,  0.0384,  0.0406,\n",
      "         0.0117, -0.0412,  0.0286,  0.0122,  0.0236,  0.0218, -0.0215,  0.0458,\n",
      "         0.0521,  0.0001, -0.0437, -0.0203,  0.0565, -0.0298,  0.0134,  0.0128,\n",
      "        -0.0195, -0.0484,  0.0038, -0.0576,  0.0058,  0.0328, -0.0467,  0.0381,\n",
      "         0.0008,  0.0343, -0.0522, -0.0519,  0.0159, -0.0380,  0.0024,  0.0163,\n",
      "         0.0327, -0.0396,  0.0672,  0.0468,  0.0260,  0.0043, -0.0124, -0.0494,\n",
      "         0.0283, -0.0452,  0.0357,  0.0512,  0.0237, -0.0273, -0.0009,  0.0287,\n",
      "        -0.0073, -0.0030, -0.0380,  0.0149,  0.0339,  0.0278, -0.0016,  0.0346,\n",
      "        -0.0139,  0.0476,  0.0088, -0.0215, -0.0337, -0.0259,  0.0553,  0.0131,\n",
      "         0.0370,  0.0470, -0.0239, -0.0632, -0.0160, -0.0039, -0.0066, -0.0047,\n",
      "         0.0346,  0.0317,  0.0725, -0.0442, -0.0428,  0.0668,  0.0157, -0.0510,\n",
      "         0.0038,  0.0262, -0.0455,  0.0337, -0.0022, -0.0106,  0.0528,  0.0465,\n",
      "        -0.0130, -0.0285, -0.0552,  0.0253,  0.0047,  0.0412, -0.0422,  0.0125,\n",
      "         0.0216,  0.0275, -0.0158, -0.0504,  0.0145, -0.0485, -0.0143, -0.0037,\n",
      "        -0.0127, -0.0068,  0.0054, -0.0416,  0.0032,  0.0027,  0.0342, -0.0289,\n",
      "        -0.0696, -0.0165,  0.0171, -0.0081,  0.0089,  0.0357,  0.0612, -0.0186])\n",
      "Layer: embedding.1.weight\n",
      "Weight: tensor([0.9763, 1.1794, 1.0540, 0.9597, 1.0171, 1.0861, 1.0550, 0.9394, 0.9926,\n",
      "        1.0066, 1.0856, 1.0469, 1.0351, 1.0689, 1.0363, 1.0783, 0.9876, 1.0140,\n",
      "        0.9533, 1.0099, 1.0908, 1.0059, 1.3189, 1.0094, 0.9708, 1.0146, 0.9981,\n",
      "        1.0783, 1.0453, 1.0456, 1.0470, 1.0382, 0.9861, 0.9522, 1.0071, 0.9453,\n",
      "        1.0351, 1.0156, 1.0955, 1.0577, 0.9811, 1.0590, 1.1276, 1.0049, 1.0481,\n",
      "        1.0446, 1.1219, 1.0959, 1.1301, 1.0836, 1.0246, 1.0260, 1.0008, 0.9655,\n",
      "        1.0157, 0.9914, 0.9598, 1.1086, 1.0758, 0.9941, 0.9535, 1.0444, 1.1592,\n",
      "        1.1205, 1.0030, 0.9549, 1.0745, 0.9694, 1.0330, 1.0165, 1.0218, 0.9385,\n",
      "        1.0227, 1.0286, 0.9911, 1.2549, 1.1891, 0.9531, 1.2683, 1.0619, 1.1554,\n",
      "        0.9922, 1.0853, 0.9166, 0.9572, 1.0368, 1.1463, 1.0610, 1.0578, 1.1370,\n",
      "        1.0092, 1.0657, 1.0425, 1.1585, 1.0037, 1.1291, 0.9353, 0.9857, 1.0155,\n",
      "        1.0933, 1.0011, 0.9182, 1.1013, 0.9631, 1.0593, 1.0023, 1.0260, 0.9553,\n",
      "        1.0156, 0.9996, 0.9892, 1.1065, 1.0792, 0.9759, 1.0127, 0.9863, 0.9858,\n",
      "        0.9700, 1.0409, 1.0783, 1.2104, 1.2052, 1.0515, 0.9951, 1.1813, 0.9933,\n",
      "        1.1504, 1.1209, 1.1482, 1.2555, 1.0844, 1.0605, 1.0313, 1.0203, 0.9316,\n",
      "        1.1468, 1.0221, 1.0201, 1.0245, 1.0113, 1.0008, 1.0367, 1.0388, 1.1096,\n",
      "        1.0418, 1.1412, 1.2091, 1.1846, 1.0231, 0.9411, 1.0144, 1.0066, 0.9903,\n",
      "        0.9935, 1.1429, 0.9555, 0.9565, 0.9944, 0.9893, 1.2564, 1.1078, 0.9146,\n",
      "        1.0088, 1.2993, 1.2671, 1.0480, 0.9761, 1.1506, 1.0120, 1.1055, 1.0566,\n",
      "        1.1127, 1.0153, 1.0457, 1.1703, 1.0813, 0.9947, 1.0804, 0.9989, 1.0018,\n",
      "        1.0076, 1.1038, 0.9962, 1.1952, 1.0173, 1.0891, 0.9991, 1.0901, 0.9804,\n",
      "        0.9743, 1.0594, 1.0090])\n",
      "Layer: embedding.1.bias\n",
      "Bias: tensor([-0.0265,  0.0628,  0.0573,  0.0398, -0.0323,  0.0412,  0.1057, -0.0695,\n",
      "         0.0037, -0.0364, -0.0133,  0.0017, -0.0350, -0.0004, -0.0065,  0.0929,\n",
      "         0.0097,  0.0426, -0.0618, -0.0552, -0.0736, -0.0351,  0.0725,  0.0048,\n",
      "        -0.0417, -0.0157,  0.0192,  0.0248,  0.0133, -0.0402, -0.0453, -0.0204,\n",
      "        -0.0670, -0.0302,  0.0609, -0.0912, -0.0416,  0.0208,  0.0159, -0.0375,\n",
      "        -0.0290,  0.1406, -0.0038, -0.0366,  0.0170, -0.0198,  0.0465,  0.0251,\n",
      "         0.0560,  0.0046,  0.0174, -0.0021, -0.0390,  0.0264, -0.0049, -0.0157,\n",
      "        -0.0510,  0.0661,  0.0324,  0.0055,  0.0813,  0.0197,  0.0602,  0.0469,\n",
      "        -0.0316, -0.0023, -0.0734, -0.0577, -0.0409, -0.0119, -0.0172,  0.0871,\n",
      "        -0.0263, -0.0280, -0.0237,  0.1014,  0.0980, -0.0521,  0.1165,  0.0728,\n",
      "         0.0746,  0.0383, -0.0102, -0.0889,  0.0240, -0.0354,  0.0722,  0.0395,\n",
      "        -0.0383,  0.0895, -0.0138, -0.0048, -0.0021,  0.0696,  0.0072,  0.0687,\n",
      "        -0.0797,  0.0032, -0.0227,  0.0569,  0.0048,  0.1153,  0.0615,  0.0319,\n",
      "         0.0181, -0.0179, -0.0347, -0.0777, -0.0379,  0.0058, -0.0203,  0.0123,\n",
      "         0.0081,  0.0226,  0.0611,  0.0054, -0.0205,  0.0108, -0.0388,  0.0058,\n",
      "         0.0711,  0.0434,  0.0114, -0.0059,  0.0662,  0.0088,  0.0771,  0.0368,\n",
      "         0.0515,  0.0322, -0.0170,  0.0227, -0.0295, -0.0236,  0.1625,  0.0609,\n",
      "         0.0874, -0.0199, -0.0271, -0.0243,  0.0010,  0.0287,  0.1083,  0.0132,\n",
      "        -0.0009,  0.0486,  0.0744,  0.0794, -0.0256, -0.0949,  0.0185,  0.0177,\n",
      "        -0.0237, -0.0034,  0.0499,  0.0010, -0.0432,  0.0147, -0.0061,  0.1122,\n",
      "         0.0223,  0.1098,  0.0567,  0.1595,  0.1422,  0.0010,  0.0363,  0.0531,\n",
      "        -0.0168,  0.0290,  0.0170,  0.0418, -0.0140, -0.0437,  0.0741,  0.0371,\n",
      "        -0.0249,  0.0049,  0.0037, -0.0140, -0.0101,  0.0235,  0.0189,  0.0498,\n",
      "        -0.0249, -0.0783,  0.0025,  0.0435, -0.0671, -0.0562, -0.0115, -0.0060])\n"
     ]
    }
   ],
   "execution_count": 286
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:58.306855Z",
     "start_time": "2024-05-04T16:33:55.355554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for var_name in model_quantized.state_dict():\n",
    "    print(var_name, \"\\t\", model_quantized.state_dict()[var_name])"
   ],
   "id": "306fd264326966ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding \t tensor([[ 0.7767,  0.4179, -0.9639,  ...,  1.0551,  0.6087,  0.1514],\n",
      "        [-0.7247,  0.7949,  0.6012,  ..., -0.0981, -0.7312, -1.5987],\n",
      "        [ 2.1631, -0.8226, -1.1655,  ..., -0.3369, -0.1976,  0.3002],\n",
      "        ...,\n",
      "        [ 0.2282,  0.4482, -0.0295,  ...,  0.1569, -0.5706, -0.3451],\n",
      "        [ 0.2163, -0.3091, -1.6473,  ..., -1.4622,  0.0603,  0.7738],\n",
      "        [-1.3462, -0.3745, -1.0837,  ...,  0.3060, -0.3337,  0.5932]])\n",
      "encoder.0.0.block.0.weight \t tensor([0.9750, 1.0008, 1.0394, 0.9690, 1.0381, 0.9551, 1.0361, 0.9804, 1.0251,\n",
      "        0.9770, 0.9904, 1.0317, 1.0246, 0.9874, 1.0160, 0.9927, 1.0636, 0.9876,\n",
      "        0.9794, 1.0233, 1.0472, 1.0000, 1.0550, 1.0249, 0.9639, 1.0036, 1.0061,\n",
      "        1.0274, 1.0092, 1.0298, 1.0008, 1.0026, 1.0025, 1.0162, 0.9898, 1.0281,\n",
      "        0.9993, 1.0172, 1.0028, 0.9791, 1.0002, 0.9492, 0.9808, 1.0047, 1.0121,\n",
      "        1.0756, 1.0484, 1.0530, 1.0308, 1.0548, 1.0228, 0.9818, 1.0365, 1.0565,\n",
      "        1.0072, 0.9981, 1.0169, 1.0323, 0.9796, 1.0250, 0.9712, 0.9878, 1.0699,\n",
      "        1.0638, 1.0120, 0.9914, 0.9969, 1.0180, 0.9557, 0.9869, 0.9812, 1.0278,\n",
      "        0.9913, 0.9831, 0.9790, 0.9776, 1.0698, 0.9919, 0.9821, 1.0128, 1.0208,\n",
      "        0.9713, 1.0205, 0.9782, 1.0168, 0.9990, 1.0669, 0.9685, 0.9903, 1.0564,\n",
      "        1.0028, 0.9732, 0.9900, 1.0016, 0.9764, 1.0538, 0.9700, 1.0321, 0.9853,\n",
      "        1.0275, 0.9730, 1.0576, 1.0065, 0.9865, 0.9718, 1.0318, 0.9698, 1.0009,\n",
      "        0.9997, 0.9998, 1.0457, 1.0856, 1.0419, 1.0456, 1.0426, 1.0052, 0.9880,\n",
      "        1.0245, 0.9877, 1.0324, 1.0372, 1.0039, 1.0107, 1.0121, 1.0386, 1.0081,\n",
      "        1.0086, 1.0957, 1.0536, 1.0189, 1.0088, 1.0196, 0.9778, 1.0053, 0.9693,\n",
      "        1.0646, 0.9503, 1.0302, 0.9749, 1.0263, 1.0327, 1.0324, 1.0163, 1.0140,\n",
      "        1.0167, 1.0796, 1.0235, 1.0249, 0.9832, 0.9742, 1.0106, 0.9565, 0.9845,\n",
      "        1.0191, 1.0266, 1.0431, 1.0238, 0.9800, 0.9822, 1.0213, 1.0231, 0.9399,\n",
      "        1.0111, 0.9937, 1.0118, 1.0086, 0.9801, 1.1129, 1.0025, 1.0467, 1.0208,\n",
      "        1.0143, 0.9412, 0.9968, 1.0069, 1.0380, 0.9438, 1.0153, 0.9957, 0.9649,\n",
      "        0.9793, 1.0316, 1.0387, 1.0581, 0.9927, 1.0166, 0.9882, 1.0216, 1.0175,\n",
      "        0.9699, 1.0435, 1.0296])\n",
      "encoder.0.0.block.0.bias \t tensor([-0.0099,  0.0175,  0.0103,  0.0544,  0.0166,  0.0551, -0.0056, -0.0475,\n",
      "         0.0165, -0.0376,  0.0286, -0.0341,  0.0089, -0.0023, -0.0366,  0.0168,\n",
      "         0.0178,  0.0113, -0.0455, -0.0371,  0.0378, -0.0489,  0.0080,  0.0226,\n",
      "        -0.0426,  0.0028,  0.0143, -0.0218, -0.0121,  0.0311,  0.0449, -0.0221,\n",
      "        -0.0491, -0.0084,  0.0581, -0.0353,  0.0214,  0.0234,  0.0338,  0.0184,\n",
      "        -0.0098,  0.0452,  0.0186, -0.0239, -0.0029, -0.0277, -0.0265, -0.0053,\n",
      "        -0.0056, -0.0203,  0.0013,  0.0084, -0.0211, -0.0017,  0.0089, -0.0236,\n",
      "        -0.0526,  0.0114,  0.0090,  0.0126,  0.0481, -0.0087, -0.0158,  0.0018,\n",
      "         0.0072,  0.0218,  0.0349, -0.0650,  0.0131,  0.0209,  0.0152,  0.0220,\n",
      "         0.0230,  0.0175, -0.0170,  0.0128,  0.0038, -0.0601,  0.0150,  0.0149,\n",
      "        -0.0081,  0.0282, -0.0190, -0.0743,  0.0226,  0.0216, -0.0114,  0.0103,\n",
      "         0.0239, -0.0071,  0.0069,  0.0270, -0.0171, -0.0114,  0.0192,  0.0134,\n",
      "        -0.0758, -0.0113,  0.0405,  0.0059,  0.0075,  0.0124,  0.0117,  0.0107,\n",
      "        -0.0002,  0.0083, -0.0284, -0.0610, -0.0523,  0.0102, -0.0296, -0.0267,\n",
      "        -0.0023,  0.0212, -0.0020,  0.0172, -0.0306,  0.0098,  0.0205,  0.0220,\n",
      "         0.0179,  0.0124,  0.0054, -0.0029,  0.0211,  0.0134, -0.0328,  0.0132,\n",
      "        -0.0132,  0.0205, -0.0649, -0.0160,  0.0219,  0.0268,  0.0212,  0.0089,\n",
      "         0.0598,  0.0410,  0.0189, -0.0208, -0.0115,  0.0057,  0.0106, -0.0285,\n",
      "        -0.0001, -0.0121,  0.0220,  0.0150,  0.0228, -0.0639,  0.0042,  0.0202,\n",
      "        -0.0070,  0.0355,  0.0124,  0.0082, -0.0410,  0.0208,  0.0175,  0.0074,\n",
      "        -0.0190,  0.0273,  0.0243,  0.0354,  0.0238,  0.0323,  0.0510, -0.0106,\n",
      "         0.0108, -0.0516, -0.0069, -0.0005,  0.0417,  0.0186, -0.0255,  0.0017,\n",
      "        -0.0525, -0.0008,  0.0209, -0.0207, -0.0056,  0.0190,  0.0051,  0.0082,\n",
      "         0.0186,  0.0309,  0.0127,  0.0068, -0.0250, -0.0340, -0.0517, -0.0038])\n",
      "encoder.0.0.block.1.queries_projection.scale \t tensor(1.)\n",
      "encoder.0.0.block.1.queries_projection.zero_point \t tensor(0)\n",
      "encoder.0.0.block.1.queries_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.0.0.block.1.queries_projection._packed_params._packed_params \t (tensor([[-0.1168,  0.0417,  0.0751,  ...,  0.0150, -0.0184,  0.0684],\n",
      "        [-0.0017, -0.0033, -0.0167,  ..., -0.0100, -0.0184, -0.0401],\n",
      "        [-0.1152,  0.1252,  0.0250,  ..., -0.0150,  0.0150, -0.0768],\n",
      "        ...,\n",
      "        [ 0.0701, -0.0267, -0.0117,  ..., -0.0567, -0.0751,  0.0451],\n",
      "        [ 0.0050, -0.0300,  0.0317,  ...,  0.0184, -0.0484,  0.0184],\n",
      "        [ 0.0551, -0.0901, -0.0684,  ...,  0.0718, -0.0384,  0.0584]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0016688834875822067,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0613,  0.0520, -0.0378, -0.0085, -0.0466,  0.0948, -0.0120,  0.0250,\n",
      "         0.0082,  0.0040, -0.0573, -0.0287,  0.0496, -0.0564, -0.0340,  0.0264,\n",
      "        -0.0421,  0.0243, -0.0334, -0.0498,  0.0702,  0.0618,  0.0083,  0.0187,\n",
      "         0.0402, -0.0576, -0.0033, -0.0152, -0.0290,  0.0737, -0.0037, -0.0544,\n",
      "        -0.0712, -0.0034,  0.0477,  0.0072, -0.0269,  0.0864, -0.0595, -0.0414,\n",
      "        -0.0122,  0.0440, -0.0066,  0.0985, -0.0628,  0.0584, -0.0099,  0.0021,\n",
      "        -0.0018, -0.0112,  0.0195, -0.0335, -0.0618,  0.0047, -0.0649,  0.0007,\n",
      "        -0.0478, -0.0016, -0.0282, -0.0555,  0.0281, -0.0389, -0.0721,  0.0244,\n",
      "         0.0081, -0.0386,  0.0230,  0.0234,  0.0585, -0.0102, -0.0704,  0.0274,\n",
      "         0.0430,  0.0684, -0.0194, -0.0304, -0.0014, -0.0090, -0.0123,  0.0029,\n",
      "        -0.0516, -0.0244, -0.0539, -0.0393,  0.0458,  0.0296,  0.0838,  0.0362,\n",
      "        -0.0228,  0.0722,  0.0193, -0.0116,  0.0273,  0.0665, -0.0429,  0.0192,\n",
      "         0.0434, -0.0433, -0.0653, -0.0161, -0.0287,  0.0726, -0.0297,  0.0330,\n",
      "         0.0728, -0.0363, -0.0496, -0.0413,  0.0305, -0.0054, -0.0188, -0.0005,\n",
      "         0.0739, -0.0441, -0.0326,  0.0046,  0.0370, -0.0763,  0.0248,  0.0700,\n",
      "         0.0088, -0.0066, -0.0450, -0.0673, -0.0548, -0.0791, -0.0429,  0.0483,\n",
      "        -0.0714,  0.0453,  0.0316, -0.0406,  0.0506, -0.0301, -0.0129,  0.0222,\n",
      "        -0.0189, -0.0641,  0.0417,  0.0214, -0.0140,  0.0905,  0.0700, -0.0629,\n",
      "         0.0295,  0.0549, -0.0110,  0.0551,  0.0119,  0.0333,  0.0682,  0.0461,\n",
      "         0.0361, -0.0567, -0.0915,  0.1000, -0.0550,  0.0219,  0.0589,  0.0267,\n",
      "         0.0615, -0.0372,  0.0237, -0.0647, -0.0169,  0.0859, -0.0509, -0.0460,\n",
      "         0.0509,  0.0057,  0.0174, -0.0441, -0.0263, -0.0093, -0.0304, -0.0279,\n",
      "         0.0099, -0.0150, -0.0148,  0.0692,  0.0157, -0.0041, -0.0114,  0.0584,\n",
      "         0.0185,  0.0327,  0.0666,  0.0179,  0.0349,  0.0456, -0.0387, -0.0060],\n",
      "       requires_grad=True))\n",
      "encoder.0.0.block.1.values_projection.scale \t tensor(1.)\n",
      "encoder.0.0.block.1.values_projection.zero_point \t tensor(0)\n",
      "encoder.0.0.block.1.values_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.0.0.block.1.values_projection._packed_params._packed_params \t (tensor([[ 0.0127,  0.0361, -0.0191,  ..., -0.0594, -0.0148,  0.0551],\n",
      "        [ 0.0509,  0.0891,  0.0679,  ..., -0.0636, -0.0085, -0.0170],\n",
      "        [-0.0085,  0.0297,  0.0658,  ...,  0.0488,  0.0170, -0.0488],\n",
      "        ...,\n",
      "        [ 0.0870, -0.0530,  0.0488,  ..., -0.0700,  0.0700,  0.0424],\n",
      "        [-0.0170,  0.0339, -0.0721,  ...,  0.0976, -0.0912, -0.0806],\n",
      "        [ 0.0297,  0.0445,  0.0297,  ...,  0.0509,  0.0530, -0.0064]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0021209814585745335,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0257,  0.0026, -0.0435,  0.0223, -0.0488, -0.0334,  0.0345,  0.0421,\n",
      "         0.0040, -0.0674, -0.0015,  0.0579, -0.0344,  0.0039,  0.0616, -0.0659,\n",
      "        -0.0444, -0.0041,  0.0649,  0.0031, -0.0309, -0.0728, -0.0393, -0.0472,\n",
      "        -0.0327,  0.0390,  0.0276, -0.0639,  0.0510,  0.0243, -0.0119, -0.0343,\n",
      "         0.0715, -0.0428, -0.0126, -0.0089, -0.0827,  0.0467, -0.0103, -0.0401,\n",
      "         0.0450,  0.0255, -0.0485, -0.0295,  0.0004, -0.0046,  0.0484,  0.0540,\n",
      "        -0.0216,  0.0450, -0.0511,  0.0280,  0.0688,  0.0449,  0.0497, -0.0461,\n",
      "         0.0205,  0.0274, -0.0279, -0.0141,  0.0414,  0.0226,  0.0084, -0.0678,\n",
      "         0.0301,  0.0176,  0.0356,  0.0140,  0.0035,  0.0728,  0.0572, -0.0657,\n",
      "         0.0207, -0.0353,  0.0916,  0.0003, -0.0239,  0.0105,  0.0063,  0.0461,\n",
      "        -0.0269,  0.0687,  0.0187, -0.0288, -0.0546, -0.0307,  0.0773, -0.0178,\n",
      "         0.0510, -0.0175, -0.0123, -0.0416,  0.0041,  0.0002,  0.0434, -0.0142,\n",
      "        -0.0032,  0.0029,  0.0193, -0.0612, -0.0391,  0.0413, -0.0212,  0.0285,\n",
      "        -0.0198, -0.0591, -0.0037,  0.0074, -0.0529, -0.0055, -0.0693,  0.0015,\n",
      "         0.0299,  0.0407,  0.0039,  0.0369, -0.0397,  0.0472, -0.0241,  0.0469,\n",
      "        -0.0364, -0.0774,  0.0587, -0.0408,  0.0536,  0.0290,  0.0085, -0.0369,\n",
      "         0.0030,  0.0776,  0.0723,  0.0286, -0.0495, -0.0403, -0.0276, -0.0105,\n",
      "        -0.0224, -0.0004,  0.0405,  0.0192,  0.0149, -0.0744, -0.0262,  0.0219,\n",
      "         0.0488, -0.0639, -0.0182,  0.0074, -0.0487, -0.0110, -0.0324, -0.0497,\n",
      "        -0.0420,  0.0241, -0.0372, -0.0490,  0.0343,  0.0264, -0.0495,  0.0094,\n",
      "        -0.0217, -0.0300, -0.0006,  0.0545, -0.0522,  0.0087, -0.0169,  0.0314,\n",
      "         0.0527, -0.0178, -0.0333,  0.0146, -0.0110, -0.0500,  0.0478, -0.0079,\n",
      "         0.0110,  0.0332,  0.0568,  0.0178,  0.0498,  0.0560,  0.0133, -0.0612,\n",
      "        -0.0161, -0.0422, -0.0020, -0.0388, -0.0190,  0.0241, -0.0686,  0.0050],\n",
      "       requires_grad=True))\n",
      "encoder.0.0.block.1.keys_projection.scale \t tensor(1.)\n",
      "encoder.0.0.block.1.keys_projection.zero_point \t tensor(0)\n",
      "encoder.0.0.block.1.keys_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.0.0.block.1.keys_projection._packed_params._packed_params \t (tensor([[ 0.0068, -0.0579, -0.0392,  ..., -0.0903, -0.0153,  0.0017],\n",
      "        [-0.0119,  0.0375,  0.0034,  ...,  0.0222,  0.1193, -0.0579],\n",
      "        [ 0.0494,  0.0222,  0.0358,  ..., -0.0170, -0.0392,  0.0835],\n",
      "        ...,\n",
      "        [ 0.0102,  0.0699, -0.0409,  ..., -0.0665, -0.1022, -0.0596],\n",
      "        [ 0.0187, -0.0511, -0.0409,  ..., -0.0375, -0.0682, -0.0665],\n",
      "        [ 0.0341, -0.0187, -0.0136,  ...,  0.0886, -0.0187,  0.0273]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0017040540697053075,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0829,  0.0307, -0.0355, -0.0944, -0.0518,  0.0797,  0.0002, -0.0050,\n",
      "        -0.0257, -0.0756,  0.0025, -0.0179,  0.0706,  0.0502, -0.0378, -0.0303,\n",
      "        -0.0269, -0.0156,  0.0483,  0.0507, -0.0074, -0.0317,  0.0589,  0.0021,\n",
      "        -0.0346, -0.0512, -0.0206, -0.0473,  0.0150, -0.0662, -0.0578,  0.0612,\n",
      "         0.0010, -0.0366, -0.0519, -0.0120,  0.0281,  0.1059,  0.0847, -0.0829,\n",
      "        -0.0237,  0.0159, -0.0073,  0.0694, -0.0310,  0.0746,  0.0256,  0.0865,\n",
      "        -0.0323, -0.0305,  0.0152, -0.0310,  0.0106,  0.0477, -0.0402, -0.0453,\n",
      "         0.0597,  0.0740, -0.0345, -0.0625, -0.0654, -0.0275,  0.0091, -0.0757,\n",
      "         0.0122, -0.0049, -0.0638,  0.0101,  0.0377, -0.0297,  0.0506, -0.0274,\n",
      "         0.0268,  0.0262, -0.0179,  0.0963, -0.0532, -0.0054,  0.0135,  0.0326,\n",
      "        -0.0268,  0.0611, -0.0050, -0.0565, -0.0753,  0.0026,  0.0048,  0.0177,\n",
      "         0.0374, -0.0294, -0.0492, -0.0294, -0.0312,  0.0296,  0.0402, -0.0613,\n",
      "        -0.0919,  0.0085, -0.0463,  0.0868,  0.0039, -0.0460,  0.0645, -0.0330,\n",
      "         0.0491,  0.0405,  0.0674, -0.0581,  0.0685, -0.0701,  0.0115,  0.0356,\n",
      "        -0.0435, -0.0274,  0.0200, -0.0027,  0.0106, -0.0168, -0.0017, -0.0829,\n",
      "        -0.0076,  0.0415,  0.0162,  0.0299,  0.0211,  0.0235, -0.0605, -0.0601,\n",
      "        -0.0396,  0.0511,  0.0086, -0.0052,  0.0398,  0.0374, -0.0314,  0.0911,\n",
      "        -0.0308,  0.0155, -0.0483, -0.0840, -0.0076,  0.0105,  0.0609, -0.0067,\n",
      "        -0.0415, -0.0361, -0.0137,  0.0406,  0.0459, -0.0146,  0.0756,  0.0221,\n",
      "         0.0349,  0.0247, -0.0425, -0.0072, -0.0666,  0.0320, -0.0037,  0.0346,\n",
      "         0.0161,  0.0506, -0.0877,  0.0483,  0.0824, -0.0312, -0.0619, -0.0393,\n",
      "        -0.0275, -0.0417, -0.0209, -0.0246, -0.0407, -0.0268,  0.0581,  0.0317,\n",
      "         0.0205,  0.0111,  0.0192, -0.0635,  0.0264,  0.0505, -0.0859, -0.0266,\n",
      "         0.0739,  0.0324,  0.0588,  0.0778, -0.0187,  0.0558,  0.0281,  0.0332],\n",
      "       requires_grad=True))\n",
      "encoder.0.0.block.1.final_projection.scale \t tensor(1.)\n",
      "encoder.0.0.block.1.final_projection.zero_point \t tensor(0)\n",
      "encoder.0.0.block.1.final_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.0.0.block.1.final_projection._packed_params._packed_params \t (tensor([[-0.0897,  0.0249,  0.0125,  ...,  0.1371, -0.1097,  0.0299],\n",
      "        [ 0.0100, -0.0922, -0.1371,  ..., -0.0723,  0.0723, -0.0224],\n",
      "        [ 0.0224, -0.0449, -0.0224,  ...,  0.0947, -0.0598, -0.0125],\n",
      "        ...,\n",
      "        [ 0.0673,  0.0374,  0.0324,  ..., -0.0548,  0.0773,  0.0922],\n",
      "        [ 0.0199, -0.0548, -0.0025,  ..., -0.0623,  0.0399,  0.0673],\n",
      "        [ 0.0598, -0.0199,  0.0199,  ...,  0.0274, -0.0100,  0.0175]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0024929861538112164,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 4.3030e-02,  4.2485e-02,  5.0295e-02,  2.6345e-02,  2.9572e-02,\n",
      "         5.8398e-02,  4.7936e-02, -5.6216e-02, -6.6108e-03, -2.5265e-02,\n",
      "        -9.7642e-03,  3.7627e-02,  2.5566e-02,  1.4230e-02,  7.8935e-03,\n",
      "        -3.2474e-02,  1.4033e-02, -6.8954e-02, -6.9097e-02,  3.4755e-02,\n",
      "         7.8711e-02,  1.8010e-03, -7.3398e-02,  2.8414e-02,  5.5729e-03,\n",
      "        -1.4818e-02, -3.5275e-02, -5.2306e-03,  1.2474e-02, -3.0924e-02,\n",
      "        -1.0861e-02, -5.7460e-02, -4.7005e-02,  5.3832e-02,  4.4195e-02,\n",
      "         5.9931e-02,  6.0091e-02, -1.2977e-02,  7.7474e-02,  4.2123e-02,\n",
      "        -1.3094e-02,  3.3608e-02, -5.7710e-03, -5.0297e-02,  8.2471e-03,\n",
      "         1.1802e-02,  9.8305e-03, -1.9483e-02, -1.0647e-03,  2.9471e-02,\n",
      "        -2.6632e-02,  4.1429e-02, -9.1181e-03,  5.3476e-02,  1.6041e-02,\n",
      "         2.3369e-02, -1.5499e-02, -1.0070e-02, -4.1414e-02, -6.1955e-02,\n",
      "         4.8548e-02,  3.2012e-02, -1.6766e-02, -2.0780e-03,  5.5884e-02,\n",
      "         1.7922e-02,  3.4534e-02,  2.2906e-02,  5.6289e-02, -6.8687e-03,\n",
      "         1.0043e-02,  4.2559e-02, -5.1396e-02,  2.7990e-02,  4.6309e-02,\n",
      "        -1.3638e-02,  2.1931e-03, -1.8942e-02,  1.9105e-02, -1.4487e-02,\n",
      "         1.1370e-02, -1.2680e-02, -2.1937e-02, -5.1997e-02,  2.3270e-02,\n",
      "        -2.4356e-02,  1.1572e-02,  9.9781e-03,  3.7717e-02,  3.2247e-02,\n",
      "         2.0946e-02, -4.3110e-02, -1.0660e-02,  4.7037e-02,  2.7900e-02,\n",
      "        -4.5158e-02, -7.6268e-02, -6.5789e-02, -2.5630e-02, -5.4969e-02,\n",
      "        -5.7394e-02, -2.7497e-02,  6.1078e-02, -5.6180e-02,  3.4444e-02,\n",
      "         1.1064e-02,  3.5664e-02, -1.4270e-02,  6.1648e-02, -2.1961e-02,\n",
      "        -4.5332e-02,  3.6038e-02, -2.7467e-02,  2.3457e-02,  3.6675e-02,\n",
      "        -4.5762e-02, -4.9344e-02, -1.1569e-02,  4.3133e-02, -1.7746e-02,\n",
      "        -2.0865e-02,  4.4789e-02,  1.7102e-02,  2.5895e-02,  2.1340e-02,\n",
      "        -4.6350e-02,  4.3408e-02, -6.2044e-02, -2.5755e-02, -1.6183e-02,\n",
      "        -3.6506e-03, -6.7016e-03,  4.6970e-02, -4.7008e-02, -5.2151e-02,\n",
      "         7.6065e-05,  2.3663e-02, -1.9345e-02, -3.3276e-02, -9.5980e-03,\n",
      "         1.8248e-02, -1.1901e-03, -2.9343e-02,  3.8731e-02,  2.1080e-03,\n",
      "         2.6622e-02,  2.4019e-02,  2.9884e-02,  3.9810e-02, -2.4162e-02,\n",
      "        -1.9282e-02,  1.3395e-02, -2.9183e-02, -3.8201e-02,  1.6877e-02,\n",
      "        -1.9893e-02,  3.4773e-03,  2.2126e-02,  4.1154e-02, -3.3532e-02,\n",
      "        -4.9512e-02, -3.5711e-03, -4.8716e-02, -4.6204e-02, -3.8139e-02,\n",
      "         5.6920e-02,  4.2506e-02,  2.9025e-03,  1.4765e-02,  2.2795e-02,\n",
      "         5.8285e-02, -4.1102e-02,  6.3477e-02,  4.5321e-02, -1.0703e-02,\n",
      "        -1.8899e-02,  8.9426e-03, -1.1994e-02,  4.4311e-02,  1.4638e-02,\n",
      "         4.5524e-02, -2.7347e-02,  1.8170e-02,  3.7883e-03, -3.2852e-02,\n",
      "         2.4933e-02,  1.4562e-02,  5.1783e-02, -5.3322e-03, -1.3875e-02,\n",
      "         7.4793e-03,  1.6891e-02], requires_grad=True))\n",
      "encoder.0.1.block.0.weight \t tensor([1.0418, 1.0060, 0.9843, 1.0244, 0.9766, 0.9731, 0.9900, 1.0045, 0.9934,\n",
      "        0.9246, 0.9790, 1.0001, 1.0164, 1.0122, 1.0299, 1.0022, 0.9819, 0.9828,\n",
      "        0.9876, 1.0061, 1.0116, 0.9758, 1.0600, 1.0011, 1.0177, 0.9773, 0.9597,\n",
      "        1.0402, 1.0084, 1.0014, 0.9844, 1.0189, 0.9999, 1.0244, 0.9638, 1.0114,\n",
      "        0.9992, 0.9415, 1.0154, 1.0409, 0.9924, 0.9905, 0.9760, 1.0082, 1.0073,\n",
      "        1.0096, 1.0062, 1.0026, 1.0364, 1.0243, 1.0242, 0.9666, 1.0254, 1.0063,\n",
      "        1.0002, 0.9557, 0.9972, 1.0193, 0.9928, 1.0067, 1.0291, 1.0142, 1.0217,\n",
      "        1.0477, 1.0090, 0.9806, 0.9889, 1.0190, 0.9976, 0.9598, 0.9704, 1.0153,\n",
      "        0.9871, 0.9514, 0.9977, 1.0173, 1.0370, 0.9797, 1.0120, 0.9704, 1.0046,\n",
      "        0.9635, 1.0448, 1.0170, 0.9863, 0.9857, 1.0390, 0.9805, 1.0160, 1.0095,\n",
      "        0.9687, 0.9715, 0.9976, 1.0055, 0.9779, 1.0118, 0.9559, 0.9797, 0.9794,\n",
      "        1.0075, 0.9688, 0.9967, 1.0082, 0.9840, 0.9996, 1.0197, 0.9926, 0.9955,\n",
      "        1.0229, 0.9760, 1.0045, 1.0288, 0.9705, 0.9852, 0.9655, 0.9944, 0.9620,\n",
      "        1.0057, 0.9649, 1.0342, 1.0620, 0.9784, 1.0075, 0.9833, 1.0068, 0.9686,\n",
      "        1.0129, 1.0176, 1.0270, 1.0250, 1.0131, 1.0113, 0.9810, 1.0118, 0.9920,\n",
      "        1.0119, 1.0422, 0.9986, 1.0011, 0.9944, 1.0000, 0.9705, 0.9171, 1.0277,\n",
      "        1.0099, 1.0078, 1.0219, 1.0379, 0.9346, 0.9725, 0.9968, 0.9953, 0.9346,\n",
      "        0.9512, 1.0464, 0.9894, 1.0040, 0.9928, 1.0144, 1.0170, 1.0321, 0.9679,\n",
      "        1.0077, 1.0281, 1.0248, 1.0251, 0.9380, 1.0445, 0.9918, 1.0437, 1.0214,\n",
      "        0.9882, 1.0295, 0.9880, 1.0508, 1.0101, 0.9888, 0.9767, 0.9874, 0.9685,\n",
      "        1.0250, 1.0143, 0.9962, 1.0165, 0.9456, 0.9749, 1.0488, 1.0022, 1.0098,\n",
      "        0.9623, 1.0359, 0.9558])\n",
      "encoder.0.1.block.0.bias \t tensor([ 0.0205, -0.0077, -0.0075, -0.0099,  0.0061,  0.0484,  0.0246, -0.0096,\n",
      "        -0.0252, -0.0579,  0.0010, -0.0087, -0.0341, -0.0034,  0.0226, -0.0206,\n",
      "         0.0332, -0.0106, -0.0256, -0.0182,  0.0377, -0.0070, -0.0336,  0.0022,\n",
      "        -0.0037, -0.0141,  0.0237, -0.0114, -0.0150, -0.0226,  0.0205, -0.0101,\n",
      "        -0.0109,  0.0186, -0.0110,  0.0039,  0.0059,  0.0316,  0.0525, -0.0080,\n",
      "         0.0172,  0.0259, -0.0392, -0.0233, -0.0301, -0.0019,  0.0143,  0.0109,\n",
      "        -0.0022, -0.0114,  0.0370,  0.0198,  0.0043,  0.0061,  0.0246, -0.0153,\n",
      "        -0.0208,  0.0226,  0.0059,  0.0013,  0.0440, -0.0301,  0.0181, -0.0015,\n",
      "        -0.0197, -0.0087,  0.0260,  0.0063,  0.0023,  0.0603,  0.0093,  0.0124,\n",
      "         0.0101,  0.0285, -0.0021, -0.0150, -0.0005, -0.0056, -0.0203, -0.0255,\n",
      "        -0.0393,  0.0072,  0.0196,  0.0170,  0.0136, -0.0124,  0.0059,  0.0412,\n",
      "        -0.0156,  0.0155,  0.0103, -0.0026,  0.0015, -0.0004, -0.0462,  0.0190,\n",
      "        -0.0288, -0.0055, -0.0130, -0.0194,  0.0107,  0.0117, -0.0022, -0.0116,\n",
      "         0.0200, -0.0204, -0.0221, -0.0108, -0.0146,  0.0037,  0.0034, -0.0085,\n",
      "        -0.0329,  0.0159, -0.0194, -0.0018, -0.0502, -0.0068,  0.0003,  0.0074,\n",
      "         0.0035,  0.0130, -0.0103, -0.0241,  0.0200,  0.0086,  0.0152,  0.0023,\n",
      "         0.0123, -0.0278,  0.0173,  0.0167,  0.0618,  0.0232,  0.0274,  0.0083,\n",
      "         0.0339, -0.0020,  0.0419, -0.0569,  0.0228,  0.0303,  0.0323, -0.0174,\n",
      "         0.0016,  0.0124,  0.0039,  0.0118,  0.0254, -0.0257,  0.0132,  0.0182,\n",
      "         0.0128,  0.0383,  0.0093, -0.0090,  0.0110, -0.0200, -0.0369, -0.0105,\n",
      "         0.0148,  0.0083, -0.0299, -0.0094,  0.0043, -0.0145,  0.0160,  0.0201,\n",
      "        -0.0085, -0.0011,  0.0121,  0.0072, -0.0082,  0.0357,  0.0118,  0.0290,\n",
      "         0.0075, -0.0462,  0.0007, -0.0310, -0.0229,  0.0222,  0.0081, -0.0074,\n",
      "         0.0065,  0.0386, -0.0168, -0.0166, -0.0256, -0.0556,  0.0115, -0.0077])\n",
      "encoder.0.1.block.1.0.scale \t tensor(1.)\n",
      "encoder.0.1.block.1.0.zero_point \t tensor(0)\n",
      "encoder.0.1.block.1.0._packed_params.dtype \t torch.qint8\n",
      "encoder.0.1.block.1.0._packed_params._packed_params \t (tensor([[-0.0110,  0.0570,  0.0460,  ..., -0.0368,  0.0423,  0.0000],\n",
      "        [-0.0276, -0.0386,  0.0460,  ...,  0.0589, -0.0092,  0.0037],\n",
      "        [-0.0920,  0.0828,  0.0184,  ..., -0.0736,  0.0074,  0.0313],\n",
      "        ...,\n",
      "        [-0.1030, -0.0368,  0.0681,  ...,  0.0662,  0.0534, -0.0129],\n",
      "        [-0.0681, -0.0386, -0.0258,  ..., -0.0626, -0.0184, -0.0552],\n",
      "        [-0.0202, -0.0460, -0.0515,  ...,  0.0184, -0.0386,  0.0294]],\n",
      "       size=(768, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0018401178531348705,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 2.8576e-02,  3.5646e-02, -2.1681e-02, -7.6115e-03, -1.1041e-01,\n",
      "        -4.6143e-02, -8.5512e-02,  2.8941e-02, -4.8355e-02, -9.8752e-02,\n",
      "        -4.0317e-02, -3.1645e-02, -1.8798e-02, -2.0200e-03, -4.5121e-02,\n",
      "        -1.8517e-02, -7.5882e-02, -6.9157e-02,  1.0584e-02,  3.0456e-02,\n",
      "        -1.0159e-01, -8.3359e-02,  3.3073e-02,  2.7604e-02, -2.1450e-02,\n",
      "        -8.5137e-03, -2.8966e-02,  2.9291e-03,  3.7450e-02, -7.3833e-02,\n",
      "         2.8369e-03, -6.3342e-02,  1.9701e-02, -3.0132e-03,  4.4088e-02,\n",
      "         9.4108e-03,  1.7615e-02, -2.0301e-02, -7.9639e-03, -8.2368e-02,\n",
      "        -8.4262e-02, -3.8943e-02,  6.8632e-03, -4.6781e-02, -8.0821e-02,\n",
      "        -7.8200e-02,  2.6409e-02, -8.2851e-02, -4.6338e-02,  4.7451e-02,\n",
      "        -1.6151e-02,  9.6701e-03,  6.9805e-03, -5.1573e-02,  2.5118e-02,\n",
      "        -4.5263e-02, -7.1824e-02, -1.4516e-02,  4.3401e-03, -2.0708e-02,\n",
      "         3.4515e-03, -3.6866e-02, -2.9079e-02, -2.2738e-02,  2.0935e-02,\n",
      "        -5.1360e-02,  2.1484e-02,  4.2914e-02, -6.0016e-02, -3.8929e-03,\n",
      "        -1.1430e-02, -7.0979e-02, -8.5272e-02, -4.4194e-02, -2.2950e-02,\n",
      "         3.6207e-02, -8.4771e-02, -8.1568e-02, -6.6456e-02,  3.0713e-02,\n",
      "         2.3567e-02, -6.2138e-02, -5.2606e-02,  3.7140e-02, -3.3170e-02,\n",
      "         1.5960e-03, -6.9625e-03,  4.9796e-02,  5.2445e-03, -2.2191e-02,\n",
      "        -8.5521e-02,  2.1835e-02,  3.4565e-02, -8.9739e-02, -2.6118e-02,\n",
      "        -5.7900e-02,  2.0751e-02, -6.6709e-02,  3.4126e-02, -9.7060e-02,\n",
      "        -4.0326e-02,  1.7015e-02,  4.4967e-02, -6.6780e-02, -2.4731e-02,\n",
      "         3.2827e-03, -3.4712e-02, -1.1285e-01, -8.2179e-03,  1.0460e-02,\n",
      "        -1.1578e-02,  3.0283e-02,  4.4986e-02,  3.0235e-02,  1.4660e-02,\n",
      "        -6.1953e-02, -6.4347e-03, -2.4810e-02, -4.2207e-02, -3.6993e-02,\n",
      "         1.0303e-02,  2.9830e-03, -3.9422e-02, -4.3998e-02,  3.1702e-02,\n",
      "        -1.1125e-01,  1.7984e-02, -5.2610e-02, -3.8646e-02,  3.3332e-02,\n",
      "        -6.8126e-03, -1.7781e-02,  3.0548e-02, -5.7575e-02, -2.2147e-03,\n",
      "        -7.3412e-03,  3.7909e-02, -3.2522e-02,  4.8039e-02, -8.4326e-03,\n",
      "        -1.0272e-01,  2.2505e-03,  7.7928e-04,  2.5682e-02, -5.6887e-02,\n",
      "        -3.6927e-04,  4.4816e-02,  3.1141e-02, -2.7010e-02,  3.0459e-02,\n",
      "         3.2692e-02, -6.6765e-02, -4.4070e-02,  3.8336e-02, -6.7855e-02,\n",
      "        -8.4317e-02,  7.5601e-03, -1.3513e-02, -5.9761e-02, -2.3465e-02,\n",
      "         2.8316e-02, -7.4227e-02, -1.2582e-02, -7.3134e-02,  1.1849e-02,\n",
      "        -4.4089e-02, -9.8948e-02, -9.5728e-02, -6.1366e-03, -5.8005e-02,\n",
      "        -3.4524e-02, -6.1773e-02, -4.6899e-02, -4.0187e-02, -2.7625e-02,\n",
      "        -7.4559e-02, -8.8751e-02, -2.4458e-02,  1.3259e-02, -3.4643e-02,\n",
      "        -1.3037e-01,  3.5915e-04,  7.2787e-03, -5.6987e-02, -8.2461e-02,\n",
      "         4.3484e-03, -3.4739e-02,  4.3310e-02,  9.0210e-03, -3.1335e-02,\n",
      "        -1.1065e-02, -1.6834e-02, -4.4476e-02,  4.5689e-02,  2.4434e-02,\n",
      "        -6.1378e-02,  1.4353e-02, -8.3882e-02,  4.3537e-02, -5.9273e-02,\n",
      "         2.9468e-02,  1.4593e-02,  3.7102e-02,  1.0858e-02, -4.7035e-02,\n",
      "        -3.6446e-02,  4.2722e-02, -7.0501e-03,  3.7635e-03, -1.1624e-02,\n",
      "        -5.9994e-02,  4.2152e-02, -3.7097e-02, -8.8609e-02, -2.2446e-02,\n",
      "        -9.3684e-02,  4.9692e-03, -8.9004e-02, -1.8680e-02, -6.0421e-02,\n",
      "        -8.7551e-02, -4.7839e-02,  2.7244e-02, -1.3385e-02,  2.9961e-02,\n",
      "         4.9831e-02, -1.1138e-03,  6.1723e-04, -4.6109e-02, -9.0791e-02,\n",
      "         3.0730e-02, -4.9404e-02,  3.9628e-02, -3.2737e-02,  3.0636e-02,\n",
      "        -7.3079e-02, -6.2298e-02, -9.1986e-03,  2.7480e-02, -1.9890e-02,\n",
      "        -6.1859e-04,  2.5354e-02, -4.7499e-02, -2.3520e-02, -9.3529e-02,\n",
      "        -8.6708e-02,  1.8163e-02, -4.3826e-02, -2.4066e-02, -9.6637e-02,\n",
      "         1.2637e-02, -2.2576e-02,  5.8938e-03, -4.8455e-02,  3.0451e-02,\n",
      "        -8.1407e-02, -6.0118e-02, -3.5575e-02, -9.8384e-02, -2.3951e-02,\n",
      "        -7.7108e-02, -6.4250e-02, -7.5213e-03,  1.1748e-02,  1.8986e-02,\n",
      "        -3.4778e-02,  1.8331e-02, -3.0727e-02, -4.5526e-02, -6.5161e-02,\n",
      "         1.7855e-02,  3.4804e-02, -8.4943e-02,  2.3243e-02,  5.4103e-02,\n",
      "         3.6205e-02,  1.1406e-02,  2.2162e-02, -7.2881e-02, -7.4152e-02,\n",
      "        -3.1925e-02, -9.0746e-02, -3.8565e-02, -5.7426e-02, -7.6188e-02,\n",
      "         2.8086e-02, -9.7412e-02,  3.7272e-03,  4.8954e-02, -7.2434e-02,\n",
      "        -1.1116e-02, -9.9622e-03, -1.1805e-02, -9.2613e-03, -4.7240e-02,\n",
      "        -2.7796e-02, -5.9192e-02, -2.4451e-02, -2.0901e-02, -1.2778e-02,\n",
      "        -3.9083e-03, -4.9874e-02, -7.1137e-02, -5.6351e-02, -3.1165e-02,\n",
      "        -4.2186e-02, -4.6068e-03, -1.1817e-02, -4.9281e-02, -6.9791e-02,\n",
      "        -9.6591e-04, -8.8149e-02, -3.3815e-02, -2.8021e-02,  1.2273e-03,\n",
      "         1.9680e-02, -5.0249e-02, -1.0920e-02,  4.2709e-03, -7.5373e-02,\n",
      "        -4.6747e-02, -2.9216e-02,  1.0119e-02, -5.1227e-02, -4.2235e-02,\n",
      "        -3.7598e-02, -8.0181e-02, -2.5183e-02, -3.8555e-02, -6.6021e-02,\n",
      "         5.1470e-02,  1.8155e-02,  2.9482e-02,  1.1846e-02, -3.1726e-02,\n",
      "         1.7432e-02,  4.0623e-04, -8.2317e-02, -7.9923e-02,  1.8748e-02,\n",
      "         9.3802e-03, -4.5680e-02, -8.1868e-02,  2.1231e-03,  3.6627e-02,\n",
      "         3.4844e-02,  9.7667e-03, -1.0300e-02, -5.0217e-02, -4.4100e-02,\n",
      "        -4.7510e-02, -1.1787e-02,  7.2929e-03, -2.1135e-02, -7.1774e-02,\n",
      "        -9.2465e-02,  5.6026e-02,  1.2857e-02,  2.7100e-02, -1.0837e-02,\n",
      "        -2.9473e-02, -2.4010e-02, -7.7591e-02, -1.5100e-03,  3.3535e-02,\n",
      "        -1.0118e-02,  5.5450e-03,  4.4439e-02, -4.3729e-02, -7.8164e-02,\n",
      "         1.0413e-04,  2.3446e-03, -8.6630e-02, -4.7420e-02, -7.8145e-03,\n",
      "        -2.7881e-02, -1.4092e-02, -3.9468e-02, -2.7923e-02, -5.4969e-02,\n",
      "         3.0592e-03, -6.1093e-02, -6.2600e-02,  4.9152e-02, -2.7361e-03,\n",
      "        -5.2255e-02,  1.2719e-02, -1.8437e-02, -5.7770e-02,  2.7047e-02,\n",
      "        -1.0165e-02,  1.3385e-02,  1.6178e-02, -6.3082e-02, -6.0487e-02,\n",
      "         1.0428e-02,  1.4331e-03,  2.2913e-02,  2.1504e-02,  2.3841e-02,\n",
      "        -6.7877e-02, -2.2181e-02,  6.3858e-03, -2.5927e-02, -4.6762e-02,\n",
      "        -7.2281e-02, -3.9759e-02,  3.5262e-03, -4.1979e-02, -3.0924e-02,\n",
      "        -5.8092e-02,  2.4263e-02, -4.3772e-02, -4.6146e-02,  3.3793e-02,\n",
      "         5.0974e-02, -7.0345e-02, -3.8437e-02, -7.4465e-02, -3.5070e-02,\n",
      "         2.7230e-02, -5.4205e-02, -8.5455e-02, -9.3170e-02,  3.3347e-02,\n",
      "         2.9595e-02,  1.7656e-02,  5.2364e-02,  6.5309e-03, -1.8502e-02,\n",
      "        -8.2051e-02,  1.8523e-03, -9.1035e-02, -7.7876e-02, -1.3717e-02,\n",
      "         4.3281e-02, -3.7736e-02, -8.0556e-02, -2.5231e-02, -5.8585e-03,\n",
      "        -8.0547e-02,  3.8337e-02, -5.5016e-03, -7.8983e-03, -2.6838e-03,\n",
      "        -5.3058e-02, -6.8646e-02,  3.0947e-02,  3.9694e-02,  3.6897e-02,\n",
      "         9.8741e-03,  1.9766e-02, -2.8305e-02, -3.6005e-02,  3.3412e-02,\n",
      "        -2.7469e-02, -1.1273e-02, -3.0923e-03, -4.1021e-02, -4.4926e-02,\n",
      "        -8.1387e-02,  1.8730e-02, -1.3407e-03, -6.0983e-02,  1.5738e-02,\n",
      "        -6.0344e-02, -6.2445e-02,  3.0084e-02, -8.8719e-03,  3.5955e-02,\n",
      "        -1.2592e-01, -4.6183e-02, -4.6509e-02, -9.1187e-02, -5.1436e-03,\n",
      "        -2.4593e-02,  3.0180e-02, -7.7550e-02, -4.2064e-02, -3.4435e-03,\n",
      "        -8.2157e-02, -4.3419e-02, -6.5225e-02,  4.3859e-02,  8.6541e-03,\n",
      "         2.2967e-02,  1.9824e-02,  5.3734e-02, -7.9578e-02, -8.2389e-02,\n",
      "        -1.1033e-01, -8.8812e-03, -6.0960e-02,  4.0689e-02, -7.1278e-04,\n",
      "        -7.6997e-02,  1.7178e-02,  2.4233e-02,  3.5384e-02, -6.2866e-03,\n",
      "         1.5801e-02, -7.8547e-02, -7.7767e-03, -7.0164e-02,  5.5516e-04,\n",
      "        -2.1389e-02, -3.6559e-02,  4.5882e-02, -4.1270e-02, -9.7651e-02,\n",
      "        -7.0487e-02, -1.2476e-02, -6.6610e-02,  4.0748e-02,  1.9540e-02,\n",
      "        -2.6021e-02, -4.7469e-02, -3.7311e-02,  2.1838e-02, -8.3636e-02,\n",
      "        -8.3586e-02,  5.2497e-02, -2.6372e-02, -4.8483e-03, -2.9646e-02,\n",
      "        -2.1809e-02, -3.5893e-02,  1.8548e-03,  2.0258e-02,  6.7794e-03,\n",
      "         3.1344e-03, -9.2190e-03,  3.6318e-03, -3.5271e-02, -4.0672e-03,\n",
      "        -3.9952e-02,  2.7765e-02, -3.9599e-02, -7.4168e-02,  3.9091e-02,\n",
      "        -5.8241e-03, -5.3059e-02, -6.5232e-03,  1.9484e-02, -4.7782e-02,\n",
      "        -1.8691e-02, -4.8748e-02, -7.0920e-02, -4.6507e-03,  4.5186e-02,\n",
      "         6.3959e-03, -6.1864e-02, -3.0437e-02, -2.5604e-02, -3.1638e-02,\n",
      "        -1.0322e-01, -3.0263e-02,  1.8887e-02, -4.2279e-02, -7.2524e-02,\n",
      "        -3.8389e-02, -9.7812e-02, -6.4893e-02, -8.9913e-02, -9.3076e-03,\n",
      "         1.8828e-02, -5.1100e-02, -6.4058e-02,  1.9162e-03, -4.5621e-02,\n",
      "         4.5895e-03, -1.2928e-02, -1.2669e-02, -9.0366e-02, -6.3053e-02,\n",
      "         4.1130e-02,  3.1922e-02, -4.9452e-02, -1.1416e-02, -4.2890e-03,\n",
      "        -3.2206e-02,  4.2317e-02, -6.1551e-02,  1.3029e-02,  2.9924e-02,\n",
      "        -6.7280e-03, -6.2075e-02, -6.8202e-02,  1.0007e-02,  9.3753e-03,\n",
      "        -4.8098e-03, -1.6142e-03,  2.6012e-02,  3.1275e-02, -3.7403e-02,\n",
      "        -8.5651e-02,  4.9185e-02, -4.2665e-02, -6.8693e-02, -5.6057e-02,\n",
      "        -5.7819e-02, -6.1788e-02, -1.3532e-01, -9.7251e-02,  4.7874e-02,\n",
      "         6.5796e-03, -3.5668e-02, -7.5173e-02,  6.1326e-03, -4.4352e-02,\n",
      "        -5.5205e-02, -1.0164e-01, -2.3721e-02, -6.1071e-02, -4.6853e-02,\n",
      "        -6.3701e-02, -3.9766e-02, -3.5984e-02, -6.7800e-02,  4.8323e-02,\n",
      "        -2.3871e-02,  3.8584e-02, -2.7926e-02, -5.2520e-02, -5.4480e-02,\n",
      "         2.2122e-02, -9.1035e-02, -3.5930e-02, -6.5614e-03, -6.9080e-02,\n",
      "        -1.8595e-02, -6.7756e-02, -8.2313e-02,  8.7555e-03, -9.5003e-02,\n",
      "        -3.8149e-02, -4.9564e-02, -6.8733e-02,  3.1650e-02, -1.9080e-03,\n",
      "         2.3939e-02,  3.7344e-02, -1.4904e-02, -7.5802e-03, -1.0077e-01,\n",
      "         3.5496e-02,  4.9943e-02, -7.6733e-02, -5.5644e-02, -1.5837e-02,\n",
      "        -4.0627e-02, -4.1949e-02, -3.1358e-02, -6.7344e-02,  4.0531e-02,\n",
      "         6.1725e-02, -7.1692e-02, -6.8562e-03, -1.5106e-02,  1.1012e-02,\n",
      "         2.4231e-02, -2.7163e-02,  4.0830e-02,  2.9777e-02,  8.6370e-03,\n",
      "        -6.9607e-02, -8.8367e-02, -6.8310e-02, -4.6440e-02, -7.0283e-02,\n",
      "        -1.8657e-02, -7.8685e-02, -4.9149e-02, -1.9931e-02,  2.2682e-02,\n",
      "        -6.5784e-02, -1.0998e-01,  5.4124e-02, -1.0539e-02, -8.5732e-02,\n",
      "         4.5414e-02,  4.9675e-03,  4.0027e-02, -4.6103e-02, -4.7476e-02,\n",
      "         2.2711e-02, -4.8734e-02, -4.3181e-02, -6.0986e-02,  1.6615e-02,\n",
      "        -2.8470e-02, -2.1841e-02, -4.9295e-02, -9.3829e-02,  4.9390e-02,\n",
      "        -4.4774e-02, -4.4283e-02, -7.8649e-02, -7.4387e-02, -6.4420e-02,\n",
      "        -9.7909e-02, -7.5089e-02,  2.8610e-02, -4.8706e-02, -6.0562e-02,\n",
      "        -6.5352e-02, -7.8396e-03, -2.0984e-02, -1.0320e-01,  4.4973e-02,\n",
      "         1.9555e-02,  5.3392e-02,  1.2471e-02, -6.6944e-02, -1.0040e-01,\n",
      "        -2.5091e-02, -7.7659e-02,  1.5069e-02,  2.2563e-03, -2.9553e-02,\n",
      "        -4.5011e-02, -4.0844e-02, -7.5845e-02, -6.4950e-02, -8.0017e-02,\n",
      "        -4.1011e-02, -4.4695e-02, -4.2910e-02,  1.8297e-02,  2.2085e-02,\n",
      "         2.4000e-02, -6.8763e-02,  1.5753e-03,  3.2306e-03,  4.2417e-02,\n",
      "        -6.8471e-02, -3.3355e-03, -3.7102e-02, -9.0222e-02,  7.1762e-03,\n",
      "        -6.9669e-03, -1.9121e-02, -5.7810e-02, -6.8577e-02, -8.4490e-02,\n",
      "         2.1749e-02, -1.5009e-02, -5.8682e-02, -4.8152e-02,  7.0415e-05,\n",
      "         3.5267e-02, -7.5325e-02, -6.9407e-02, -5.3857e-03, -6.7596e-02,\n",
      "         1.0795e-02,  2.5196e-02, -9.6857e-04, -8.2847e-02,  7.9605e-02,\n",
      "        -3.7925e-04, -2.1324e-02,  4.7655e-02,  1.9610e-02,  1.9876e-02,\n",
      "         1.6085e-02,  3.0361e-02, -2.6438e-02], requires_grad=True))\n",
      "encoder.0.1.block.1.2.scale \t tensor(1.)\n",
      "encoder.0.1.block.1.2.zero_point \t tensor(0)\n",
      "encoder.0.1.block.1.2._packed_params.dtype \t torch.qint8\n",
      "encoder.0.1.block.1.2._packed_params._packed_params \t (tensor([[-0.0074,  0.0000,  0.0706,  ...,  0.0211, -0.0136, -0.0161],\n",
      "        [-0.0074,  0.0322,  0.0334,  ...,  0.0334,  0.0248, -0.0272],\n",
      "        [ 0.0111, -0.0161,  0.0359,  ...,  0.0087,  0.0371,  0.0235],\n",
      "        ...,\n",
      "        [-0.0050,  0.0582,  0.0235,  ...,  0.0012, -0.0087,  0.0074],\n",
      "        [ 0.0235,  0.0371,  0.0334,  ...,  0.0161, -0.0099, -0.0087],\n",
      "        [ 0.0235,  0.0087,  0.0136,  ..., -0.0359,  0.0495,  0.0198]],\n",
      "       size=(192, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0012382707791402936,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0024,  0.0251, -0.0255,  0.0078, -0.0193,  0.0038, -0.0276, -0.0142,\n",
      "        -0.0235,  0.0115, -0.0273, -0.0201, -0.0298,  0.0008,  0.0208, -0.0202,\n",
      "         0.0248, -0.0296, -0.0328, -0.0019, -0.0098, -0.0141,  0.0322, -0.0220,\n",
      "        -0.0238,  0.0118,  0.0186,  0.0203, -0.0058,  0.0178,  0.0470, -0.0299,\n",
      "        -0.0184, -0.0053, -0.0096, -0.0415,  0.0145,  0.0033,  0.0186, -0.0163,\n",
      "         0.0195,  0.0340,  0.0279,  0.0190,  0.0198,  0.0077,  0.0395, -0.0188,\n",
      "         0.0245, -0.0531,  0.0096, -0.0221,  0.0046, -0.0160, -0.0297, -0.0243,\n",
      "        -0.0208,  0.0385,  0.0097, -0.0319, -0.0233,  0.0061,  0.0254,  0.0026,\n",
      "        -0.0176,  0.0153,  0.0103, -0.0139,  0.0020,  0.0225,  0.0329, -0.0032,\n",
      "        -0.0073,  0.0102,  0.0340, -0.0233,  0.0356, -0.0077,  0.0093, -0.0342,\n",
      "         0.0200,  0.0148, -0.0012, -0.0053, -0.0181,  0.0366,  0.0418, -0.0101,\n",
      "        -0.0186,  0.0052, -0.0292,  0.0075, -0.0249, -0.0323,  0.0051,  0.0144,\n",
      "        -0.0031, -0.0042,  0.0319, -0.0113,  0.0113, -0.0178, -0.0043, -0.0149,\n",
      "        -0.0257, -0.0227, -0.0306, -0.0359,  0.0176,  0.0264, -0.0358, -0.0226,\n",
      "        -0.0464, -0.0030,  0.0062, -0.0131, -0.0010,  0.0093, -0.0039, -0.0011,\n",
      "         0.0197, -0.0195, -0.0019, -0.0050,  0.0347,  0.0013,  0.0440,  0.0105,\n",
      "         0.0490, -0.0058,  0.0139, -0.0181,  0.0294, -0.0005, -0.0265,  0.0119,\n",
      "        -0.0171,  0.0335, -0.0115, -0.0236,  0.0264,  0.0177, -0.0201, -0.0292,\n",
      "        -0.0287,  0.0434,  0.0137,  0.0408, -0.0107, -0.0535, -0.0079, -0.0067,\n",
      "        -0.0322, -0.0249,  0.0021, -0.0074, -0.0091,  0.0185,  0.0141, -0.0065,\n",
      "        -0.0158, -0.0214,  0.0198,  0.0250, -0.0186, -0.0194, -0.0241,  0.0214,\n",
      "        -0.0039, -0.0075, -0.0273,  0.0163,  0.0319, -0.0302, -0.0036,  0.0054,\n",
      "        -0.0197, -0.0310, -0.0192,  0.0236, -0.0374, -0.0161,  0.0293,  0.0375,\n",
      "         0.0031, -0.0199, -0.0269,  0.0150,  0.0137,  0.0070, -0.0161,  0.0398],\n",
      "       requires_grad=True))\n",
      "encoder.1.0.block.0.weight \t tensor([1.0161, 1.0231, 0.9826, 1.0039, 1.0581, 0.9974, 1.0265, 1.0120, 1.0098,\n",
      "        1.0004, 1.0070, 1.0387, 1.0114, 1.0275, 1.0338, 1.0315, 1.0734, 1.0083,\n",
      "        0.9813, 1.0265, 1.0171, 1.0316, 1.0424, 1.0061, 1.0116, 1.0042, 1.0110,\n",
      "        1.0697, 0.9837, 1.0320, 1.0239, 1.0531, 0.9996, 1.0433, 1.0123, 1.0268,\n",
      "        1.0513, 1.0079, 1.0205, 1.0160, 1.0132, 0.9690, 1.0056, 1.0217, 1.0385,\n",
      "        1.0267, 0.9913, 1.0171, 1.0294, 1.0621, 1.0289, 1.0332, 1.0093, 1.0363,\n",
      "        1.0138, 1.0071, 1.0193, 1.0710, 1.0434, 1.0243, 1.0262, 1.0821, 1.0374,\n",
      "        1.0362, 1.0669, 1.0127, 1.0232, 0.9810, 1.0262, 1.0304, 1.0018, 1.0526,\n",
      "        1.0157, 0.9959, 1.0096, 1.0217, 1.0842, 1.0062, 1.0342, 1.0534, 1.0193,\n",
      "        0.9937, 1.0154, 1.0441, 1.0103, 1.0114, 1.1301, 1.0217, 1.0187, 1.0270,\n",
      "        0.9955, 1.0262, 1.0033, 0.9940, 1.0163, 1.0469, 1.0049, 1.0180, 1.0043,\n",
      "        1.0170, 1.0243, 1.0135, 1.0374, 1.0354, 1.0450, 1.0780, 1.0485, 1.0240,\n",
      "        1.0112, 1.0181, 0.9876, 1.0595, 1.0302, 1.0117, 1.0392, 1.0132, 1.0485,\n",
      "        1.0010, 1.0135, 0.9909, 1.0728, 1.0114, 1.0322, 1.0227, 1.0695, 1.0364,\n",
      "        1.0500, 1.0235, 1.0362, 1.0110, 1.0441, 1.0424, 0.9942, 1.0456, 1.0169,\n",
      "        1.0465, 1.0282, 1.0208, 1.0075, 1.0244, 1.0217, 1.0301, 1.0334, 1.0507,\n",
      "        1.0406, 1.0415, 1.0365, 1.0162, 1.0004, 1.0512, 1.0222, 1.0560, 1.0208,\n",
      "        1.0022, 1.0629, 0.9957, 1.0324, 1.0552, 1.0035, 1.0346, 1.0233, 0.9962,\n",
      "        1.0264, 1.0202, 1.0412, 1.0259, 1.0064, 1.0739, 1.0398, 0.9889, 1.0388,\n",
      "        1.0368, 1.0002, 0.9850, 1.0199, 1.0288, 0.9936, 1.0549, 1.0265, 1.0428,\n",
      "        1.0554, 1.0233, 1.0740, 1.0567, 1.0159, 0.9935, 0.9823, 0.9785, 1.0223,\n",
      "        1.0236, 1.0577, 1.0401])\n",
      "encoder.1.0.block.0.bias \t tensor([ 1.9074e-02,  2.0685e-03, -1.6077e-02,  4.1630e-03, -3.4105e-03,\n",
      "        -4.0206e-03,  1.0803e-02, -1.9882e-02, -7.9523e-03, -1.3674e-02,\n",
      "         1.0176e-02, -1.0876e-02,  2.9409e-02,  2.6015e-02, -1.1198e-02,\n",
      "        -8.0522e-03, -2.2917e-02,  7.9198e-03, -3.0215e-02, -1.9141e-02,\n",
      "        -1.5296e-02, -2.5309e-03,  5.2448e-03, -2.5735e-02, -1.9260e-02,\n",
      "         6.6864e-03, -3.9883e-03, -2.3682e-03,  1.0864e-02,  1.1410e-02,\n",
      "        -2.2547e-02,  3.0410e-03, -2.3581e-02, -2.0890e-02, -1.1495e-02,\n",
      "        -2.0054e-02,  4.5477e-03,  6.6608e-03, -3.6667e-03,  2.0926e-02,\n",
      "        -2.7839e-02,  7.0908e-03, -3.6578e-03, -2.1765e-02,  3.6649e-03,\n",
      "        -1.4733e-03,  1.6454e-02, -1.2484e-02, -1.8236e-02, -1.4967e-02,\n",
      "         1.0232e-02,  9.5959e-03, -8.7438e-03, -1.6214e-02,  5.0850e-03,\n",
      "         1.1155e-02, -1.0752e-03,  2.2741e-03, -2.2039e-02, -1.9147e-02,\n",
      "         2.1197e-02, -1.2477e-03, -8.2279e-03, -3.5126e-02,  5.9926e-04,\n",
      "         2.1498e-03, -6.5927e-03, -1.9038e-02,  1.1821e-02,  3.2151e-02,\n",
      "        -3.9834e-03, -7.0652e-03, -8.7383e-03, -7.6366e-03, -1.7619e-02,\n",
      "        -6.8003e-03, -1.8757e-02, -1.7504e-02,  6.5936e-03,  2.0987e-02,\n",
      "        -1.1828e-02, -9.4542e-03, -4.7940e-02, -1.2574e-02,  1.1707e-02,\n",
      "         1.4162e-02, -8.1208e-03, -8.1660e-03,  8.0668e-03, -1.8352e-02,\n",
      "        -2.3183e-02, -2.9567e-02,  2.5228e-02,  2.2482e-02, -7.9362e-03,\n",
      "         3.1878e-03,  8.6810e-03, -1.5530e-02,  1.3472e-02, -2.9333e-03,\n",
      "         3.1686e-02, -1.3530e-02, -3.1270e-03,  8.4381e-03, -4.8111e-03,\n",
      "         3.6472e-02,  2.7102e-02, -5.8864e-03, -1.3934e-02, -1.7258e-04,\n",
      "         2.9685e-03,  1.2392e-02,  3.4103e-03, -1.7109e-03,  9.7698e-04,\n",
      "        -3.7045e-03,  5.0796e-03, -3.5148e-03,  1.1446e-02,  2.1964e-02,\n",
      "         1.1731e-03, -2.9656e-02,  9.1646e-03,  3.1780e-02, -2.0218e-03,\n",
      "        -2.3785e-02, -1.5282e-02, -2.0057e-03, -1.3830e-02, -2.9385e-02,\n",
      "         6.0658e-03, -9.9083e-03, -1.2375e-02, -3.8539e-02,  1.4985e-02,\n",
      "         4.5107e-04,  2.4175e-02, -5.4305e-03, -1.7981e-02, -5.0235e-03,\n",
      "        -8.1290e-04,  1.3264e-02, -3.4122e-02, -6.6395e-04, -1.7322e-02,\n",
      "        -1.7638e-02,  2.2514e-02,  3.1183e-02,  9.3279e-03, -1.2283e-02,\n",
      "        -5.1916e-03,  1.3756e-02, -6.4683e-03,  1.1527e-02, -1.1520e-02,\n",
      "        -8.4035e-04,  1.7670e-02, -1.9797e-02, -2.3503e-02, -4.3949e-03,\n",
      "        -4.1185e-02,  7.1069e-04,  1.6029e-02,  3.2590e-02,  1.4538e-02,\n",
      "         2.8918e-02, -2.9300e-03,  8.2022e-03,  2.3143e-03, -1.6289e-03,\n",
      "         3.1038e-02, -7.4599e-05,  1.8657e-02, -6.9727e-04, -2.9207e-02,\n",
      "         3.6911e-03, -2.5156e-02, -9.7597e-03, -7.4264e-03,  1.8907e-02,\n",
      "        -1.3512e-02,  1.1529e-02,  1.8028e-03,  9.9850e-03,  4.9829e-03,\n",
      "        -1.7746e-03,  2.7619e-02, -8.0883e-03, -7.3363e-03,  1.2097e-02,\n",
      "        -1.1326e-02, -9.5146e-03])\n",
      "encoder.1.0.block.1.queries_projection.scale \t tensor(1.)\n",
      "encoder.1.0.block.1.queries_projection.zero_point \t tensor(0)\n",
      "encoder.1.0.block.1.queries_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.1.0.block.1.queries_projection._packed_params._packed_params \t (tensor([[ 0.0731,  0.0124,  0.0405,  ...,  0.0607,  0.0467, -0.0093],\n",
      "        [-0.0918,  0.0576,  0.0109,  ...,  0.0716, -0.0420,  0.0560],\n",
      "        [-0.1120,  0.0187,  0.0763,  ...,  0.0109,  0.0545, -0.0265],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0233, -0.0420,  ..., -0.0405, -0.0311, -0.0062],\n",
      "        [-0.0467,  0.0373, -0.0280,  ..., -0.0778,  0.0405,  0.0467],\n",
      "        [-0.0451, -0.0685, -0.0980,  ...,  0.0311,  0.0093, -0.0482]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.001556245144456625,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.1038,  0.0351, -0.0662,  0.0160,  0.0563,  0.0979,  0.0345,  0.0521,\n",
      "         0.0280, -0.0962,  0.0021, -0.0037, -0.0011, -0.0510, -0.0297,  0.0241,\n",
      "         0.0081, -0.0222, -0.0286,  0.0017, -0.0409, -0.0932,  0.0596,  0.0251,\n",
      "         0.0911,  0.0332, -0.0078,  0.0571,  0.0724, -0.0073,  0.0690, -0.0343,\n",
      "         0.0460,  0.0846, -0.0563,  0.0748, -0.0618,  0.0436, -0.0198,  0.0336,\n",
      "         0.0356, -0.0636,  0.0668, -0.0582,  0.0289, -0.0859,  0.0483, -0.0524,\n",
      "        -0.0627, -0.0360, -0.0103,  0.0556, -0.0500, -0.0280,  0.0189,  0.0327,\n",
      "         0.0570, -0.0139, -0.0079,  0.0523, -0.0172, -0.0623, -0.0425,  0.0540,\n",
      "        -0.0640, -0.0543, -0.0232, -0.0765, -0.0903,  0.0106, -0.0265, -0.0069,\n",
      "         0.1294, -0.0771,  0.0172, -0.0820,  0.0539, -0.0804,  0.0685,  0.0058,\n",
      "         0.0611,  0.0182, -0.0819,  0.0444,  0.0866,  0.0197,  0.0342,  0.0151,\n",
      "        -0.0495,  0.0086,  0.0052, -0.0424, -0.0556, -0.0228, -0.0671,  0.0266,\n",
      "        -0.0363,  0.0531, -0.0358,  0.0482,  0.0668, -0.0006,  0.0071, -0.0388,\n",
      "        -0.0740, -0.0243,  0.0235, -0.0601,  0.0666, -0.0571, -0.0409,  0.0717,\n",
      "         0.0116,  0.0361, -0.0568, -0.0536, -0.0326,  0.0237,  0.0772,  0.0112,\n",
      "        -0.0930,  0.0202, -0.0510, -0.1038, -0.0178,  0.0255, -0.0577,  0.0227,\n",
      "         0.0554, -0.0084,  0.0333, -0.0236,  0.0293, -0.0498,  0.0252, -0.0389,\n",
      "        -0.0931,  0.0152,  0.0433,  0.0119,  0.0306, -0.0023,  0.0259,  0.0172,\n",
      "        -0.0291,  0.0380,  0.0249, -0.0357, -0.0002, -0.0336, -0.0257,  0.0141,\n",
      "        -0.0207,  0.0703,  0.0165, -0.0594, -0.0517, -0.0717, -0.0068,  0.0483,\n",
      "         0.0564, -0.0132, -0.0481, -0.0236, -0.0615, -0.0346,  0.0796, -0.0665,\n",
      "         0.0605,  0.0650,  0.0357, -0.0794, -0.0683,  0.0958,  0.0721,  0.0226,\n",
      "        -0.0244,  0.0443,  0.0004, -0.0254,  0.0585, -0.0314,  0.0009, -0.0743,\n",
      "        -0.0019,  0.0051, -0.0880, -0.0102,  0.0100, -0.0146, -0.0042, -0.0121],\n",
      "       requires_grad=True))\n",
      "encoder.1.0.block.1.values_projection.scale \t tensor(1.)\n",
      "encoder.1.0.block.1.values_projection.zero_point \t tensor(0)\n",
      "encoder.1.0.block.1.values_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.1.0.block.1.values_projection._packed_params._packed_params \t (tensor([[-0.0959, -0.0240,  0.1177,  ..., -0.0087,  0.0589, -0.0240],\n",
      "        [-0.0916, -0.0719,  0.0349,  ..., -0.0109, -0.1243,  0.0065],\n",
      "        [ 0.0240, -0.0414, -0.0872,  ..., -0.0436, -0.0610,  0.0087],\n",
      "        ...,\n",
      "        [-0.0654, -0.0414,  0.0632,  ...,  0.0589,  0.1286,  0.0523],\n",
      "        [-0.0196, -0.0828,  0.0349,  ..., -0.0894, -0.1134, -0.0305],\n",
      "        [ 0.0240,  0.1046, -0.0240,  ...,  0.0153,  0.1046,  0.0131]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0021801635157316923,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0558, -0.0103,  0.0753,  0.0691, -0.0315,  0.0456, -0.0441, -0.0284,\n",
      "         0.0614, -0.0503,  0.0595,  0.0029, -0.0072,  0.0790, -0.0030,  0.0255,\n",
      "        -0.0703,  0.0097, -0.0614, -0.0003, -0.0332, -0.0690,  0.0212, -0.0435,\n",
      "        -0.0285,  0.0470,  0.0438,  0.0826,  0.0501, -0.0485,  0.0309, -0.0816,\n",
      "         0.0572,  0.0013,  0.0455,  0.0400,  0.0176,  0.0640, -0.0280, -0.0490,\n",
      "         0.0066, -0.0600, -0.0064, -0.0256, -0.0472,  0.0109, -0.0213,  0.0261,\n",
      "         0.0639, -0.0260, -0.0026, -0.0234,  0.0538,  0.0054, -0.0292,  0.0752,\n",
      "        -0.0728,  0.0138,  0.0044, -0.0409,  0.0273, -0.0492, -0.0394,  0.0235,\n",
      "        -0.0573, -0.0137,  0.0670, -0.0397,  0.0705, -0.0306, -0.0076, -0.0626,\n",
      "        -0.0277, -0.0060,  0.0496,  0.0065,  0.0704, -0.0104,  0.0312, -0.0649,\n",
      "        -0.0257,  0.0329,  0.0278, -0.0093,  0.0518,  0.0130, -0.0713, -0.0585,\n",
      "        -0.0491,  0.0107, -0.0013,  0.0658,  0.0028,  0.0367, -0.0045,  0.0470,\n",
      "        -0.0507,  0.0406, -0.0099,  0.0561, -0.0363, -0.0446, -0.0184, -0.0080,\n",
      "        -0.0529,  0.0561,  0.0427, -0.0383, -0.0577, -0.0413,  0.0688,  0.0298,\n",
      "         0.0634, -0.0144, -0.0065, -0.0326,  0.0249,  0.0502, -0.0120, -0.0144,\n",
      "         0.0524, -0.0390, -0.0247,  0.0504, -0.0440,  0.0135, -0.0329, -0.0138,\n",
      "         0.0309, -0.0743,  0.0620, -0.0195, -0.0288, -0.0288,  0.0110,  0.0658,\n",
      "         0.0332, -0.0572, -0.0417, -0.0190,  0.0466, -0.0728, -0.0143, -0.0161,\n",
      "         0.0267,  0.0035, -0.0447,  0.0408, -0.0419,  0.0622, -0.0380, -0.0337,\n",
      "        -0.0709,  0.0106, -0.0423, -0.0674,  0.0621, -0.0633, -0.0544, -0.0255,\n",
      "        -0.0015,  0.0540, -0.0783,  0.0432,  0.0061,  0.0353, -0.0169, -0.0204,\n",
      "         0.0194, -0.0460, -0.0661, -0.0203,  0.0498, -0.0686,  0.0613,  0.0365,\n",
      "        -0.0618,  0.0672,  0.0074,  0.0173,  0.0291,  0.0666,  0.0055, -0.0535,\n",
      "         0.0178,  0.0493, -0.0166,  0.0038, -0.0429,  0.0175,  0.0484, -0.0531],\n",
      "       requires_grad=True))\n",
      "encoder.1.0.block.1.keys_projection.scale \t tensor(1.)\n",
      "encoder.1.0.block.1.keys_projection.zero_point \t tensor(0)\n",
      "encoder.1.0.block.1.keys_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.1.0.block.1.keys_projection._packed_params._packed_params \t (tensor([[ 0.0040,  0.0218, -0.0337,  ..., -0.0258,  0.0258,  0.0357],\n",
      "        [-0.1091, -0.0635,  0.0159,  ..., -0.0476,  0.0734,  0.0238],\n",
      "        [ 0.0159, -0.0297,  0.0377,  ..., -0.0714, -0.0079,  0.0654],\n",
      "        ...,\n",
      "        [ 0.0972, -0.0753, -0.0535,  ..., -0.0040,  0.0198, -0.0218],\n",
      "        [ 0.0278,  0.0535, -0.0416,  ..., -0.0734, -0.0575,  0.0297],\n",
      "        [-0.0912, -0.0218,  0.0416,  ..., -0.0476, -0.0555, -0.0833]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.001982869580388069,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0387,  0.0159,  0.0703, -0.0285, -0.0650,  0.0007, -0.0129,  0.0695,\n",
      "         0.0094, -0.0303,  0.0426,  0.0003, -0.0042, -0.0003, -0.0128,  0.0252,\n",
      "         0.0075,  0.0664, -0.0329,  0.0860, -0.0041, -0.0265, -0.0120,  0.0286,\n",
      "        -0.0641, -0.0649,  0.0413, -0.0056, -0.0196, -0.0689,  0.0478, -0.0570,\n",
      "         0.0137,  0.0706, -0.0711, -0.0491, -0.0052,  0.0710, -0.0563,  0.0194,\n",
      "         0.0123, -0.0077,  0.0343, -0.0160, -0.0287, -0.0515,  0.0032, -0.0670,\n",
      "        -0.0141, -0.0493, -0.0480, -0.0070,  0.0515,  0.0651,  0.0838,  0.0707,\n",
      "        -0.0728,  0.0378, -0.0459, -0.0631, -0.0184, -0.0203,  0.0506,  0.0330,\n",
      "         0.0343, -0.0325, -0.0406,  0.0106, -0.0321, -0.0153,  0.0571, -0.0426,\n",
      "        -0.0653,  0.0172, -0.0255, -0.0742,  0.0453, -0.0487, -0.0652, -0.0257,\n",
      "        -0.0639, -0.0203,  0.0080, -0.0510, -0.0169, -0.0124,  0.0460,  0.0398,\n",
      "        -0.0137, -0.0167,  0.0269, -0.0028, -0.0003, -0.0493, -0.0655, -0.0405,\n",
      "         0.0015,  0.0602,  0.0317,  0.0402, -0.0255,  0.0815,  0.0383, -0.0172,\n",
      "         0.0330,  0.0804,  0.0363, -0.0205,  0.0098,  0.0300, -0.0345,  0.0567,\n",
      "         0.0330,  0.0284,  0.0222, -0.0119,  0.0658, -0.0409,  0.0009,  0.0462,\n",
      "         0.0397,  0.0522, -0.0487, -0.0477,  0.0555,  0.0484, -0.0502,  0.0061,\n",
      "        -0.0445, -0.0416, -0.0418,  0.0620,  0.0672,  0.0558,  0.0646, -0.0641,\n",
      "        -0.0575, -0.0387,  0.0526,  0.0301,  0.0746, -0.0070, -0.0333, -0.0426,\n",
      "        -0.0564, -0.0065,  0.0068,  0.0633, -0.0349,  0.0340,  0.0023, -0.0558,\n",
      "        -0.0563,  0.0233, -0.0149, -0.0699, -0.0640, -0.0188, -0.0734,  0.0626,\n",
      "        -0.0433, -0.0527, -0.0124,  0.0255, -0.0438, -0.0048,  0.0508,  0.0465,\n",
      "         0.0030, -0.0703, -0.0130, -0.0037,  0.0380,  0.0480,  0.0375, -0.0638,\n",
      "         0.0179,  0.0185,  0.0454, -0.0328,  0.0470,  0.0016,  0.0523, -0.0039,\n",
      "        -0.0324, -0.0432,  0.0220, -0.0149,  0.0337, -0.0243, -0.0413,  0.0118],\n",
      "       requires_grad=True))\n",
      "encoder.1.0.block.1.final_projection.scale \t tensor(1.)\n",
      "encoder.1.0.block.1.final_projection.zero_point \t tensor(0)\n",
      "encoder.1.0.block.1.final_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.1.0.block.1.final_projection._packed_params._packed_params \t (tensor([[ 0.0339,  0.0634, -0.0294,  ...,  0.0611, -0.0769,  0.0837],\n",
      "        [ 0.0181,  0.0792, -0.0498,  ..., -0.1154, -0.0611,  0.0928],\n",
      "        [ 0.0769,  0.1086,  0.0724,  ..., -0.0317,  0.0769, -0.0475],\n",
      "        ...,\n",
      "        [ 0.0905,  0.0385,  0.0181,  ...,  0.0837,  0.0294,  0.0634],\n",
      "        [ 0.0679, -0.0815, -0.0226,  ...,  0.0815, -0.0588,  0.0543],\n",
      "        [-0.0136, -0.0045,  0.0837,  ...,  0.0181, -0.0091,  0.1064]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.002262883121147752,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0755, -0.0252, -0.0014, -0.0368, -0.0634,  0.0534, -0.0572, -0.0723,\n",
      "         0.0326, -0.0518, -0.0087, -0.0375, -0.0193, -0.0237, -0.0311,  0.0322,\n",
      "         0.0326, -0.0226, -0.0107,  0.0042, -0.0176,  0.0535,  0.0708, -0.0288,\n",
      "        -0.0275, -0.0107, -0.0285, -0.0476, -0.0221,  0.0288,  0.0535, -0.0343,\n",
      "        -0.0474, -0.0096, -0.0388,  0.0355, -0.0223,  0.0113, -0.0388,  0.0134,\n",
      "         0.0336,  0.0580,  0.0321,  0.0516,  0.0591,  0.0343, -0.0501, -0.0559,\n",
      "        -0.0476,  0.0499,  0.0396,  0.0603, -0.0182, -0.0396, -0.0720, -0.0129,\n",
      "        -0.0287, -0.0179, -0.0251, -0.0683,  0.0567, -0.0083, -0.0516,  0.0537,\n",
      "         0.0491, -0.0592, -0.0064, -0.0122,  0.0399,  0.0316, -0.0815, -0.0092,\n",
      "        -0.0268, -0.0415, -0.0113, -0.0296, -0.0168, -0.0483,  0.0564, -0.0024,\n",
      "        -0.0451, -0.0663, -0.0419,  0.0581, -0.0071,  0.0018, -0.0082,  0.0490,\n",
      "         0.0408,  0.0583,  0.0485, -0.0392, -0.0457,  0.0066,  0.0329,  0.0404,\n",
      "        -0.0684,  0.0432,  0.0011, -0.0586,  0.0461,  0.0524,  0.0612, -0.0287,\n",
      "         0.0035,  0.0593, -0.0732,  0.0014, -0.0293,  0.0469, -0.0224, -0.0150,\n",
      "        -0.0207,  0.0173,  0.0234,  0.0150, -0.0293,  0.0438,  0.0195, -0.0397,\n",
      "        -0.0264, -0.0341, -0.0206,  0.0568,  0.0207, -0.0110,  0.0189,  0.0449,\n",
      "        -0.0069,  0.0026, -0.0017,  0.0493, -0.0161,  0.0168, -0.0703, -0.0195,\n",
      "         0.0064, -0.0263,  0.0430,  0.0410,  0.0131,  0.0121, -0.0387,  0.0758,\n",
      "        -0.0117, -0.0188, -0.0241,  0.0558, -0.0641, -0.0351,  0.0300, -0.0450,\n",
      "         0.0415,  0.0669,  0.0155,  0.0423, -0.0370, -0.0652,  0.0440,  0.0257,\n",
      "        -0.0521,  0.0615,  0.0452,  0.0037,  0.0467,  0.0470, -0.0334,  0.0738,\n",
      "         0.0658, -0.0027,  0.0625, -0.0673,  0.0400,  0.0551,  0.0135, -0.0399,\n",
      "        -0.0508,  0.0542, -0.0412,  0.0264, -0.0480,  0.0344,  0.0352, -0.0818,\n",
      "         0.0063, -0.0497, -0.0589, -0.0090,  0.0111,  0.0298,  0.0155,  0.0541],\n",
      "       requires_grad=True))\n",
      "encoder.1.1.block.0.weight \t tensor([0.9937, 0.9714, 0.9593, 1.0001, 0.9654, 0.9882, 1.0192, 1.0262, 0.9958,\n",
      "        0.9649, 0.9616, 1.0572, 1.0028, 1.0241, 0.9845, 0.9521, 0.9181, 1.0075,\n",
      "        0.9892, 0.9921, 0.9717, 0.9962, 0.9611, 0.9773, 0.9953, 0.9913, 1.0107,\n",
      "        0.9917, 1.0318, 0.9331, 0.9890, 0.9947, 0.9410, 0.9994, 0.9932, 0.9796,\n",
      "        0.9983, 0.9620, 1.0082, 1.0188, 0.9757, 1.0089, 0.9730, 1.0271, 0.9559,\n",
      "        0.9663, 0.9945, 0.9812, 0.9947, 1.0281, 0.9631, 0.9874, 0.9615, 0.9778,\n",
      "        0.9866, 0.9812, 1.0167, 0.9659, 1.0027, 1.0020, 1.0021, 0.9653, 0.9868,\n",
      "        0.9906, 1.0175, 0.9818, 1.0886, 0.9776, 0.9650, 1.0003, 0.9981, 0.9346,\n",
      "        0.9766, 1.0103, 1.0005, 0.9716, 0.9810, 0.9807, 1.0277, 0.9830, 0.9632,\n",
      "        1.0075, 1.0407, 0.9863, 0.9853, 0.9445, 0.9828, 0.9778, 0.9768, 0.9744,\n",
      "        0.9685, 0.9772, 0.9851, 1.0019, 0.9003, 0.9585, 0.9774, 1.0165, 0.9708,\n",
      "        1.0047, 1.0300, 0.9570, 0.9828, 0.9602, 0.9993, 0.9666, 0.9802, 1.0011,\n",
      "        0.9903, 1.0057, 1.0067, 0.9951, 0.9702, 0.9724, 0.9725, 0.9783, 1.0173,\n",
      "        0.9145, 0.9807, 0.9719, 1.0214, 0.9698, 0.9521, 0.9868, 0.9648, 0.9992,\n",
      "        0.9718, 1.0061, 0.9806, 0.9610, 0.9677, 0.9595, 1.0387, 0.9973, 0.9922,\n",
      "        0.9530, 1.0113, 0.9896, 0.9858, 0.9875, 0.9518, 1.0137, 0.9432, 0.9480,\n",
      "        0.9465, 0.9908, 1.0353, 0.9774, 0.9881, 1.0240, 0.9728, 0.9582, 0.9858,\n",
      "        1.0119, 1.0194, 0.9795, 0.9401, 0.9596, 1.0130, 0.9825, 1.0257, 0.8709,\n",
      "        0.9738, 0.9818, 0.9568, 1.0064, 0.9707, 1.0130, 0.9886, 0.9547, 0.9714,\n",
      "        0.9901, 1.0310, 1.0049, 1.0027, 0.9570, 0.9742, 0.9477, 1.0104, 0.9760,\n",
      "        1.0082, 0.9400, 0.9876, 1.0089, 0.9767, 0.9772, 0.9799, 0.9866, 1.0075,\n",
      "        1.0061, 0.9923, 1.0012])\n",
      "encoder.1.1.block.0.bias \t tensor([ 0.0325, -0.0406, -0.0538, -0.0492, -0.0241, -0.0227, -0.0540,  0.0465,\n",
      "        -0.0445,  0.0665, -0.0278,  0.0831, -0.0207, -0.0281,  0.0558, -0.0350,\n",
      "        -0.0202, -0.0456,  0.0222,  0.0348, -0.0292,  0.0649, -0.0198, -0.0437,\n",
      "         0.0517, -0.0372, -0.0576, -0.0188, -0.0257, -0.0104, -0.0399, -0.0589,\n",
      "         0.0535,  0.0003, -0.0499,  0.0144, -0.0225,  0.0088,  0.0080, -0.0745,\n",
      "         0.0080, -0.0024, -0.0391,  0.0528, -0.0358, -0.0389,  0.0713,  0.0869,\n",
      "         0.0288, -0.0359,  0.0539, -0.0002,  0.0251, -0.0218, -0.0636,  0.0564,\n",
      "         0.0429,  0.0358, -0.0485, -0.0075, -0.0772, -0.0341,  0.0062, -0.0044,\n",
      "        -0.0114, -0.0581, -0.0392,  0.0384, -0.0218, -0.0359, -0.0529,  0.0006,\n",
      "        -0.0394, -0.0295,  0.0661, -0.0599,  0.0139,  0.0441, -0.0629, -0.0657,\n",
      "        -0.0282, -0.0390,  0.0638,  0.0337, -0.0104, -0.0502,  0.0383,  0.0498,\n",
      "        -0.0545,  0.0611, -0.0640,  0.0071,  0.0135, -0.0678, -0.0383,  0.0161,\n",
      "         0.0477, -0.0725, -0.0784, -0.0270, -0.0377,  0.0010, -0.0007, -0.0784,\n",
      "         0.0678,  0.0375, -0.0256,  0.0658,  0.0337, -0.0336,  0.0309, -0.0393,\n",
      "        -0.0385, -0.0314,  0.0030, -0.0211,  0.0408, -0.0220, -0.0095, -0.0109,\n",
      "         0.0264, -0.0452, -0.0269,  0.0332,  0.0258, -0.0139,  0.0988,  0.0328,\n",
      "         0.0569, -0.0179,  0.0247, -0.0376, -0.0473, -0.0297, -0.0384,  0.0016,\n",
      "        -0.0758, -0.0801, -0.0372, -0.0294, -0.0059, -0.0217, -0.0427,  0.0141,\n",
      "        -0.0277,  0.0538, -0.0129,  0.0496, -0.0295,  0.0625, -0.0141, -0.0573,\n",
      "         0.0036, -0.0743,  0.0385, -0.0593,  0.0006, -0.0495, -0.0633, -0.0841,\n",
      "         0.0769, -0.0081, -0.0590, -0.0636, -0.0461, -0.0404, -0.0495,  0.0827,\n",
      "        -0.0563,  0.0650, -0.0034, -0.0189, -0.0211, -0.0722,  0.0438,  0.0745,\n",
      "         0.0355, -0.0529, -0.0147,  0.0226, -0.0306,  0.0597, -0.0596,  0.0245,\n",
      "        -0.0314, -0.0195, -0.0300, -0.0419,  0.0179,  0.0376,  0.0331,  0.0795])\n",
      "encoder.1.1.block.1.0.scale \t tensor(1.)\n",
      "encoder.1.1.block.1.0.zero_point \t tensor(0)\n",
      "encoder.1.1.block.1.0._packed_params.dtype \t torch.qint8\n",
      "encoder.1.1.block.1.0._packed_params._packed_params \t (tensor([[ 0.0198,  0.0198, -0.0656,  ..., -0.0046,  0.0671, -0.0427],\n",
      "        [ 0.0473, -0.0031,  0.0595,  ...,  0.0717,  0.0046,  0.0214],\n",
      "        [-0.0961, -0.0565, -0.0504,  ..., -0.0717,  0.0015, -0.0061],\n",
      "        ...,\n",
      "        [-0.0290,  0.0366,  0.0290,  ...,  0.0534,  0.0717, -0.0458],\n",
      "        [-0.0275,  0.0046, -0.0290,  ..., -0.0214,  0.0366,  0.0427],\n",
      "        [ 0.0793,  0.0183, -0.0504,  ..., -0.0824,  0.0320, -0.0198]],\n",
      "       size=(768, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0015259035862982273,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0182,  0.0171,  0.0344, -0.0906, -0.0996, -0.0831,  0.0267, -0.0796,\n",
      "        -0.0923,  0.0099, -0.0195, -0.0743, -0.0255,  0.0058, -0.0051, -0.0416,\n",
      "        -0.0903,  0.0227, -0.1088, -0.0388, -0.1040,  0.0015, -0.1123, -0.0399,\n",
      "        -0.1011,  0.0355, -0.0762,  0.0157,  0.0130, -0.0526, -0.0125, -0.0275,\n",
      "        -0.0900, -0.0629, -0.0646, -0.0528, -0.0385, -0.0641,  0.0023, -0.0238,\n",
      "        -0.0529,  0.0421, -0.0650, -0.0186, -0.0345, -0.1108, -0.0581, -0.0057,\n",
      "         0.0122, -0.0056,  0.0298, -0.0110, -0.0883,  0.0216, -0.0370, -0.0542,\n",
      "        -0.1011, -0.0077, -0.0509, -0.0128, -0.1059, -0.0082, -0.0120, -0.0438,\n",
      "        -0.0407, -0.0561, -0.0415, -0.0779, -0.0380, -0.0011, -0.0053,  0.0153,\n",
      "        -0.0259, -0.0988, -0.0822, -0.0321, -0.0067, -0.0804,  0.0185, -0.0285,\n",
      "        -0.1001, -0.0444, -0.1026,  0.0225,  0.0315, -0.0405, -0.0127, -0.1022,\n",
      "        -0.0153, -0.1019,  0.0160, -0.0023, -0.0996, -0.0540, -0.1278, -0.0243,\n",
      "         0.0194, -0.0073,  0.0229, -0.0376,  0.0048, -0.0597,  0.0115, -0.0342,\n",
      "         0.0444,  0.0018,  0.0046, -0.0324, -0.0642, -0.0980, -0.0306,  0.0454,\n",
      "        -0.0030,  0.0268, -0.1016, -0.0302,  0.0286, -0.0801, -0.0139,  0.0386,\n",
      "        -0.1027, -0.1174, -0.0289,  0.0172, -0.0318, -0.0398,  0.0254, -0.0892,\n",
      "         0.0234,  0.0451,  0.0018, -0.0254,  0.0061, -0.0902, -0.0369,  0.0399,\n",
      "        -0.0934,  0.0281, -0.0445, -0.0728, -0.0528, -0.0672, -0.0515, -0.0004,\n",
      "         0.0021,  0.0333, -0.0552,  0.0160,  0.0010,  0.0225, -0.0787, -0.0653,\n",
      "        -0.0798, -0.0127, -0.0237, -0.0303,  0.0326, -0.0177,  0.0105, -0.0457,\n",
      "        -0.0004, -0.0234,  0.0428, -0.1105, -0.0353, -0.0616, -0.0250, -0.0004,\n",
      "         0.0078,  0.0071, -0.0966, -0.0508, -0.0942, -0.1124, -0.0240, -0.0266,\n",
      "        -0.0256,  0.0417, -0.0783,  0.0177, -0.0612,  0.0292,  0.0093, -0.0350,\n",
      "        -0.0499, -0.0478, -0.0626, -0.0347, -0.0926, -0.0518, -0.0332, -0.0191,\n",
      "         0.0172,  0.0296, -0.0504,  0.0449, -0.0407, -0.0367, -0.0496, -0.0500,\n",
      "        -0.0771,  0.0396, -0.0126, -0.0506, -0.0260,  0.0226, -0.1118, -0.0080,\n",
      "        -0.0826, -0.0333, -0.0740,  0.0202, -0.0348, -0.0434, -0.0035, -0.0365,\n",
      "         0.0369, -0.0260, -0.0402, -0.0337,  0.0345, -0.0063, -0.0514,  0.0069,\n",
      "        -0.0319,  0.0403, -0.0147,  0.0091,  0.0382, -0.0586, -0.0270, -0.0181,\n",
      "        -0.0518, -0.0176, -0.0328, -0.0591,  0.0255, -0.0372,  0.0022, -0.0592,\n",
      "        -0.1069, -0.0975, -0.0601,  0.0499, -0.0102,  0.0052, -0.0244, -0.0366,\n",
      "        -0.0742,  0.0151,  0.0002, -0.0357, -0.0027,  0.0316, -0.0554, -0.0856,\n",
      "        -0.0656, -0.0913, -0.0949, -0.0624, -0.0538, -0.0339, -0.0214, -0.0549,\n",
      "         0.0377,  0.0447,  0.0165,  0.0326, -0.0313, -0.0438, -0.0353, -0.0553,\n",
      "         0.0298, -0.0210,  0.0190, -0.0747, -0.0435, -0.0428, -0.0799,  0.0337,\n",
      "        -0.0294, -0.0859, -0.0182, -0.1112, -0.0016, -0.0614, -0.0280,  0.0322,\n",
      "         0.0038, -0.0327, -0.0135,  0.0252, -0.0352, -0.0862, -0.0268,  0.0209,\n",
      "        -0.0484, -0.0728, -0.0795, -0.0768, -0.0747, -0.0427, -0.0418, -0.0196,\n",
      "        -0.0655,  0.0289, -0.0935, -0.1109,  0.0075, -0.1006,  0.0068, -0.0593,\n",
      "        -0.0404, -0.1146, -0.0709, -0.0507, -0.0966, -0.0187, -0.0087, -0.0641,\n",
      "        -0.0781, -0.1312, -0.0460, -0.0132, -0.0282, -0.0799, -0.0673, -0.0093,\n",
      "        -0.0161, -0.0803, -0.0462, -0.0171, -0.0172,  0.0087, -0.0776, -0.0144,\n",
      "         0.0311, -0.0009, -0.0635,  0.0060,  0.0242, -0.0255, -0.0729, -0.0433,\n",
      "         0.0421, -0.0002, -0.0557,  0.0006, -0.0281, -0.0053, -0.0369,  0.0160,\n",
      "        -0.0177, -0.0524, -0.0711, -0.0587, -0.0109,  0.0454, -0.0247, -0.0124,\n",
      "         0.0049,  0.0150, -0.0607, -0.1093,  0.0271, -0.0426,  0.0111, -0.0343,\n",
      "        -0.0480, -0.0382, -0.0534, -0.0260, -0.0436, -0.0086, -0.0676,  0.0562,\n",
      "        -0.0488,  0.0195, -0.0242,  0.0361, -0.1056, -0.0407, -0.0989, -0.0123,\n",
      "        -0.0341, -0.0371, -0.1143,  0.0285,  0.0163, -0.0517, -0.0200, -0.0422,\n",
      "        -0.0031,  0.0250, -0.0496, -0.0769, -0.0393, -0.0005, -0.0369, -0.0523,\n",
      "        -0.0755, -0.0030, -0.1036,  0.0222, -0.0076, -0.0231, -0.1002, -0.0098,\n",
      "        -0.0450, -0.0568, -0.0122, -0.0116,  0.0397, -0.0942, -0.1028, -0.1057,\n",
      "        -0.0624,  0.0112, -0.0969, -0.0133, -0.0893,  0.0066, -0.0945, -0.0600,\n",
      "        -0.0387, -0.0011,  0.0343,  0.0271, -0.0843, -0.0532, -0.0362, -0.0325,\n",
      "        -0.0331,  0.0063, -0.0498, -0.0336, -0.0694,  0.0240, -0.0746,  0.0128,\n",
      "        -0.0610,  0.0261, -0.0742, -0.0173,  0.0439, -0.0269, -0.0340, -0.0390,\n",
      "        -0.1164,  0.0341, -0.1164, -0.0712, -0.0064, -0.0053,  0.0375, -0.0035,\n",
      "        -0.0224, -0.0821,  0.0265, -0.0291,  0.0227,  0.0023,  0.0180, -0.0508,\n",
      "         0.0322, -0.0709,  0.0317, -0.0540, -0.0025, -0.0622,  0.0068,  0.0245,\n",
      "        -0.0389, -0.0494,  0.0306, -0.0315,  0.0218,  0.0035, -0.1014, -0.0389,\n",
      "         0.0300, -0.0603,  0.0021,  0.0007, -0.0531,  0.0010,  0.0332, -0.0028,\n",
      "        -0.0776, -0.0843,  0.0143,  0.0175, -0.1093, -0.0839,  0.0043,  0.0177,\n",
      "        -0.0796, -0.0958,  0.0144, -0.0609, -0.0439, -0.0361, -0.0808, -0.0355,\n",
      "        -0.0041, -0.0861, -0.0594,  0.0372, -0.1120, -0.0393, -0.1041, -0.0435,\n",
      "         0.0208, -0.0840, -0.0116, -0.0875, -0.0173, -0.0531, -0.0352, -0.0382,\n",
      "        -0.0820,  0.0205,  0.0168,  0.0253, -0.0633,  0.0175,  0.0026,  0.0236,\n",
      "         0.0332, -0.0261, -0.0208, -0.0877, -0.0460, -0.0875, -0.0848,  0.0040,\n",
      "         0.0513, -0.0478, -0.0397, -0.0332,  0.0037, -0.0320, -0.0102, -0.0147,\n",
      "        -0.0840, -0.0604, -0.0325, -0.0008,  0.0084, -0.0171, -0.0229,  0.0049,\n",
      "        -0.0604, -0.1192, -0.0727,  0.0116,  0.0210,  0.0271, -0.0899,  0.0194,\n",
      "        -0.0618,  0.0322,  0.0037, -0.0648, -0.0759, -0.0493, -0.0696, -0.0898,\n",
      "        -0.0896, -0.1015,  0.0252, -0.0856,  0.0066,  0.0243, -0.0395, -0.0287,\n",
      "         0.0052, -0.0084, -0.0956, -0.0195, -0.0410,  0.0364, -0.0248, -0.0033,\n",
      "        -0.0772, -0.0071, -0.1003, -0.0468, -0.0537, -0.0853, -0.0383, -0.0277,\n",
      "         0.0105, -0.0365, -0.0701, -0.0086, -0.0762, -0.0424, -0.0249, -0.0515,\n",
      "        -0.0179,  0.0137, -0.0951, -0.1139,  0.0135, -0.0067,  0.0266,  0.0005,\n",
      "        -0.0063, -0.0995,  0.0364, -0.0752,  0.0011,  0.0079,  0.0015, -0.0055,\n",
      "        -0.0326, -0.0295, -0.1013, -0.0189, -0.0043,  0.0303, -0.0724, -0.0264,\n",
      "        -0.0595, -0.0545, -0.0587, -0.0070, -0.0701, -0.0042, -0.0710, -0.0227,\n",
      "        -0.0047, -0.0055, -0.0179, -0.0401, -0.1078, -0.0590, -0.0666, -0.0670,\n",
      "        -0.0606, -0.0419, -0.0921, -0.0379, -0.0611, -0.0386,  0.0274,  0.0224,\n",
      "        -0.0613, -0.0933, -0.0257, -0.0053, -0.0036, -0.0832, -0.0195, -0.0714,\n",
      "        -0.0042,  0.0301, -0.0567, -0.0444,  0.0420, -0.0830, -0.0030,  0.0131,\n",
      "         0.0308, -0.0304,  0.0344, -0.0514, -0.0600,  0.0004, -0.0444, -0.0797,\n",
      "        -0.0074, -0.0230, -0.0301, -0.0262, -0.0454, -0.0968,  0.0289, -0.0772,\n",
      "         0.0271,  0.0353, -0.0471, -0.0798,  0.0210,  0.0313, -0.0395, -0.0614,\n",
      "        -0.0148, -0.0213, -0.0972, -0.0292, -0.0981, -0.0534,  0.0240, -0.0808,\n",
      "        -0.1010,  0.0035, -0.0514, -0.0257, -0.0313, -0.0959, -0.0183,  0.0143,\n",
      "         0.0394, -0.0288, -0.1012, -0.0667,  0.0400,  0.0068, -0.0739,  0.0069,\n",
      "        -0.0998, -0.0233, -0.0055, -0.0623, -0.0038, -0.0706,  0.0010, -0.0921,\n",
      "        -0.0195, -0.0279, -0.0558, -0.0104, -0.0176, -0.0713, -0.0073, -0.0889,\n",
      "        -0.0956, -0.0846, -0.0117, -0.0430, -0.0581, -0.0808,  0.0306, -0.0508,\n",
      "        -0.0331,  0.0161, -0.0695, -0.0664,  0.0221, -0.1079, -0.0894, -0.0123,\n",
      "         0.0203,  0.0404,  0.0290, -0.1045, -0.0226, -0.0288, -0.0193, -0.0235,\n",
      "        -0.0808, -0.1036, -0.0382, -0.0802, -0.1034,  0.0008,  0.0144,  0.0025,\n",
      "        -0.0477,  0.0394, -0.0726, -0.0920, -0.0072, -0.0339, -0.0147, -0.0131],\n",
      "       requires_grad=True))\n",
      "encoder.1.1.block.1.2.scale \t tensor(1.)\n",
      "encoder.1.1.block.1.2.zero_point \t tensor(0)\n",
      "encoder.1.1.block.1.2._packed_params.dtype \t torch.qint8\n",
      "encoder.1.1.block.1.2._packed_params._packed_params \t (tensor([[ 0.0040,  0.0260, -0.0020,  ...,  0.0130,  0.0170,  0.0150],\n",
      "        [ 0.0130,  0.0651,  0.0591,  ..., -0.0040,  0.0200, -0.0130],\n",
      "        [-0.0271, -0.0080, -0.0771,  ..., -0.0180, -0.0321, -0.0200],\n",
      "        ...,\n",
      "        [ 0.0090, -0.0311,  0.0110,  ...,  0.0100,  0.0160,  0.0110],\n",
      "        [ 0.0020, -0.0040,  0.0180,  ..., -0.0110, -0.0160, -0.0010],\n",
      "        [ 0.0531, -0.0140, -0.0271,  ..., -0.0020,  0.0250, -0.0341]],\n",
      "       size=(192, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0010019170586019754,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0257,  0.0174,  0.0237, -0.0033, -0.0239,  0.0093,  0.0101, -0.0020,\n",
      "         0.0221, -0.0301,  0.0261,  0.0189,  0.0166, -0.0340, -0.0408, -0.0176,\n",
      "        -0.0383, -0.0084,  0.0196, -0.0005,  0.0237, -0.0072,  0.0141,  0.0133,\n",
      "        -0.0282,  0.0124, -0.0030,  0.0285,  0.0146, -0.0311,  0.0068,  0.0035,\n",
      "        -0.0216, -0.0471,  0.0045, -0.0649,  0.0284, -0.0216, -0.0057,  0.0363,\n",
      "         0.0141, -0.0173, -0.0015,  0.0084,  0.0216, -0.0107, -0.0288,  0.0183,\n",
      "        -0.0137, -0.0124,  0.0279, -0.0348, -0.0225,  0.0466,  0.0214,  0.0340,\n",
      "         0.0130, -0.0329,  0.0251,  0.0252,  0.0270, -0.0358, -0.0113,  0.0241,\n",
      "         0.0119, -0.0085,  0.0344, -0.0350, -0.0149,  0.0122, -0.0294,  0.0254,\n",
      "        -0.0069,  0.0095,  0.0020, -0.0272,  0.0288,  0.0080, -0.0214,  0.0082,\n",
      "         0.0186,  0.0152, -0.0534,  0.0317,  0.0196,  0.0328, -0.0511,  0.0094,\n",
      "        -0.0191, -0.0184,  0.0139,  0.0007, -0.0097, -0.0194,  0.0162,  0.0083,\n",
      "        -0.0223,  0.0020,  0.0125, -0.0105, -0.0089,  0.0263,  0.0173, -0.0299,\n",
      "         0.0092,  0.0250, -0.0093,  0.0040, -0.0335, -0.0036,  0.0229,  0.0218,\n",
      "         0.0343,  0.0147,  0.0208,  0.0232, -0.0324, -0.0121,  0.0097, -0.0057,\n",
      "         0.0016,  0.0143, -0.0261, -0.0081, -0.0069, -0.0182, -0.0365, -0.0350,\n",
      "        -0.0328,  0.0222,  0.0298, -0.0145,  0.0305,  0.0102,  0.0198,  0.0051,\n",
      "         0.0273, -0.0038,  0.0118,  0.0114,  0.0313,  0.0079,  0.0028, -0.0105,\n",
      "         0.0353,  0.0057,  0.0305, -0.0506, -0.0267, -0.0500, -0.0105, -0.0099,\n",
      "        -0.0245, -0.0230,  0.0283, -0.0226, -0.0233,  0.0220, -0.0320,  0.0224,\n",
      "        -0.0269,  0.0001, -0.0263,  0.0011,  0.0012,  0.0476,  0.0048,  0.0187,\n",
      "         0.0444,  0.0089,  0.0293, -0.0351,  0.0009,  0.0341, -0.0158,  0.0229,\n",
      "        -0.0244,  0.0002, -0.0423, -0.0081,  0.0082, -0.0109,  0.0049, -0.0274,\n",
      "         0.0331,  0.0383, -0.0290, -0.0227, -0.0512, -0.0152, -0.0091, -0.0368],\n",
      "       requires_grad=True))\n",
      "encoder.2.0.block.0.weight \t tensor([1.0683, 1.1038, 1.0350, 1.0554, 1.0545, 1.0968, 1.0435, 1.0464, 1.0424,\n",
      "        1.0330, 1.0687, 1.0465, 1.0285, 1.0409, 1.0376, 1.0527, 1.0340, 1.0383,\n",
      "        1.0693, 1.0516, 1.0870, 1.0546, 1.0515, 1.0133, 1.0666, 1.0618, 1.0167,\n",
      "        1.0883, 1.0738, 1.0407, 1.0547, 1.0548, 1.0306, 1.0189, 1.0361, 1.0457,\n",
      "        1.0166, 1.0672, 1.0638, 1.0604, 1.0296, 1.0497, 1.0919, 1.0701, 1.0854,\n",
      "        1.0413, 1.0159, 1.0239, 1.0166, 1.0809, 1.0415, 1.0555, 1.0513, 1.0283,\n",
      "        1.0021, 1.0298, 1.0211, 1.0502, 1.0596, 1.0428, 1.0692, 1.0472, 1.0645,\n",
      "        1.0871, 1.0801, 1.0582, 1.0663, 1.0357, 1.0764, 1.0415, 1.0337, 1.0298,\n",
      "        1.0500, 1.0724, 1.1178, 1.0711, 1.0815, 1.0112, 1.0684, 1.0699, 1.0131,\n",
      "        1.0372, 1.0205, 0.9901, 1.0182, 1.0343, 1.0692, 1.0715, 1.0384, 1.0586,\n",
      "        1.0698, 1.0338, 1.0445, 1.0453, 1.0465, 1.0413, 1.0643, 1.0003, 1.0596,\n",
      "        1.0190, 1.0605, 1.0160, 0.9936, 1.0770, 1.0673, 1.0238, 1.0453, 1.0252,\n",
      "        1.0623, 1.0521, 1.0450, 1.0459, 1.1072, 1.0517, 1.0281, 1.0441, 1.0597,\n",
      "        1.0749, 1.0324, 1.0577, 1.1065, 1.0629, 1.0858, 1.0406, 1.0875, 1.0768,\n",
      "        1.0579, 1.0378, 1.0388, 1.0467, 1.0440, 1.0442, 1.0870, 1.0864, 1.0348,\n",
      "        1.0399, 1.0827, 1.0150, 1.0478, 1.0101, 1.0656, 1.0410, 1.0152, 1.0353,\n",
      "        1.0281, 1.0667, 1.0512, 1.0563, 1.0402, 1.0478, 1.0174, 1.0094, 1.0656,\n",
      "        1.0313, 1.0459, 1.0306, 1.0728, 1.0465, 1.0714, 1.0757, 1.0682, 1.0778,\n",
      "        1.0566, 1.0521, 1.0196, 1.0586, 1.0045, 1.0909, 1.0369, 1.0134, 0.9898,\n",
      "        1.0396, 1.0366, 1.0565, 1.0528, 1.0106, 1.0321, 1.0342, 1.1135, 0.9966,\n",
      "        1.0753, 1.0765, 1.0382, 1.0453, 1.0385, 1.0471, 1.0371, 1.0347, 1.0597,\n",
      "        1.0563, 1.0756, 1.0350])\n",
      "encoder.2.0.block.0.bias \t tensor([-3.6446e-03, -6.5881e-03, -1.1986e-02, -2.4973e-03, -9.9924e-03,\n",
      "        -1.4291e-02,  1.7567e-02, -1.6188e-02,  3.1986e-03,  1.4056e-02,\n",
      "        -5.9809e-03, -1.7776e-02, -1.0477e-03, -2.1115e-02, -6.9671e-04,\n",
      "        -2.7420e-03, -1.9178e-02, -6.8525e-03,  1.4848e-02,  1.5895e-02,\n",
      "         1.1005e-02,  5.6878e-03, -6.9563e-03,  5.9341e-03, -1.0367e-02,\n",
      "        -1.3444e-02,  3.8690e-03, -3.0317e-02,  1.3401e-02,  5.8541e-04,\n",
      "         1.1199e-02,  8.0511e-03,  2.4858e-03, -1.8745e-02,  2.1255e-02,\n",
      "         5.9271e-03,  1.1017e-02,  1.4185e-03,  1.5176e-03,  1.5505e-02,\n",
      "        -8.5912e-03, -1.6486e-02,  2.2144e-02,  5.1069e-03, -3.1707e-02,\n",
      "         2.0572e-02, -2.2695e-02,  1.0978e-02, -6.9593e-03,  2.7257e-03,\n",
      "         2.3423e-02,  1.3376e-02,  9.1066e-03, -1.3712e-03, -1.5862e-02,\n",
      "         2.0420e-02,  2.2255e-03, -7.1601e-03,  7.5766e-04,  1.2090e-02,\n",
      "         9.9160e-03, -9.5357e-03, -2.3562e-03, -5.7854e-03, -1.1888e-03,\n",
      "        -1.3136e-02,  1.2679e-02,  1.9396e-03,  2.3740e-02, -3.2642e-03,\n",
      "        -1.4836e-02, -6.9739e-03, -1.7619e-02, -1.8351e-02,  2.7037e-03,\n",
      "         2.7729e-03, -7.6364e-03, -9.2561e-03, -1.8954e-02,  8.0507e-03,\n",
      "         1.1746e-02,  1.1925e-02, -3.1888e-02, -7.4455e-03,  9.7302e-03,\n",
      "        -1.3232e-02, -1.4125e-02,  2.1757e-02, -1.0573e-02,  3.5075e-03,\n",
      "        -3.2732e-02, -7.2386e-03,  2.0204e-02, -1.0977e-02,  1.5115e-02,\n",
      "         2.5450e-03, -4.1520e-04, -1.0603e-02,  6.1987e-03, -1.3471e-02,\n",
      "        -2.6834e-02, -1.6710e-02,  1.6658e-03, -1.3414e-02,  1.3979e-02,\n",
      "         2.4177e-02,  6.1661e-03, -7.4873e-03,  1.7036e-02, -7.8963e-03,\n",
      "        -7.3729e-04,  2.2395e-02, -1.7012e-03,  1.1685e-02,  2.2202e-02,\n",
      "        -2.0007e-02,  1.0618e-02, -1.7621e-02,  6.3320e-03,  2.0332e-02,\n",
      "        -1.7218e-02, -5.4499e-03, -1.0615e-02,  6.4871e-03, -2.2855e-02,\n",
      "         5.4921e-03,  5.4272e-03, -2.9475e-02, -2.2723e-02, -2.8847e-03,\n",
      "        -9.6929e-03, -5.8019e-03, -5.4211e-03, -9.9635e-03,  1.9887e-02,\n",
      "        -1.0792e-02, -1.3309e-02, -9.6228e-03, -8.7096e-03, -1.8253e-02,\n",
      "         4.4729e-03, -5.8649e-03,  1.6258e-02, -1.6976e-02, -1.8517e-03,\n",
      "        -4.0630e-02, -8.2244e-03,  1.8420e-02, -4.0826e-03, -6.9102e-03,\n",
      "        -2.2173e-02, -6.3609e-03,  2.6126e-02, -9.1581e-03,  2.0242e-03,\n",
      "        -1.8304e-03, -1.4936e-02, -2.7712e-02, -2.2822e-03, -2.7136e-03,\n",
      "        -6.8565e-03, -9.4793e-03,  5.0700e-03, -1.9538e-02, -2.4476e-02,\n",
      "         1.3336e-02, -1.2966e-02,  1.7717e-03,  2.4539e-02, -3.3361e-04,\n",
      "         2.6973e-02, -6.5365e-04,  1.4755e-02,  1.8234e-03,  7.7648e-03,\n",
      "         7.7296e-03, -1.3725e-02,  4.2852e-03,  2.4533e-03,  1.0880e-02,\n",
      "         5.0256e-03,  2.1404e-02,  2.1154e-03, -1.7917e-02, -1.5392e-02,\n",
      "         2.0912e-02, -8.4101e-04, -3.7363e-02,  9.3017e-03, -5.0554e-03,\n",
      "         6.3814e-05,  1.8658e-02])\n",
      "encoder.2.0.block.1.queries_projection.scale \t tensor(1.)\n",
      "encoder.2.0.block.1.queries_projection.zero_point \t tensor(0)\n",
      "encoder.2.0.block.1.queries_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.2.0.block.1.queries_projection._packed_params._packed_params \t (tensor([[ 0.0659,  0.0557, -0.0203,  ...,  0.0051,  0.0135,  0.0524],\n",
      "        [ 0.0186,  0.0845,  0.0490,  ..., -0.0372, -0.0608,  0.0405],\n",
      "        [-0.1199,  0.0946,  0.0422,  ..., -0.0946,  0.0507, -0.1233],\n",
      "        ...,\n",
      "        [ 0.0726, -0.0084,  0.0186,  ..., -0.0203, -0.0287,  0.0000],\n",
      "        [ 0.0456, -0.0051,  0.0608,  ..., -0.1216, -0.0338,  0.0135],\n",
      "        [ 0.0051, -0.0338,  0.0439,  ...,  0.0473, -0.0203,  0.0203]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0016892320709303021,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0474, -0.0591,  0.0354,  0.0356,  0.0047, -0.0257, -0.0453, -0.0116,\n",
      "         0.0356,  0.0200,  0.0329, -0.0147,  0.0399, -0.0467,  0.0271, -0.0449,\n",
      "        -0.0562,  0.0506,  0.0365, -0.0007, -0.0083, -0.0664,  0.0189, -0.0215,\n",
      "         0.0105,  0.0515, -0.0146,  0.0300,  0.0451,  0.0180, -0.0243,  0.0550,\n",
      "         0.0850, -0.0167, -0.0406, -0.0415,  0.0517,  0.0090,  0.0246,  0.0473,\n",
      "        -0.0680, -0.0364, -0.0202,  0.0599,  0.0359,  0.0315,  0.0284, -0.0043,\n",
      "        -0.0520,  0.0522, -0.0829,  0.0416,  0.0577,  0.0427,  0.0262,  0.1026,\n",
      "         0.0340, -0.0464, -0.0228, -0.0404, -0.0023,  0.0222,  0.0028,  0.0480,\n",
      "         0.0213, -0.0096,  0.0375,  0.0446,  0.0804, -0.0941, -0.0218, -0.0878,\n",
      "         0.0012, -0.0336,  0.0021,  0.0437,  0.0027,  0.0693, -0.0531, -0.0227,\n",
      "        -0.0637,  0.0196, -0.0526,  0.0869,  0.0410,  0.0274,  0.0809, -0.0611,\n",
      "        -0.0492, -0.1160,  0.0094, -0.0385,  0.0739,  0.0549,  0.0510,  0.0356,\n",
      "         0.1354, -0.0106, -0.0444, -0.0399, -0.0437,  0.0502, -0.0210,  0.0681,\n",
      "         0.0557,  0.0002,  0.0106, -0.0545, -0.0645, -0.0085, -0.0273,  0.0278,\n",
      "        -0.0790, -0.0440, -0.0046,  0.0896,  0.0093, -0.1086,  0.0340,  0.0217,\n",
      "        -0.1201,  0.0837, -0.0037,  0.0102, -0.0507,  0.0486, -0.0395, -0.0102,\n",
      "         0.0012, -0.0202,  0.0360,  0.0057,  0.0283, -0.0506,  0.0385, -0.0979,\n",
      "        -0.0304, -0.0381,  0.0305,  0.0615,  0.0078,  0.0104,  0.0819, -0.0788,\n",
      "        -0.0533,  0.0445,  0.0433, -0.0396, -0.0487, -0.0839, -0.0483,  0.0189,\n",
      "         0.0456, -0.0183,  0.0405, -0.0615,  0.0946, -0.0663, -0.0071, -0.0489,\n",
      "        -0.0027,  0.0488, -0.0059, -0.0240, -0.0779, -0.0231,  0.0443,  0.0553,\n",
      "        -0.0729, -0.0794,  0.0382,  0.0330, -0.0044,  0.0138, -0.0115, -0.0436,\n",
      "        -0.0390, -0.0644,  0.0246,  0.0177, -0.0186, -0.0277, -0.0305, -0.0711,\n",
      "        -0.0247, -0.0575,  0.0279, -0.0415, -0.0487,  0.0317, -0.0035,  0.0784],\n",
      "       requires_grad=True))\n",
      "encoder.2.0.block.1.values_projection.scale \t tensor(1.)\n",
      "encoder.2.0.block.1.values_projection.zero_point \t tensor(0)\n",
      "encoder.2.0.block.1.values_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.2.0.block.1.values_projection._packed_params._packed_params \t (tensor([[ 0.0189,  0.0142,  0.0708,  ...,  0.0519, -0.0660, -0.0283],\n",
      "        [ 0.0684, -0.0047,  0.1109,  ...,  0.0236,  0.0354, -0.0472],\n",
      "        [ 0.0731,  0.0212, -0.0755,  ..., -0.0944, -0.0472,  0.0354],\n",
      "        ...,\n",
      "        [-0.0991, -0.0165, -0.0354,  ..., -0.0637, -0.0330, -0.0071],\n",
      "        [-0.0472, -0.0448, -0.0495,  ..., -0.0354, -0.0425,  0.0425],\n",
      "        [ 0.0660, -0.0165, -0.0377,  ..., -0.0212,  0.0377,  0.0071]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0023588594049215317,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0334, -0.0700,  0.0336,  0.0698, -0.0622, -0.0227, -0.0196,  0.0694,\n",
      "         0.0144,  0.0158,  0.0085,  0.0283,  0.0205, -0.0590,  0.0164,  0.0162,\n",
      "         0.0164, -0.0097, -0.0309, -0.0020, -0.0551, -0.0324,  0.0687,  0.0277,\n",
      "        -0.0071,  0.0630, -0.0033,  0.0383,  0.0046,  0.0138,  0.0549, -0.0185,\n",
      "         0.0259,  0.0538,  0.0659,  0.0193, -0.0386, -0.0531,  0.0229, -0.0028,\n",
      "        -0.0136, -0.0204, -0.0063, -0.0479,  0.0007,  0.0120, -0.0198, -0.0202,\n",
      "         0.0524,  0.0228, -0.0103, -0.0686,  0.0224, -0.0495, -0.0356,  0.0022,\n",
      "        -0.0196,  0.0594,  0.0218, -0.0406,  0.0566, -0.0506, -0.0536, -0.0449,\n",
      "         0.0177, -0.0523, -0.0097,  0.0094, -0.0295,  0.0503,  0.0341, -0.0665,\n",
      "        -0.0505, -0.0470, -0.0393,  0.0403,  0.0325, -0.0455,  0.0572,  0.0651,\n",
      "         0.0665, -0.0228,  0.0473, -0.0705,  0.0145,  0.0382,  0.0608,  0.0021,\n",
      "        -0.0592,  0.0564,  0.0703, -0.0422, -0.0617,  0.0439,  0.0421,  0.0046,\n",
      "         0.0263, -0.0350, -0.0101,  0.0639, -0.0179, -0.0803,  0.0515,  0.0040,\n",
      "        -0.0344, -0.0043,  0.0449, -0.0810, -0.0441,  0.0428,  0.0663, -0.0099,\n",
      "         0.0007,  0.0257,  0.0029,  0.0552,  0.0034, -0.0509, -0.0204, -0.0626,\n",
      "         0.0296,  0.0512,  0.0209,  0.0436,  0.0614,  0.0175, -0.0147,  0.0518,\n",
      "        -0.0604,  0.0149,  0.0488,  0.0178, -0.0027, -0.0471, -0.0464,  0.0460,\n",
      "        -0.0598,  0.0575,  0.0561, -0.0557, -0.0567, -0.0001, -0.0037, -0.0457,\n",
      "        -0.0548, -0.0079,  0.0359,  0.0833, -0.0124,  0.0048, -0.0096, -0.0442,\n",
      "         0.0694, -0.0558,  0.0205, -0.0589,  0.0219,  0.0625,  0.0178, -0.0767,\n",
      "         0.0073, -0.0542, -0.0129, -0.0290, -0.0674,  0.0122,  0.0506,  0.0320,\n",
      "         0.0377, -0.0208,  0.0084, -0.0291, -0.0353,  0.0344,  0.0649, -0.0416,\n",
      "        -0.0941, -0.0224, -0.0470,  0.0513,  0.0421, -0.0470,  0.0280, -0.0281,\n",
      "         0.0338, -0.0615, -0.0426, -0.0343, -0.0123, -0.0199, -0.0608, -0.0035],\n",
      "       requires_grad=True))\n",
      "encoder.2.0.block.1.keys_projection.scale \t tensor(1.)\n",
      "encoder.2.0.block.1.keys_projection.zero_point \t tensor(0)\n",
      "encoder.2.0.block.1.keys_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.2.0.block.1.keys_projection._packed_params._packed_params \t (tensor([[ 0.1019,  0.0182, -0.0289,  ...,  0.0137,  0.0350, -0.0745],\n",
      "        [ 0.0243, -0.0076, -0.0106,  ...,  0.0213, -0.0426,  0.0684],\n",
      "        [ 0.0882, -0.0258,  0.0304,  ..., -0.0517, -0.0547,  0.0760],\n",
      "        ...,\n",
      "        [-0.0228, -0.0578, -0.0091,  ...,  0.0289, -0.0487, -0.0319],\n",
      "        [ 0.0532,  0.0335, -0.0699,  ..., -0.0532,  0.0380,  0.0471],\n",
      "        [-0.0730, -0.0258, -0.0243,  ...,  0.0335, -0.0122, -0.0745]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.001520564896054566,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0136, -0.0529,  0.0368, -0.0257,  0.0147, -0.0493,  0.0347,  0.0636,\n",
      "         0.0218,  0.0084, -0.0482,  0.0049,  0.0694,  0.0163,  0.0188, -0.0062,\n",
      "        -0.0519, -0.0171, -0.0254, -0.0672, -0.0328,  0.0410, -0.0030,  0.0481,\n",
      "        -0.0376,  0.0242,  0.0358,  0.0547,  0.0484,  0.0313,  0.0629,  0.0086,\n",
      "        -0.0250,  0.0025,  0.0246,  0.0103,  0.0288,  0.0617, -0.0308, -0.0370,\n",
      "        -0.0611,  0.0684,  0.0605,  0.0094,  0.0588, -0.0439,  0.0455,  0.0396,\n",
      "         0.0326,  0.0098,  0.0161, -0.0178,  0.0039, -0.0339, -0.0141, -0.0723,\n",
      "         0.0200, -0.0374, -0.0656, -0.0612,  0.0659, -0.0201, -0.0642, -0.0303,\n",
      "         0.0644,  0.0177,  0.0248,  0.0241, -0.0580, -0.0673, -0.0529,  0.0467,\n",
      "        -0.0796,  0.0768,  0.0590,  0.0439,  0.0554, -0.0161, -0.0479,  0.0776,\n",
      "         0.0370, -0.0003,  0.0023,  0.0413,  0.0023,  0.0403, -0.0843, -0.0132,\n",
      "         0.0640,  0.0513, -0.0251, -0.0313, -0.0568, -0.0678, -0.0502,  0.0488,\n",
      "        -0.0196, -0.0258, -0.0478,  0.0027, -0.0122, -0.0788,  0.0208, -0.0244,\n",
      "         0.0265, -0.0276,  0.0221, -0.0475, -0.0102, -0.0555,  0.0405,  0.0450,\n",
      "        -0.0012, -0.0089, -0.0452, -0.0597, -0.0808,  0.0365,  0.0353, -0.0093,\n",
      "         0.0712,  0.0678,  0.0508, -0.0763,  0.0577,  0.0390,  0.0541,  0.0303,\n",
      "        -0.0261, -0.0331,  0.0153,  0.0365,  0.0567,  0.0188, -0.0640, -0.0370,\n",
      "        -0.0631,  0.0413, -0.0154,  0.0278,  0.0388,  0.0480, -0.0088,  0.0635,\n",
      "        -0.0746,  0.0365,  0.0555,  0.0788,  0.0326, -0.0244,  0.0673, -0.0533,\n",
      "         0.0255, -0.0721,  0.0501, -0.0150,  0.0508,  0.0581, -0.0336,  0.0420,\n",
      "        -0.0653, -0.0084, -0.0316,  0.0297,  0.0496,  0.0766, -0.0014, -0.0195,\n",
      "        -0.0560, -0.0149,  0.0408,  0.0232, -0.0135, -0.0192,  0.0376, -0.0325,\n",
      "         0.0543,  0.0107, -0.0678, -0.0577, -0.0647,  0.0266,  0.0326, -0.0666,\n",
      "         0.0546,  0.0314,  0.0182,  0.0436,  0.0061,  0.0292,  0.0634,  0.0107],\n",
      "       requires_grad=True))\n",
      "encoder.2.0.block.1.final_projection.scale \t tensor(1.)\n",
      "encoder.2.0.block.1.final_projection.zero_point \t tensor(0)\n",
      "encoder.2.0.block.1.final_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.2.0.block.1.final_projection._packed_params._packed_params \t (tensor([[-0.0355,  0.0284,  0.0024,  ...,  0.0284,  0.0308, -0.0592],\n",
      "        [ 0.0971,  0.0237,  0.0568,  ...,  0.0568, -0.0166,  0.1137],\n",
      "        [-0.0474,  0.0758,  0.0213,  ...,  0.0426, -0.0047,  0.0450],\n",
      "        ...,\n",
      "        [-0.0592,  0.1090,  0.0450,  ..., -0.1303,  0.0095, -0.0213],\n",
      "        [-0.0497,  0.0332,  0.0545,  ...,  0.0497, -0.0687,  0.0213],\n",
      "        [ 0.0876, -0.0758, -0.0047,  ...,  0.0734, -0.0497,  0.0474]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0023687107022851706,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0630,  0.0040,  0.0037,  0.0760,  0.0086, -0.0367, -0.0239,  0.0466,\n",
      "         0.0009, -0.0672,  0.0540,  0.0512, -0.0157, -0.0589, -0.0204,  0.0609,\n",
      "         0.0284,  0.0352, -0.0409, -0.0471,  0.0191, -0.0088, -0.0459,  0.0696,\n",
      "        -0.0461, -0.0531, -0.0134,  0.0440,  0.0346,  0.0740,  0.0507,  0.0514,\n",
      "         0.0238, -0.0414, -0.0549, -0.0603,  0.0519,  0.0020,  0.0393,  0.0582,\n",
      "        -0.0365, -0.0633,  0.0712, -0.0013, -0.0012,  0.0372, -0.0775, -0.0281,\n",
      "        -0.0251,  0.0250,  0.0271, -0.0368,  0.0424,  0.0792, -0.0728, -0.0266,\n",
      "        -0.0252,  0.0435, -0.0292, -0.0497,  0.0432,  0.0136, -0.0528, -0.0289,\n",
      "         0.0016,  0.0304,  0.0348,  0.0237, -0.0715,  0.0454, -0.0313,  0.0032,\n",
      "         0.0230, -0.0307,  0.0501, -0.0658,  0.0381,  0.0277, -0.0407,  0.0218,\n",
      "         0.0069,  0.0059, -0.0607, -0.0436, -0.0648, -0.0200, -0.0776, -0.0237,\n",
      "         0.0541, -0.0339,  0.0110,  0.0450, -0.0445,  0.0191, -0.0198, -0.0114,\n",
      "        -0.0457,  0.0549, -0.0027, -0.0369,  0.0046,  0.0662,  0.0827,  0.0019,\n",
      "         0.0541,  0.0294,  0.0190, -0.0245,  0.0363,  0.0313, -0.0382,  0.0562,\n",
      "         0.0299, -0.0480,  0.0570,  0.0310, -0.0164,  0.0047,  0.0275,  0.0295,\n",
      "         0.0118, -0.0077, -0.0502,  0.0433,  0.0431, -0.0452, -0.0461, -0.0205,\n",
      "         0.0052,  0.0429,  0.0069,  0.0456,  0.0411, -0.0709, -0.0430,  0.0399,\n",
      "         0.0414,  0.0232, -0.0582,  0.0079, -0.0147, -0.0679, -0.0429,  0.0609,\n",
      "         0.0650, -0.0241,  0.0697, -0.0803,  0.0351,  0.0546,  0.0610, -0.0045,\n",
      "         0.0251,  0.0218,  0.0044, -0.0579, -0.0120,  0.0061,  0.0506,  0.0450,\n",
      "        -0.0709,  0.0791,  0.0352,  0.0861,  0.0207, -0.0520,  0.0444,  0.0137,\n",
      "         0.0632,  0.0374,  0.0199,  0.0541, -0.0656,  0.0015, -0.0803, -0.0543,\n",
      "         0.0433,  0.0077, -0.0356, -0.0134, -0.0470, -0.0428,  0.0165, -0.0577,\n",
      "         0.0035, -0.0601,  0.0232,  0.0247, -0.0016, -0.0389,  0.0447,  0.0266],\n",
      "       requires_grad=True))\n",
      "encoder.2.1.block.0.weight \t tensor([0.9604, 0.9702, 0.9625, 0.9899, 0.9430, 0.9265, 1.0174, 0.9771, 0.9863,\n",
      "        0.9299, 0.9082, 0.9838, 0.9573, 0.9714, 0.9392, 0.9898, 0.9705, 0.9983,\n",
      "        0.9842, 0.9486, 0.9837, 0.9831, 0.9413, 0.9383, 0.9921, 0.9690, 1.0003,\n",
      "        0.9383, 0.9739, 0.9805, 0.9284, 0.9039, 0.9759, 0.9463, 0.9862, 0.9301,\n",
      "        0.9799, 0.9204, 0.9651, 1.0066, 0.9678, 0.9733, 0.9133, 0.9989, 0.8608,\n",
      "        0.9607, 0.9630, 0.9296, 0.9742, 0.9874, 0.9096, 0.9727, 0.9306, 1.0065,\n",
      "        0.9193, 0.9623, 0.9913, 0.9112, 0.9507, 0.9679, 0.9672, 0.9972, 0.9646,\n",
      "        0.9902, 0.9552, 0.9271, 1.0274, 1.0129, 0.9819, 0.9967, 0.9749, 0.9530,\n",
      "        0.9745, 1.0167, 0.9811, 0.9867, 0.9521, 1.0276, 0.9553, 0.9515, 0.9395,\n",
      "        1.0144, 0.9751, 0.9910, 1.0008, 0.9806, 0.9329, 0.9616, 0.9435, 0.9108,\n",
      "        0.9094, 0.9640, 0.9496, 0.9441, 0.9270, 0.9559, 0.9212, 0.9691, 0.9573,\n",
      "        0.9845, 0.9953, 0.9698, 0.9845, 0.9763, 0.9631, 0.9563, 0.9386, 0.9853,\n",
      "        0.9887, 1.0121, 1.0006, 0.9401, 0.9388, 0.9855, 1.0013, 0.9376, 0.9829,\n",
      "        0.9425, 1.0040, 0.9210, 0.9428, 0.9573, 0.9649, 0.9723, 0.8837, 0.9805,\n",
      "        0.9405, 0.9876, 0.9552, 0.8952, 0.9806, 0.9425, 1.0286, 0.9928, 0.9902,\n",
      "        0.9660, 0.9686, 0.9436, 0.9856, 0.9319, 0.9292, 0.9356, 0.9363, 0.9427,\n",
      "        0.9449, 0.9452, 1.0231, 0.9625, 0.9426, 0.9657, 0.9919, 0.9494, 0.9340,\n",
      "        0.9809, 0.9802, 0.9616, 0.9872, 0.9853, 0.9651, 0.9915, 0.9223, 0.9705,\n",
      "        1.0202, 0.9494, 0.9347, 0.9520, 0.9805, 0.9438, 0.9169, 0.9269, 0.9484,\n",
      "        0.9435, 0.9931, 0.9784, 0.9132, 0.9528, 0.9932, 0.9208, 0.9833, 1.0210,\n",
      "        0.9582, 0.9616, 0.9301, 0.9771, 0.9686, 0.9762, 0.9568, 0.9246, 0.9793,\n",
      "        0.9890, 0.9467, 0.9512])\n",
      "encoder.2.1.block.0.bias \t tensor([ 6.5064e-02, -4.2360e-02, -3.3914e-02,  7.6034e-03, -3.8546e-02,\n",
      "        -1.5970e-02, -1.9620e-02, -6.5303e-03, -4.4471e-02,  3.9039e-02,\n",
      "        -2.0819e-02,  6.0189e-02, -5.4702e-02,  5.5413e-05,  4.0514e-02,\n",
      "        -6.4140e-02, -7.0102e-02, -7.3142e-02,  3.4783e-02,  5.9185e-02,\n",
      "        -3.8584e-02,  5.7760e-02, -7.4662e-03, -3.7117e-02,  3.9000e-02,\n",
      "        -4.1652e-02, -7.7203e-02,  1.2661e-03,  2.7334e-02, -5.6674e-02,\n",
      "        -3.3540e-02,  6.5584e-03,  3.9006e-02, -1.3795e-02, -5.4448e-02,\n",
      "         4.7286e-02, -3.2024e-02, -3.5763e-02,  6.1744e-03, -4.1948e-02,\n",
      "        -2.3556e-02, -5.3704e-02, -3.3588e-02,  3.6380e-02, -1.5646e-02,\n",
      "        -1.1528e-03,  5.0696e-02,  7.3024e-03,  5.1372e-02, -3.7693e-02,\n",
      "         8.9251e-02,  1.0023e-02,  6.2679e-02, -4.9637e-02, -2.6189e-02,\n",
      "         2.9024e-02,  3.0482e-02,  8.2182e-03, -2.6155e-02, -6.5898e-02,\n",
      "        -6.1311e-02, -7.1712e-02, -6.7864e-03,  4.7098e-02, -6.4554e-03,\n",
      "        -3.8496e-02, -3.8159e-02,  6.1236e-02, -3.2810e-02, -4.5818e-02,\n",
      "        -4.8381e-02, -3.5158e-02, -3.1957e-02, -3.2896e-02,  3.7845e-02,\n",
      "        -5.5424e-02, -2.0304e-03,  5.8466e-02, -3.2628e-02, -1.9251e-02,\n",
      "        -4.4554e-02, -3.3253e-02,  4.0440e-02,  5.0707e-02, -6.4182e-02,\n",
      "        -5.0139e-02,  2.2951e-02,  1.5118e-02, -6.9297e-02,  4.6552e-02,\n",
      "        -3.2610e-02, -1.1273e-02,  6.2132e-02, -3.7434e-02, -1.7403e-02,\n",
      "         7.0505e-02, -7.3490e-03, -3.7903e-02, -4.9994e-02, -1.1032e-02,\n",
      "        -4.2502e-02, -2.4060e-03, -1.1470e-03, -5.1540e-02,  5.5797e-02,\n",
      "         4.8778e-02,  5.4378e-02,  4.4549e-02,  6.1894e-02, -7.8829e-03,\n",
      "         7.5731e-03, -3.0439e-03, -3.8247e-02, -3.5189e-02, -2.3481e-02,\n",
      "        -5.7438e-02,  3.5470e-02, -2.3416e-02, -3.0580e-02, -2.6936e-02,\n",
      "         5.3203e-03, -1.9079e-02, -1.4839e-02,  6.4990e-02, -7.8995e-05,\n",
      "        -3.3238e-02,  3.4295e-02,  2.3954e-02,  4.6344e-02, -3.8128e-02,\n",
      "         6.1788e-02,  2.6278e-02, -2.4347e-02, -5.8374e-02, -4.1738e-02,\n",
      "         3.7929e-02, -3.0199e-02,  2.3979e-02, -4.9691e-02, -2.2863e-02,\n",
      "        -2.7803e-02,  2.9465e-02, -6.2360e-02, -1.8745e-03, -1.2799e-02,\n",
      "         7.1732e-02,  2.1108e-02,  3.7094e-02, -1.6215e-02,  3.5928e-02,\n",
      "        -3.6588e-02, -1.9868e-02, -6.8886e-03, -4.2518e-02,  4.7942e-02,\n",
      "         2.5029e-03,  4.4559e-02, -4.5821e-02, -5.5000e-02, -6.0584e-02,\n",
      "         2.9116e-02, -3.5638e-02, -7.9312e-02, -9.0149e-02, -2.0068e-02,\n",
      "        -9.8251e-03, -5.0091e-02,  6.5205e-02, -8.5833e-02,  2.2896e-02,\n",
      "         2.1697e-02, -1.8905e-02, -2.9528e-02, -8.7866e-03,  2.6421e-02,\n",
      "         2.2958e-02,  8.4655e-02,  2.2694e-02, -6.0124e-02,  4.6190e-02,\n",
      "         6.1033e-03,  5.2477e-02, -1.4667e-02,  5.0129e-02, -1.6060e-02,\n",
      "        -5.3808e-02, -4.0469e-02, -2.7371e-02,  4.5355e-02,  5.5676e-02,\n",
      "         3.9939e-02,  4.6023e-02])\n",
      "encoder.2.1.block.1.0.scale \t tensor(1.)\n",
      "encoder.2.1.block.1.0.zero_point \t tensor(0)\n",
      "encoder.2.1.block.1.0._packed_params.dtype \t torch.qint8\n",
      "encoder.2.1.block.1.0._packed_params._packed_params \t (tensor([[-0.0229, -0.0491, -0.0458,  ...,  0.0164, -0.0900, -0.0638],\n",
      "        [ 0.0393,  0.0573, -0.0180,  ..., -0.0458, -0.0589, -0.0393],\n",
      "        [-0.0295,  0.0278, -0.0720,  ..., -0.0049, -0.0098, -0.0753],\n",
      "        ...,\n",
      "        [-0.0638, -0.0327,  0.0671,  ..., -0.0573, -0.0246,  0.0327],\n",
      "        [ 0.0458, -0.0360,  0.0295,  ...,  0.0098, -0.0687,  0.0589],\n",
      "        [-0.0278,  0.0622,  0.0540,  ...,  0.0098,  0.0000, -0.0393]],\n",
      "       size=(768, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0016368464566767216,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-6.2825e-02, -5.9925e-02,  1.9689e-02, -1.0500e-01, -1.2624e-02,\n",
      "        -4.6200e-04, -6.0403e-02,  4.3060e-02, -6.0316e-02, -1.1787e-02,\n",
      "        -1.2164e-02, -8.8890e-03,  1.6618e-03, -3.5662e-02, -7.2256e-02,\n",
      "        -6.7720e-02, -5.9831e-02,  1.0516e-04, -1.6534e-04,  6.9006e-04,\n",
      "        -4.8849e-02, -2.1132e-02, -9.7015e-02, -3.1489e-03,  8.0167e-03,\n",
      "        -7.5050e-02, -5.2075e-02, -6.4045e-02, -1.6519e-02, -1.0034e-01,\n",
      "        -1.9744e-02,  1.4580e-02, -9.6054e-02, -7.9813e-02, -1.5511e-02,\n",
      "        -5.1413e-02,  3.5885e-02, -3.9163e-02, -1.8044e-02, -4.7575e-02,\n",
      "        -6.0403e-02, -5.1688e-02, -1.2840e-02, -5.0485e-02, -5.0426e-02,\n",
      "        -5.3754e-02,  1.5582e-02, -8.2961e-02, -3.9504e-02, -5.3855e-02,\n",
      "        -3.2479e-02, -5.0422e-02, -5.4641e-02, -6.5602e-03, -9.4862e-03,\n",
      "        -5.4123e-02, -3.7985e-02,  1.9307e-02,  1.9761e-03, -7.8746e-02,\n",
      "        -4.0072e-04, -9.4103e-02, -5.1214e-02, -5.2107e-03,  4.3129e-02,\n",
      "        -6.1322e-02, -7.6879e-02, -4.3715e-03, -1.8587e-02, -1.1619e-02,\n",
      "        -2.2741e-02, -2.7308e-04, -7.4833e-02, -3.7731e-02, -7.1029e-02,\n",
      "        -3.4070e-02, -5.7208e-02, -5.8213e-02,  2.4690e-02, -4.6770e-02,\n",
      "        -7.3090e-02, -5.4261e-02, -1.5676e-02,  1.1495e-02, -6.8473e-02,\n",
      "        -6.4094e-02, -8.7307e-02, -5.9911e-02, -3.5490e-02, -3.6739e-03,\n",
      "        -2.7189e-02, -1.1225e-01, -1.1838e-02, -2.4918e-02,  6.4507e-03,\n",
      "        -3.8314e-02, -3.9589e-02, -9.5506e-03, -1.8436e-02,  1.5015e-02,\n",
      "         3.4851e-02, -5.9090e-03, -2.1473e-02, -9.3484e-02, -7.7877e-02,\n",
      "        -4.1228e-02, -1.3902e-02,  1.9434e-02, -9.2169e-02, -2.1916e-03,\n",
      "        -8.8754e-02, -7.6131e-02, -1.2583e-02, -3.5828e-02, -5.3109e-02,\n",
      "        -1.0296e-01, -9.6197e-02,  7.6389e-03,  5.6291e-05, -1.1361e-03,\n",
      "         2.9365e-02, -6.8705e-02, -9.8236e-02, -7.9652e-02, -7.1043e-02,\n",
      "         2.0137e-02,  1.0857e-02, -9.0371e-02, -9.2813e-02, -2.2223e-02,\n",
      "        -4.4116e-02, -1.0564e-02, -1.3713e-02, -2.0586e-02, -2.0304e-02,\n",
      "        -5.5083e-02, -3.5181e-02, -5.6896e-02, -4.7451e-04, -2.8464e-02,\n",
      "        -6.2986e-02,  2.9243e-02, -5.0625e-02, -2.5361e-02, -2.5950e-02,\n",
      "        -3.5454e-02,  5.8305e-03,  1.6140e-02, -1.8010e-02, -2.5505e-02,\n",
      "        -4.5037e-02, -7.0028e-02, -8.8873e-03,  4.4776e-03, -8.4056e-02,\n",
      "        -9.8921e-02, -9.5272e-02,  1.4174e-02,  6.0814e-03, -5.9052e-02,\n",
      "        -4.6415e-02, -5.0804e-02,  1.9727e-03, -7.1533e-03, -6.5197e-02,\n",
      "        -8.8603e-02, -9.0326e-02, -2.0231e-02, -7.1921e-02, -7.9490e-02,\n",
      "        -3.1049e-02,  1.5224e-02, -4.0253e-02, -2.6942e-02,  1.5572e-02,\n",
      "        -6.9262e-02,  1.1639e-02,  4.1420e-03,  9.0298e-03, -2.2344e-03,\n",
      "        -1.0722e-01,  3.0865e-03, -1.0992e-02, -6.8640e-02, -8.1689e-02,\n",
      "        -1.1236e-01,  3.4494e-02, -1.2312e-02, -6.6043e-02, -7.6974e-03,\n",
      "         3.7515e-02, -2.5202e-03, -7.2661e-02, -7.8695e-02,  1.6326e-02,\n",
      "        -2.7705e-03,  2.0764e-02, -1.0687e-01, -1.8905e-03, -9.5563e-02,\n",
      "         3.6841e-03, -3.7069e-02, -5.5153e-03, -6.0769e-02, -2.0950e-02,\n",
      "        -6.0330e-02, -2.1936e-02, -1.7735e-02, -1.1540e-01,  4.4541e-02,\n",
      "         2.7743e-02, -7.6732e-04, -5.5392e-02, -7.4367e-03, -4.6239e-02,\n",
      "        -5.0124e-02,  3.1346e-02, -2.4499e-03, -1.7387e-02, -3.2708e-02,\n",
      "        -1.6484e-02, -6.4176e-02, -1.3629e-02, -4.8212e-02,  1.0296e-04,\n",
      "        -3.1171e-02, -2.2158e-02, -4.3269e-03,  1.3180e-02, -1.6560e-02,\n",
      "         2.0188e-02, -3.8777e-02, -1.6199e-02, -3.2312e-02,  2.5805e-02,\n",
      "        -5.7137e-02, -2.0137e-02, -3.0557e-02, -6.6463e-02,  3.1983e-02,\n",
      "        -2.2998e-02,  1.7146e-02, -9.4896e-02, -1.1201e-01,  2.3858e-02,\n",
      "        -2.1190e-02, -1.6471e-02, -8.0851e-02,  5.4213e-03, -1.3440e-03,\n",
      "         9.3585e-03, -8.7035e-02, -1.0217e-02, -2.6127e-02, -5.7664e-02,\n",
      "        -3.4274e-02,  1.6571e-02,  4.0440e-03, -4.1139e-02,  1.8818e-03,\n",
      "        -2.1799e-02,  4.0418e-02,  2.6436e-03, -9.2305e-02, -9.5224e-02,\n",
      "        -4.7590e-02, -5.5037e-03, -7.1984e-03, -1.0678e-01, -4.6851e-02,\n",
      "        -2.6137e-02, -2.2595e-03, -8.0420e-03, -4.4855e-02, -8.9241e-02,\n",
      "        -6.2174e-02, -8.9936e-02, -6.1660e-02, -5.0714e-02,  3.3178e-03,\n",
      "        -7.2544e-02,  1.9639e-02, -7.9092e-02, -9.2734e-02,  1.0351e-02,\n",
      "        -6.9643e-02, -3.4151e-02, -4.4597e-02, -6.1513e-04,  4.1661e-02,\n",
      "        -2.6298e-02, -1.0017e-01, -3.2174e-02, -2.8658e-02, -1.1477e-01,\n",
      "         1.5543e-03,  1.2301e-02,  2.1011e-02, -6.2491e-02, -5.5843e-02,\n",
      "        -8.6496e-02,  3.2853e-03,  1.1083e-02, -4.3909e-03, -5.6554e-02,\n",
      "         2.1305e-02,  2.0746e-02,  3.3695e-03, -6.1452e-02, -5.0776e-02,\n",
      "        -4.7318e-02, -3.6800e-02,  1.4742e-02, -1.0251e-01,  7.2439e-03,\n",
      "         1.3907e-03, -2.9886e-02, -8.2529e-02, -1.1928e-02, -1.0092e-01,\n",
      "        -1.0533e-02, -9.5069e-02, -3.3560e-02, -8.7856e-02, -7.8063e-02,\n",
      "        -5.2378e-03, -8.1579e-02, -7.9801e-02,  9.7008e-03, -5.3339e-02,\n",
      "        -8.9517e-02, -7.7386e-02, -2.4451e-02,  3.4241e-02, -1.2690e-03,\n",
      "         4.0956e-02,  3.4203e-03, -9.8111e-02, -5.2920e-02,  9.9523e-03,\n",
      "         3.5432e-02,  4.3526e-02, -9.8196e-02, -5.1930e-02, -4.6710e-02,\n",
      "        -1.1480e-02, -7.6065e-02, -4.6784e-02, -9.6453e-02, -2.3366e-02,\n",
      "        -3.9636e-02, -2.8950e-02, -7.4791e-02, -4.8498e-02, -2.8773e-02,\n",
      "        -2.0264e-02, -1.0134e-01, -9.4735e-02,  1.8693e-03, -9.1980e-02,\n",
      "        -1.4861e-02, -2.2730e-02,  4.6826e-03, -8.3180e-02, -6.7549e-02,\n",
      "        -3.3244e-02, -5.7670e-02, -5.4141e-03, -2.0881e-02, -2.6163e-03,\n",
      "         2.5686e-02,  4.4869e-03, -1.9351e-02, -1.6964e-02, -9.4010e-02,\n",
      "        -5.2025e-03, -5.6243e-02, -9.4005e-02,  1.9325e-02, -8.4582e-02,\n",
      "        -2.1012e-02, -2.1088e-02, -6.6500e-02,  3.8251e-02,  2.7949e-02,\n",
      "         1.3616e-02, -2.2105e-02, -7.0978e-02,  3.1185e-02,  1.6650e-02,\n",
      "         1.3369e-02, -9.4901e-02, -4.1913e-02, -8.1575e-02, -8.3472e-03,\n",
      "        -3.1526e-02,  1.4368e-02, -3.3094e-02,  2.8762e-02,  1.6968e-02,\n",
      "        -9.0383e-02, -8.5282e-03,  9.1166e-03, -4.5345e-02,  1.8647e-02,\n",
      "         2.8758e-02, -8.0407e-02, -5.9288e-02, -7.9068e-02, -2.0545e-02,\n",
      "         1.7127e-04, -4.6787e-02,  2.0879e-02, -1.8992e-02, -9.4204e-02,\n",
      "        -6.8703e-02, -4.9041e-02, -5.2482e-02, -5.8339e-02, -4.1749e-02,\n",
      "         1.1890e-02,  1.0353e-02, -5.0819e-02, -7.3530e-02, -8.8908e-02,\n",
      "        -8.9334e-02,  1.6425e-02, -3.7677e-02, -2.3369e-02, -8.1217e-02,\n",
      "        -9.5448e-02, -7.2363e-02,  1.4757e-02, -1.4471e-02,  2.4515e-02,\n",
      "        -3.1313e-02,  3.7496e-02, -2.6144e-03,  8.3677e-04, -7.5205e-02,\n",
      "        -3.9007e-02,  1.2713e-02, -6.1551e-02,  6.9904e-03, -2.9246e-03,\n",
      "         9.3714e-03, -8.3196e-02, -6.4955e-02, -4.9139e-02, -3.1840e-02,\n",
      "        -2.1862e-02,  1.6531e-02, -6.2276e-03, -8.5279e-02, -2.2457e-02,\n",
      "        -9.0360e-02, -8.6761e-02, -1.0425e-01, -1.0278e-01,  1.8212e-02,\n",
      "        -9.5732e-02, -8.6527e-02,  1.3754e-02, -4.7328e-02,  8.8141e-03,\n",
      "        -7.5525e-02, -5.0557e-02,  6.6861e-03, -5.6322e-02,  1.8771e-02,\n",
      "         9.1603e-03, -8.1352e-02, -3.2598e-02,  3.6195e-02,  1.5765e-02,\n",
      "        -7.0108e-03, -7.2404e-02, -1.1497e-02, -7.2817e-02, -1.1946e-02,\n",
      "        -5.1923e-03, -1.0919e-02, -6.8661e-02, -1.0259e-02,  1.0043e-02,\n",
      "        -2.0018e-02,  2.1078e-02, -6.1074e-02,  3.4750e-02, -5.1055e-02,\n",
      "        -1.0279e-01, -2.0405e-02,  5.5936e-03, -7.2191e-02, -8.2432e-02,\n",
      "        -1.0333e-01, -5.9830e-02, -1.1741e-02, -6.2885e-04,  1.2781e-02,\n",
      "        -3.1420e-03, -2.7209e-03, -3.2933e-02,  2.3147e-02, -3.0728e-02,\n",
      "        -8.9733e-02, -5.3949e-02, -3.6737e-02, -2.0360e-02, -2.2143e-02,\n",
      "        -6.2328e-02, -2.3810e-02, -9.1121e-02, -4.9425e-02, -8.9072e-02,\n",
      "        -7.5524e-03, -4.4672e-02, -1.0353e-01, -8.1059e-02,  3.7141e-02,\n",
      "         6.0836e-03, -5.0727e-02, -8.5370e-02, -2.9111e-02, -6.4260e-02,\n",
      "        -1.2768e-02, -4.7028e-02,  1.1098e-02,  1.0640e-02, -3.5751e-04,\n",
      "        -1.3744e-03, -2.3780e-02, -9.4255e-02, -1.0797e-01, -6.0007e-02,\n",
      "        -1.6748e-03, -9.2900e-02, -7.3527e-02, -9.3293e-02, -5.3835e-02,\n",
      "         4.0053e-02, -4.9636e-02, -1.3589e-03, -9.8251e-02, -8.5069e-02,\n",
      "        -1.2862e-01, -4.0287e-02, -6.4913e-02, -7.6112e-02, -4.5021e-03,\n",
      "        -1.8141e-02, -9.0785e-02, -6.7719e-03, -8.4915e-02, -4.0020e-02,\n",
      "        -9.6746e-03, -1.1889e-02, -7.1309e-03, -3.4110e-02, -6.5799e-02,\n",
      "        -7.4375e-03, -8.7582e-02, -3.6930e-02,  8.0791e-03,  2.8562e-02,\n",
      "         3.8928e-03, -8.1794e-02, -1.0181e-02, -6.2707e-03, -1.4808e-02,\n",
      "        -5.6316e-02,  1.6669e-02,  4.2177e-02,  1.2637e-02, -8.4925e-02,\n",
      "        -6.2359e-02,  6.9673e-03, -8.7173e-02,  3.5685e-02, -7.4361e-02,\n",
      "        -2.7928e-02, -8.0826e-02, -9.5858e-02,  6.3934e-03, -4.8087e-02,\n",
      "        -4.5292e-02, -5.0708e-02, -1.9293e-02, -1.1107e-01, -9.8397e-03,\n",
      "         1.4028e-02, -2.4430e-02,  1.9622e-02, -6.3337e-02, -1.0807e-01,\n",
      "        -1.9839e-02, -1.0578e-01, -4.2978e-02, -1.9559e-02, -1.7847e-03,\n",
      "        -9.2881e-02,  1.7043e-02, -6.3358e-02, -6.9912e-02, -1.1856e-01,\n",
      "        -1.7057e-02, -5.4020e-02, -3.2552e-03, -1.7955e-02, -7.7893e-02,\n",
      "        -4.3621e-03, -1.8432e-02,  2.4195e-02, -7.3187e-02, -4.0473e-02,\n",
      "        -6.3496e-03, -3.9492e-03, -7.6702e-02,  1.2021e-02, -6.2026e-02,\n",
      "         1.5652e-02,  9.0797e-03,  3.3137e-02, -8.1672e-02, -5.7117e-02,\n",
      "         2.4732e-02, -5.9668e-02, -8.0016e-02, -8.4890e-02,  2.2408e-02,\n",
      "        -4.4575e-02,  3.1215e-02, -3.1240e-02, -1.6516e-02, -2.2390e-02,\n",
      "        -6.0180e-02,  2.4719e-02, -1.0407e-01,  2.3522e-02, -3.4266e-02,\n",
      "        -2.7226e-02,  8.7234e-03,  3.5024e-02, -4.8315e-02, -5.8108e-02,\n",
      "         1.2335e-02, -1.8068e-02, -7.4040e-02, -7.4793e-02,  1.3554e-02,\n",
      "         2.9603e-02, -2.5704e-02,  1.3551e-02,  7.5116e-03,  4.0533e-02,\n",
      "        -9.4732e-02, -5.1760e-03, -7.9522e-03, -6.9637e-02, -2.0136e-02,\n",
      "        -1.0274e-01, -5.9871e-02, -1.7819e-04, -9.9077e-02, -9.3987e-02,\n",
      "        -1.1878e-02, -2.0909e-02, -7.2264e-02, -6.8333e-02, -1.9597e-02,\n",
      "        -7.3894e-02, -6.3600e-02, -1.4603e-02,  2.1491e-02,  1.9091e-02,\n",
      "        -7.8177e-02, -9.5765e-02, -6.8314e-02, -6.8076e-02, -9.3592e-02,\n",
      "        -3.1090e-02, -6.1442e-02, -8.3919e-02, -3.9596e-02,  8.2747e-03,\n",
      "         2.4785e-02, -1.2931e-02, -2.7780e-02, -6.9410e-03,  1.6213e-02,\n",
      "        -4.4674e-02, -5.3168e-02,  1.2015e-02, -1.5813e-02, -1.0346e-01,\n",
      "        -1.8457e-02, -5.1871e-02, -3.3592e-02, -3.7241e-02,  2.4677e-02,\n",
      "        -3.4543e-02, -1.7719e-02, -5.7534e-02, -3.0769e-02, -3.2744e-02,\n",
      "        -3.0560e-02, -5.8522e-02, -6.1040e-02,  1.4603e-02, -1.9459e-02,\n",
      "        -5.9728e-02, -7.2351e-02, -5.7231e-02, -2.3530e-02,  1.8453e-02,\n",
      "         1.9623e-03, -4.6914e-02, -3.3175e-02, -8.8304e-02,  3.0096e-02,\n",
      "         3.1331e-03, -8.2430e-02,  2.1705e-02, -7.2449e-02, -9.8618e-02,\n",
      "         3.5610e-02, -6.9230e-02, -1.1074e-01,  2.0970e-02, -9.1771e-02,\n",
      "        -7.8779e-02,  4.2274e-03, -3.4298e-02, -7.7311e-02, -9.4822e-02,\n",
      "        -3.4031e-02, -6.6181e-02, -5.7138e-03,  4.6508e-02, -4.0292e-02,\n",
      "        -9.0512e-02, -7.6827e-02, -3.8805e-02,  2.3373e-02, -1.8574e-03,\n",
      "        -8.7478e-02, -2.2341e-02,  1.1120e-02, -1.9421e-02, -2.5370e-02,\n",
      "         9.5575e-03, -4.2593e-02, -5.3775e-02,  2.1899e-02, -4.4056e-02,\n",
      "         2.8894e-02,  9.3918e-03, -4.5204e-02, -7.1903e-02, -2.4300e-03,\n",
      "        -1.0341e-01, -8.6810e-02, -2.5628e-02, -2.5381e-02,  3.1013e-02,\n",
      "         2.6985e-02, -1.6544e-02, -8.4687e-03], requires_grad=True))\n",
      "encoder.2.1.block.1.2.scale \t tensor(1.)\n",
      "encoder.2.1.block.1.2.zero_point \t tensor(0)\n",
      "encoder.2.1.block.1.2._packed_params.dtype \t torch.qint8\n",
      "encoder.2.1.block.1.2._packed_params._packed_params \t (tensor([[-0.0196,  0.0141,  0.0685,  ...,  0.0087,  0.0435,  0.0120],\n",
      "        [-0.0457, -0.0598,  0.0130,  ...,  0.0272,  0.0478, -0.0402],\n",
      "        [ 0.0152, -0.0098, -0.0380,  ...,  0.0261,  0.0239,  0.0022],\n",
      "        ...,\n",
      "        [ 0.0174,  0.0152, -0.0076,  ..., -0.0033, -0.0544,  0.0163],\n",
      "        [ 0.0163,  0.0380,  0.0315,  ..., -0.0272, -0.0370, -0.0370],\n",
      "        [ 0.0239,  0.0043, -0.0109,  ...,  0.0098,  0.0228, -0.0359]],\n",
      "       size=(192, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.001087119453586638,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-4.1336e-02,  4.5801e-03,  2.2126e-02,  2.3822e-02, -2.6815e-02,\n",
      "         2.7287e-02,  1.5227e-02, -1.6175e-02,  7.2780e-04,  3.4957e-02,\n",
      "        -2.7929e-02, -4.1313e-02,  2.3468e-02,  1.5889e-02, -6.5835e-02,\n",
      "         3.0434e-02,  2.6640e-02, -6.9037e-03, -1.6055e-02, -2.9718e-02,\n",
      "         5.6194e-03,  9.1998e-03,  2.7723e-02,  2.6039e-02, -2.2929e-02,\n",
      "         2.8780e-02, -2.3998e-03, -1.4049e-02, -3.6893e-03, -1.3857e-02,\n",
      "         7.7030e-03, -2.7831e-02, -2.0406e-02, -3.2954e-02, -2.5833e-03,\n",
      "        -2.2123e-02, -2.5976e-02,  1.1305e-02, -5.1825e-02,  1.0536e-02,\n",
      "         2.0024e-02,  5.2821e-03,  4.5590e-02,  1.6092e-02,  1.7242e-02,\n",
      "        -3.5606e-02, -4.8611e-02, -1.4111e-02,  3.5987e-03, -1.3671e-02,\n",
      "         2.7544e-02, -1.8263e-02,  2.0302e-02, -6.7009e-03, -2.6875e-02,\n",
      "        -2.4101e-02, -4.6095e-02, -4.1305e-02,  1.4058e-02, -1.3969e-02,\n",
      "        -2.4202e-02, -1.3085e-02, -6.3109e-03, -1.4449e-02,  3.0289e-02,\n",
      "        -7.5053e-04,  2.9985e-02,  9.7298e-03, -2.1443e-02,  4.3011e-03,\n",
      "        -2.3308e-02,  1.3980e-02,  8.3623e-03, -2.0629e-02,  2.2714e-02,\n",
      "        -1.7243e-03, -1.6768e-02, -1.7893e-02, -1.8564e-02,  3.4308e-02,\n",
      "         1.0617e-02,  8.7901e-03, -2.5478e-02, -3.1424e-02, -2.0903e-02,\n",
      "         1.1776e-02, -6.1900e-02,  1.5049e-03,  5.6944e-02, -5.6714e-02,\n",
      "         2.5937e-02, -5.6062e-03, -7.7449e-03,  2.0190e-02, -1.8714e-02,\n",
      "         1.0769e-02, -6.6137e-02,  1.9843e-02,  2.4148e-02, -8.8280e-03,\n",
      "        -3.1424e-02, -1.6539e-02,  3.6630e-02,  1.3717e-02, -9.5428e-03,\n",
      "         4.8574e-04, -2.1357e-02, -5.4897e-02, -2.0686e-02, -2.2666e-02,\n",
      "        -1.9598e-02, -7.1080e-03, -1.4985e-02, -8.9427e-03, -1.0009e-02,\n",
      "        -3.9445e-03,  8.1100e-03,  2.4291e-02, -1.1446e-02,  6.4249e-03,\n",
      "        -3.8435e-02, -3.8145e-02,  1.2886e-02,  2.4926e-02, -2.3288e-02,\n",
      "        -3.3482e-03, -2.6558e-02, -2.2742e-02, -3.3689e-02, -4.9741e-02,\n",
      "        -5.8191e-02, -6.3206e-03, -1.7045e-02, -1.3661e-02,  2.8246e-02,\n",
      "        -3.2521e-02, -1.6637e-02, -2.2820e-02,  1.5036e-02, -3.8246e-03,\n",
      "        -7.7782e-05, -1.0405e-02,  8.3125e-03, -2.1578e-02,  1.4777e-02,\n",
      "        -4.3269e-02,  1.9868e-02, -7.4834e-03,  1.0887e-02,  9.2601e-03,\n",
      "        -1.2346e-02, -4.3302e-03, -2.5231e-02,  2.4491e-02, -2.0670e-03,\n",
      "         3.0971e-02,  2.1516e-02,  1.9835e-02, -3.1372e-02, -7.6633e-03,\n",
      "         4.3476e-03,  2.5469e-02,  3.2322e-02,  2.9501e-03, -1.8306e-02,\n",
      "        -1.7891e-02,  2.1939e-02, -5.1756e-02,  1.7926e-02,  1.5656e-02,\n",
      "        -2.7191e-02, -9.5799e-03,  2.2232e-02, -3.4753e-02, -4.2954e-02,\n",
      "        -3.7147e-02, -4.3021e-02, -3.4651e-03, -1.0416e-02, -1.5006e-02,\n",
      "        -2.0783e-02,  6.8024e-03,  2.6789e-02, -2.5690e-02, -1.0627e-02,\n",
      "         3.7415e-02,  4.5537e-03, -1.3005e-02,  1.3236e-02,  1.9604e-03,\n",
      "         9.2773e-03, -5.9155e-02], requires_grad=True))\n",
      "encoder.3.0.block.0.weight \t tensor([1.0814, 1.0765, 1.0622, 1.0329, 1.1102, 1.0746, 1.0925, 1.0411, 1.1040,\n",
      "        1.0981, 1.0950, 1.0356, 1.0815, 1.1120, 1.0693, 1.0973, 1.1254, 1.0787,\n",
      "        1.0592, 1.0606, 1.0992, 1.0615, 0.9902, 1.0858, 1.0415, 1.1419, 1.0079,\n",
      "        1.0934, 1.1100, 1.0668, 1.0658, 1.1220, 1.0822, 1.0712, 1.0245, 1.0677,\n",
      "        1.0573, 1.0459, 1.0717, 1.0428, 1.0451, 1.0981, 1.0555, 1.0663, 1.0625,\n",
      "        1.1187, 1.1033, 1.0720, 1.0314, 1.0786, 1.0697, 1.0485, 1.0711, 1.0635,\n",
      "        1.0471, 1.0872, 1.0857, 1.0699, 1.0979, 1.0730, 1.0468, 1.0317, 1.0492,\n",
      "        1.0516, 1.0897, 1.0804, 1.0888, 1.0284, 1.0744, 1.1144, 1.0221, 1.0429,\n",
      "        1.0943, 1.0632, 1.0699, 1.0654, 1.0951, 0.9977, 1.0544, 1.0982, 1.0271,\n",
      "        1.1078, 1.0240, 1.0728, 1.0496, 1.0858, 1.1356, 1.0749, 1.0588, 1.0832,\n",
      "        1.0828, 1.0805, 1.1403, 1.0709, 1.1037, 1.0706, 1.0642, 1.0828, 1.0812,\n",
      "        1.0569, 1.0600, 1.0737, 1.0908, 1.0724, 1.0601, 1.0410, 1.1175, 1.0310,\n",
      "        1.0576, 1.0817, 1.0750, 1.0618, 1.0438, 1.0588, 1.0755, 1.0793, 1.0518,\n",
      "        1.0599, 1.0509, 1.0689, 1.0683, 1.0697, 1.1012, 1.0485, 1.1046, 1.0697,\n",
      "        1.0422, 1.1182, 1.0877, 1.0309, 1.0545, 1.0816, 1.1037, 1.0711, 1.1021,\n",
      "        1.1056, 1.0422, 1.0668, 1.0743, 1.0884, 1.0687, 1.1000, 1.0329, 1.0914,\n",
      "        1.0499, 1.0940, 1.1198, 1.0762, 1.0780, 1.0616, 1.0400, 1.0842, 1.0716,\n",
      "        1.0918, 1.0975, 1.0989, 1.0515, 1.0576, 1.0828, 1.0621, 1.0996, 1.0448,\n",
      "        1.0778, 1.0919, 1.0676, 1.0589, 1.0729, 1.0909, 1.0650, 1.0993, 1.0963,\n",
      "        1.0632, 1.0514, 1.0683, 1.0946, 1.1234, 1.0735, 1.1094, 1.0514, 1.0855,\n",
      "        1.0378, 1.0669, 1.0823, 1.0403, 1.0336, 1.0844, 1.0274, 1.0631, 1.0846,\n",
      "        1.0818, 1.1047, 1.0734])\n",
      "encoder.3.0.block.0.bias \t tensor([-1.0134e-02, -2.5528e-02, -4.4422e-05,  1.1421e-02, -4.3505e-03,\n",
      "         8.3018e-03,  1.1776e-02, -2.4206e-02,  1.1184e-02, -4.4472e-03,\n",
      "        -7.8210e-03, -6.7566e-03,  2.8519e-03, -6.2728e-03, -2.5072e-02,\n",
      "         1.0841e-02, -1.6337e-02, -6.2578e-03,  9.7308e-04,  2.8110e-04,\n",
      "         5.3742e-02, -1.5228e-02,  3.3288e-02, -1.7523e-02,  1.3413e-02,\n",
      "         3.2621e-03,  1.9949e-02,  1.8062e-02, -2.8281e-03, -1.8056e-03,\n",
      "         1.5385e-03, -9.0413e-03, -3.6256e-02, -1.2111e-02,  7.1971e-03,\n",
      "         5.1344e-03,  1.4347e-02,  1.1417e-03,  1.1738e-02,  2.6959e-02,\n",
      "         2.9889e-04,  1.0288e-02,  1.6434e-02, -7.6264e-03, -2.3411e-02,\n",
      "         2.1314e-02,  2.0690e-03, -1.1223e-02, -1.8921e-02,  1.5514e-02,\n",
      "         1.5659e-02,  2.0060e-02, -1.2657e-02,  8.0964e-03,  1.5373e-02,\n",
      "         1.3688e-02,  1.7458e-03,  5.0623e-04, -1.4722e-02,  1.8194e-02,\n",
      "         1.0136e-02,  2.2101e-02,  9.5553e-05, -8.6899e-04,  5.1772e-03,\n",
      "         2.9227e-02, -9.9683e-03, -1.9101e-02, -2.7740e-03,  4.4398e-03,\n",
      "        -1.5628e-04, -8.2828e-03,  3.4074e-03,  1.1169e-02,  1.3783e-02,\n",
      "         5.8001e-03, -1.9167e-02, -3.6250e-02,  1.7682e-02, -6.9122e-03,\n",
      "         1.6996e-03,  1.4231e-02, -3.5898e-02,  9.2764e-04,  1.4663e-03,\n",
      "         1.3185e-02, -7.5674e-03,  1.1094e-02,  1.4695e-04, -7.0876e-03,\n",
      "         2.0398e-03, -4.5756e-03,  8.2286e-03,  4.5048e-03,  2.4132e-02,\n",
      "        -2.0049e-02, -3.9077e-03,  1.3582e-02,  1.2941e-02,  4.3835e-03,\n",
      "        -1.1672e-03,  3.7391e-04,  1.6218e-02,  1.2284e-02, -7.6780e-03,\n",
      "         4.5622e-03, -2.1223e-03, -1.3082e-02, -1.4154e-02,  1.3219e-03,\n",
      "        -3.0446e-03,  9.1153e-03,  8.7442e-03,  1.2832e-02,  1.9866e-02,\n",
      "        -2.8401e-03, -1.3944e-02, -6.4145e-03,  2.6456e-02, -1.0234e-02,\n",
      "         6.4360e-04, -9.5210e-03, -1.2136e-02, -1.4196e-02, -6.3746e-03,\n",
      "         9.8575e-03, -1.7399e-02,  7.2580e-03,  6.3795e-04, -1.0779e-02,\n",
      "        -2.5968e-02, -1.7997e-02, -1.4348e-02,  1.5219e-02,  1.5313e-03,\n",
      "         1.1217e-02, -9.1681e-03,  1.2280e-03,  1.5434e-03,  1.7528e-02,\n",
      "        -7.6899e-03,  2.9323e-03,  5.3500e-03,  1.0023e-02,  4.5727e-03,\n",
      "        -1.4378e-03, -6.9523e-03,  1.1435e-02,  1.8095e-02, -1.3672e-02,\n",
      "        -7.9346e-03,  2.5498e-03,  6.5080e-04,  6.5759e-05, -4.0068e-03,\n",
      "        -5.7062e-03, -2.0910e-02, -1.2364e-03,  7.8252e-04, -1.1298e-02,\n",
      "        -1.5854e-02,  2.6461e-02,  1.6140e-02,  1.7532e-02,  4.5712e-03,\n",
      "        -5.9016e-03, -1.5556e-02, -7.0299e-03,  1.3481e-02, -4.3510e-02,\n",
      "         1.4426e-02,  1.4176e-02,  2.5755e-02,  2.5410e-02,  4.2517e-03,\n",
      "        -1.6566e-02, -9.7021e-03,  1.8497e-02,  7.8582e-03,  2.0400e-02,\n",
      "        -7.4422e-03, -2.8774e-02, -3.1756e-02, -3.3795e-02,  1.2743e-02,\n",
      "         1.3058e-02,  3.4846e-02,  1.6342e-02,  1.8074e-02,  7.4222e-03,\n",
      "        -1.2218e-03, -6.3475e-03])\n",
      "encoder.3.0.block.1.queries_projection.scale \t tensor(1.)\n",
      "encoder.3.0.block.1.queries_projection.zero_point \t tensor(0)\n",
      "encoder.3.0.block.1.queries_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.3.0.block.1.queries_projection._packed_params._packed_params \t (tensor([[-0.0093,  0.0371,  0.0000,  ..., -0.0056,  0.0056, -0.0019],\n",
      "        [ 0.0871, -0.0093,  0.0185,  ..., -0.0297, -0.0222,  0.0853],\n",
      "        [ 0.0519,  0.0593, -0.0260,  ...,  0.0742, -0.0797,  0.0667],\n",
      "        ...,\n",
      "        [ 0.0371,  0.0482, -0.0260,  ..., -0.0983, -0.0074, -0.0853],\n",
      "        [ 0.0538, -0.0705, -0.0111,  ..., -0.0742, -0.0074, -0.0946],\n",
      "        [ 0.0667,  0.0556, -0.0241,  ...,  0.0612, -0.0111, -0.0111]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0018541408935561776,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0359,  0.0520,  0.0130,  0.0192, -0.0129, -0.0217,  0.0028, -0.0354,\n",
      "        -0.0802, -0.0535,  0.0499,  0.0394,  0.0455,  0.0489, -0.0126,  0.0351,\n",
      "         0.0091,  0.0399,  0.0232,  0.0172,  0.0468, -0.0543, -0.1107,  0.0295,\n",
      "        -0.0098,  0.0670, -0.0362, -0.0751,  0.0047, -0.0123, -0.0358, -0.0097,\n",
      "        -0.0455, -0.0307, -0.0469,  0.0635, -0.0362, -0.0335,  0.0391, -0.0297,\n",
      "        -0.0131, -0.0334, -0.0361, -0.0664,  0.0946, -0.0252,  0.0386, -0.0518,\n",
      "         0.0619,  0.0290, -0.0554,  0.0513,  0.0332, -0.0322,  0.0507, -0.0346,\n",
      "        -0.0090,  0.0039,  0.0088,  0.0144, -0.0029,  0.0094, -0.0505,  0.0166,\n",
      "        -0.0147, -0.0186, -0.0159,  0.0748, -0.0377, -0.0708,  0.0619, -0.0660,\n",
      "        -0.0425,  0.0549, -0.0499, -0.0389,  0.0112, -0.0459, -0.0415, -0.0770,\n",
      "         0.0194,  0.0640, -0.0082, -0.0291,  0.0151, -0.0286,  0.0541, -0.0418,\n",
      "         0.0872, -0.0305, -0.0008, -0.0407, -0.0527,  0.0168, -0.0494, -0.0404,\n",
      "         0.0384, -0.0317,  0.0768, -0.0657,  0.0386,  0.0210,  0.0511, -0.0576,\n",
      "         0.0239,  0.0447,  0.0085,  0.0167,  0.0412,  0.0634, -0.0174,  0.0035,\n",
      "         0.0966, -0.0297, -0.0334, -0.0301, -0.0172,  0.0815, -0.0304,  0.0325,\n",
      "         0.0709, -0.0821, -0.0337,  0.0528,  0.0402,  0.0118,  0.0467,  0.0142,\n",
      "         0.0376,  0.0633,  0.0058, -0.0345,  0.0118, -0.0283, -0.0153,  0.0122,\n",
      "        -0.0482, -0.0007,  0.0530,  0.0229,  0.0018,  0.0411,  0.0331, -0.0092,\n",
      "        -0.0257, -0.0284, -0.0718, -0.0565, -0.0249,  0.0136, -0.0759,  0.0469,\n",
      "         0.0046, -0.0175,  0.0146, -0.0361, -0.0462,  0.0095,  0.0864,  0.0366,\n",
      "         0.0615, -0.0319,  0.0134,  0.0178,  0.0289, -0.0220,  0.0906,  0.0848,\n",
      "         0.0577,  0.0491, -0.0766,  0.0261, -0.0275, -0.0189,  0.0234,  0.0464,\n",
      "        -0.0194,  0.0826, -0.0442, -0.0603, -0.0508,  0.0328,  0.0643,  0.0694,\n",
      "        -0.0228, -0.0822, -0.0300,  0.0811,  0.0212,  0.0644,  0.0325, -0.0328],\n",
      "       requires_grad=True))\n",
      "encoder.3.0.block.1.values_projection.scale \t tensor(1.)\n",
      "encoder.3.0.block.1.values_projection.zero_point \t tensor(0)\n",
      "encoder.3.0.block.1.values_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.3.0.block.1.values_projection._packed_params._packed_params \t (tensor([[-0.0240, -0.0370, -0.0566,  ..., -0.0131, -0.0261,  0.1219],\n",
      "        [ 0.0806, -0.0479,  0.0675,  ..., -0.0632,  0.0218, -0.0327],\n",
      "        [-0.0065, -0.0566, -0.0806,  ...,  0.0501,  0.0523,  0.0871],\n",
      "        ...,\n",
      "        [-0.0022, -0.1002,  0.0871,  ..., -0.0348, -0.0871,  0.0065],\n",
      "        [ 0.0109,  0.1503,  0.1002,  ...,  0.0305, -0.0283,  0.0065],\n",
      "        [ 0.0087,  0.0196,  0.0261,  ...,  0.1372,  0.1002, -0.0109]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0021776140201836824,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-1.8627e-02, -4.3878e-02,  3.0386e-02, -9.0771e-03,  6.4997e-02,\n",
      "         1.3909e-02,  4.2109e-02, -4.6108e-02, -6.3745e-02,  6.8599e-02,\n",
      "        -1.9122e-02,  4.6904e-02, -1.7008e-02, -5.4764e-02, -4.5532e-02,\n",
      "         7.5426e-02,  4.6319e-02, -5.5850e-02,  5.6144e-02,  6.4452e-02,\n",
      "        -3.7236e-02, -3.3246e-02,  5.1906e-02, -1.6680e-02, -2.2416e-02,\n",
      "         7.6488e-02,  6.1575e-03,  1.1236e-03, -4.2900e-02,  1.6354e-02,\n",
      "         3.8671e-02,  2.8949e-02,  4.7820e-03,  3.6182e-02, -2.2576e-02,\n",
      "        -7.4321e-02, -5.1352e-02, -4.4083e-02, -6.4162e-02, -2.6958e-02,\n",
      "         4.4976e-02, -5.6647e-02,  6.3612e-02, -4.0450e-02, -1.5776e-05,\n",
      "         9.9622e-03, -6.4098e-02,  1.9413e-02,  3.3225e-02,  4.1251e-02,\n",
      "        -4.7372e-02, -1.1767e-02,  4.8844e-03, -5.6332e-02, -6.4923e-02,\n",
      "         1.1726e-03, -5.9593e-02, -2.6627e-02, -5.7443e-02, -3.0700e-02,\n",
      "        -1.5003e-02, -5.3839e-02,  4.4623e-02,  5.5929e-02, -5.4194e-02,\n",
      "        -3.0195e-02, -4.3561e-03,  5.1839e-02,  3.9929e-02, -7.5012e-03,\n",
      "        -5.6717e-02,  6.1861e-02,  3.7536e-02, -1.3286e-02, -1.4462e-02,\n",
      "        -3.0207e-02,  7.5615e-02, -2.9839e-02,  3.7269e-02, -3.3005e-02,\n",
      "         6.5594e-02, -4.8426e-02,  5.7481e-02, -4.3456e-02, -4.7630e-02,\n",
      "         5.8360e-02, -4.5112e-02,  1.7742e-02,  3.0869e-02,  3.8056e-02,\n",
      "        -4.6854e-02,  3.8081e-02,  7.1573e-03, -5.8796e-02, -9.0919e-03,\n",
      "        -6.7218e-02,  3.3946e-02, -7.7046e-02,  4.9335e-02, -2.1596e-02,\n",
      "        -3.3229e-02,  3.7816e-02,  3.3158e-02, -1.6959e-02, -2.8203e-02,\n",
      "         3.5621e-02, -2.4840e-02, -1.2198e-02, -1.4581e-02, -3.0446e-02,\n",
      "        -2.2278e-02, -3.4568e-02, -6.8032e-02,  4.4890e-02, -1.9235e-02,\n",
      "         4.3933e-03,  7.1321e-02, -6.7825e-02, -6.6121e-02, -1.7426e-02,\n",
      "         5.0659e-02, -3.7577e-02, -4.3897e-02,  5.0975e-02,  4.4334e-02,\n",
      "        -4.3769e-03,  2.2281e-02, -6.0068e-02, -6.2605e-02,  8.9926e-03,\n",
      "         1.0928e-03,  5.0441e-02,  2.3453e-03,  2.5639e-02, -2.4920e-02,\n",
      "         5.7549e-02,  3.2589e-02, -5.5314e-02,  5.2479e-03, -4.9523e-02,\n",
      "        -3.2717e-02, -2.5194e-02, -3.5227e-02,  6.7698e-03,  1.6051e-02,\n",
      "         3.1840e-02, -5.6988e-02,  3.2806e-03, -3.2078e-02,  2.0541e-02,\n",
      "         6.6848e-02, -3.5439e-02, -4.2255e-02, -6.1124e-02, -1.5311e-02,\n",
      "        -4.3044e-02,  4.4349e-02,  4.1350e-02,  3.8279e-02, -2.2775e-02,\n",
      "         3.4741e-02, -2.9598e-02,  5.0590e-02, -1.4463e-02, -2.4443e-02,\n",
      "         2.1954e-02,  6.7381e-02, -3.3791e-02, -5.0199e-02,  6.9101e-02,\n",
      "        -7.4168e-02, -1.6203e-02, -5.1621e-02, -2.1846e-02,  2.0785e-02,\n",
      "         5.9565e-02, -7.1396e-02, -2.6188e-02, -5.7225e-02, -2.3865e-02,\n",
      "        -4.1376e-02, -6.0427e-02,  1.2254e-02,  3.8355e-02, -5.5849e-02,\n",
      "        -3.3683e-03, -3.6224e-02,  2.7925e-02, -3.9555e-02,  5.4546e-02,\n",
      "         2.7005e-02, -1.2565e-02], requires_grad=True))\n",
      "encoder.3.0.block.1.keys_projection.scale \t tensor(1.)\n",
      "encoder.3.0.block.1.keys_projection.zero_point \t tensor(0)\n",
      "encoder.3.0.block.1.keys_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.3.0.block.1.keys_projection._packed_params._packed_params \t (tensor([[ 0.0601,  0.0237, -0.0437,  ..., -0.1312,  0.0109, -0.0128],\n",
      "        [-0.0437,  0.0492, -0.0310,  ...,  0.0747, -0.0620, -0.0219],\n",
      "        [ 0.0292,  0.0510,  0.0128,  ...,  0.0073,  0.0583,  0.0036],\n",
      "        ...,\n",
      "        [ 0.0328, -0.0073,  0.0182,  ...,  0.0583, -0.0529, -0.0219],\n",
      "        [ 0.0255,  0.0820, -0.0437,  ..., -0.0547, -0.1002, -0.0109],\n",
      "        [-0.0292,  0.1130, -0.0766,  ..., -0.0911, -0.1185, -0.0036]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0018226621905341744,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0168, -0.0558,  0.0213,  0.0414, -0.0583,  0.0139,  0.0314, -0.0470,\n",
      "         0.0552, -0.0622,  0.0093,  0.0062, -0.0072,  0.0152, -0.0001,  0.0234,\n",
      "        -0.0667, -0.0265, -0.0547, -0.0699, -0.0379, -0.0578,  0.0260,  0.0734,\n",
      "         0.0189,  0.0241, -0.0370,  0.0735,  0.0407,  0.0032, -0.0133,  0.0431,\n",
      "        -0.0107,  0.0042,  0.0185, -0.0412, -0.0114, -0.0594,  0.0718, -0.0340,\n",
      "         0.0107,  0.0163, -0.0102, -0.0072,  0.0441,  0.0628,  0.0483, -0.0207,\n",
      "        -0.0254, -0.0603,  0.0025, -0.0076, -0.0461,  0.0279, -0.0520, -0.0339,\n",
      "        -0.0384, -0.0415, -0.0133,  0.0611, -0.0623, -0.0077,  0.0157,  0.0383,\n",
      "        -0.0068, -0.0006,  0.0014, -0.0054, -0.0156, -0.0469, -0.0197,  0.0352,\n",
      "        -0.0692,  0.0423, -0.0515, -0.0297,  0.0169, -0.0503, -0.0486,  0.0192,\n",
      "        -0.0201,  0.0002, -0.0257, -0.0357,  0.0346,  0.0196, -0.0195,  0.0691,\n",
      "        -0.0270,  0.0508,  0.0278, -0.0698,  0.0266, -0.0061,  0.0170,  0.0100,\n",
      "         0.0046,  0.0311, -0.0166, -0.0495, -0.0763,  0.0646,  0.0171,  0.0228,\n",
      "        -0.0524, -0.0485, -0.0271, -0.0375,  0.0691,  0.0131, -0.0012,  0.0076,\n",
      "         0.0172, -0.0607,  0.0397,  0.0153,  0.0088, -0.0357, -0.0306, -0.0449,\n",
      "         0.0663,  0.0119,  0.0696,  0.0344, -0.0591,  0.0156,  0.0198,  0.0723,\n",
      "         0.0380,  0.0437, -0.0254, -0.0478, -0.0587, -0.0112,  0.0051, -0.0472,\n",
      "         0.0520,  0.0189, -0.0246, -0.0427,  0.0139,  0.0367,  0.0469,  0.0610,\n",
      "         0.0336, -0.0686,  0.0961, -0.0320,  0.0174,  0.0419,  0.0609, -0.0354,\n",
      "         0.0156, -0.0651,  0.0194,  0.0004, -0.0354,  0.0555, -0.0491,  0.0262,\n",
      "         0.0116,  0.0006, -0.0023,  0.0572, -0.0793, -0.0079, -0.0343, -0.0732,\n",
      "         0.0070, -0.0495, -0.0511,  0.0067, -0.0445, -0.0367,  0.0648, -0.0599,\n",
      "         0.0617,  0.0240,  0.0326, -0.0410,  0.0059, -0.0547, -0.0079,  0.0153,\n",
      "         0.0417,  0.0592,  0.0723, -0.0002, -0.0341,  0.0435, -0.0296, -0.0106],\n",
      "       requires_grad=True))\n",
      "encoder.3.0.block.1.final_projection.scale \t tensor(1.)\n",
      "encoder.3.0.block.1.final_projection.zero_point \t tensor(0)\n",
      "encoder.3.0.block.1.final_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.3.0.block.1.final_projection._packed_params._packed_params \t (tensor([[ 0.0071,  0.2029, -0.0354,  ..., -0.0047,  0.0896,  0.1062],\n",
      "        [ 0.0236,  0.1014,  0.0377,  ...,  0.1085,  0.0802, -0.0236],\n",
      "        [ 0.0613, -0.1085, -0.0094,  ..., -0.0307,  0.0401, -0.0401],\n",
      "        ...,\n",
      "        [ 0.0849, -0.0212,  0.1109,  ..., -0.0330, -0.0637, -0.0613],\n",
      "        [ 0.0967, -0.0708,  0.0425,  ..., -0.0873,  0.1510,  0.0519],\n",
      "        [ 0.0967,  0.0661, -0.0047,  ...,  0.0425, -0.1062, -0.0142]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.002359129721298814,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0725, -0.0306,  0.0382, -0.0450, -0.0248,  0.0554,  0.0271,  0.0127,\n",
      "         0.0490, -0.0521,  0.0512, -0.0082, -0.0057,  0.0657, -0.0411, -0.0007,\n",
      "        -0.0245, -0.0377,  0.0111,  0.0190,  0.0687, -0.0731, -0.0201,  0.0064,\n",
      "        -0.0029, -0.0624,  0.0508, -0.0592,  0.0223,  0.0777,  0.0588, -0.0685,\n",
      "        -0.0548, -0.0096, -0.0550,  0.0440, -0.0405, -0.0547,  0.0494,  0.0116,\n",
      "        -0.0223,  0.0511, -0.0050, -0.0491, -0.0296,  0.0342, -0.0598, -0.0008,\n",
      "        -0.0165, -0.0543,  0.0546,  0.0563, -0.0376, -0.0461, -0.0027,  0.0548,\n",
      "        -0.0879, -0.0259, -0.0028,  0.0713,  0.0370,  0.0055, -0.0355, -0.0775,\n",
      "        -0.0498,  0.0321, -0.0626,  0.0301,  0.0382,  0.0303, -0.0630, -0.0092,\n",
      "        -0.0445,  0.0410, -0.0400,  0.0551, -0.0267, -0.0818, -0.0038,  0.0616,\n",
      "         0.0634, -0.0630,  0.0311, -0.0835,  0.0552,  0.0238, -0.0953, -0.0481,\n",
      "        -0.0294, -0.0777, -0.0438, -0.0194, -0.0773,  0.0366, -0.0149, -0.0552,\n",
      "        -0.0363, -0.0669, -0.0205,  0.0311,  0.0250,  0.0818,  0.0847, -0.0125,\n",
      "         0.0349, -0.0081,  0.0387, -0.0281,  0.0227,  0.0646,  0.0292,  0.0387,\n",
      "        -0.0059,  0.0772, -0.0122,  0.0383,  0.0394, -0.0438, -0.0282, -0.0397,\n",
      "        -0.0041, -0.0680,  0.0183, -0.0552, -0.0111, -0.0258, -0.0046,  0.0357,\n",
      "        -0.0750, -0.0630, -0.0629, -0.0218, -0.0425,  0.0422, -0.0437,  0.0251,\n",
      "         0.0008,  0.0128, -0.0250,  0.0117, -0.0781, -0.0688,  0.0210, -0.0149,\n",
      "         0.0543,  0.0209, -0.0037, -0.0060,  0.0476,  0.0156,  0.0639,  0.0396,\n",
      "         0.0224, -0.0409, -0.0427,  0.0212,  0.0073, -0.0624, -0.0373, -0.0449,\n",
      "        -0.0735, -0.0462,  0.0033,  0.0199, -0.0322, -0.0355, -0.0203, -0.0158,\n",
      "        -0.0401,  0.0173, -0.0711, -0.0577,  0.0353,  0.0215, -0.0341, -0.0500,\n",
      "         0.0227,  0.0057, -0.0758,  0.0668,  0.0465,  0.0014,  0.0652, -0.0614,\n",
      "         0.0710,  0.0663,  0.0487,  0.0682, -0.0090,  0.0548, -0.0081, -0.0786],\n",
      "       requires_grad=True))\n",
      "encoder.3.1.block.0.weight \t tensor([0.9066, 0.9549, 0.9005, 0.9677, 0.9374, 0.9001, 0.9792, 0.9835, 0.9143,\n",
      "        0.9321, 0.9239, 0.9890, 0.9120, 0.9731, 0.9166, 0.9120, 0.8989, 0.8742,\n",
      "        0.9751, 0.9445, 0.9493, 0.9389, 0.9456, 0.9089, 0.9042, 0.9618, 0.9344,\n",
      "        0.9328, 0.9151, 0.9459, 0.9610, 0.9111, 0.9331, 0.8996, 0.9495, 0.9342,\n",
      "        0.9607, 0.9440, 0.9047, 0.9750, 0.9387, 0.9338, 0.8973, 0.9585, 0.8789,\n",
      "        0.9052, 0.9494, 0.9259, 0.9597, 0.9287, 0.9070, 0.9320, 0.9194, 0.9477,\n",
      "        0.8842, 0.9239, 0.9677, 0.8407, 0.8824, 0.9557, 0.9609, 0.9392, 0.9502,\n",
      "        0.9706, 0.9700, 0.9410, 0.9798, 0.9730, 0.9317, 0.9516, 0.9685, 0.9847,\n",
      "        0.9344, 0.9720, 0.9624, 0.9134, 0.9182, 0.8976, 0.9797, 0.9265, 0.8619,\n",
      "        0.9799, 0.9834, 0.9351, 0.9317, 0.9333, 0.8785, 0.9528, 0.8928, 0.9045,\n",
      "        0.9446, 0.9226, 0.9423, 0.9003, 0.9174, 0.9145, 0.8956, 0.9545, 0.9564,\n",
      "        0.9211, 0.9645, 0.9022, 0.9441, 0.9229, 0.9321, 0.9309, 0.8879, 1.0082,\n",
      "        0.9600, 0.9380, 0.9158, 0.9012, 0.9356, 0.9247, 0.9203, 0.9054, 0.9848,\n",
      "        0.9380, 1.0223, 0.9361, 0.8965, 0.9450, 0.9288, 0.9287, 0.8653, 0.9260,\n",
      "        0.9288, 0.9780, 0.9203, 0.9156, 0.9730, 0.9319, 0.9143, 0.9719, 0.9519,\n",
      "        0.9156, 0.9135, 0.8683, 0.9648, 0.8951, 0.8977, 0.9285, 0.9385, 0.8888,\n",
      "        0.8578, 0.9370, 0.9535, 0.9214, 0.9613, 0.9586, 0.9445, 0.9689, 0.9685,\n",
      "        0.9414, 0.9345, 0.8977, 0.9753, 0.9153, 0.9055, 0.9173, 0.9642, 0.9099,\n",
      "        0.9438, 0.9518, 0.9411, 0.9281, 0.9639, 0.8878, 0.8680, 0.9565, 0.9291,\n",
      "        0.9047, 0.9400, 0.8986, 0.9066, 0.9123, 0.9656, 0.8870, 0.9437, 0.9578,\n",
      "        0.9383, 0.9238, 0.9340, 0.8809, 0.9424, 0.9483, 0.9663, 0.9065, 0.9823,\n",
      "        0.9635, 0.9082, 0.8686])\n",
      "encoder.3.1.block.0.bias \t tensor([ 0.0640, -0.0412, -0.0450, -0.0043, -0.0797,  0.0126, -0.0350,  0.0474,\n",
      "        -0.0172,  0.0257, -0.0365,  0.0557,  0.0105, -0.0296,  0.0206, -0.0070,\n",
      "        -0.0298, -0.0201,  0.0545,  0.0808,  0.0003,  0.0237, -0.0207, -0.0193,\n",
      "         0.0203, -0.0337, -0.0438, -0.0133,  0.0446, -0.0189, -0.0452,  0.0227,\n",
      "         0.0084,  0.0260, -0.0393,  0.0334, -0.0027, -0.0431,  0.0121, -0.0265,\n",
      "         0.0178, -0.0301, -0.0138,  0.0182, -0.0073,  0.0010,  0.0332,  0.0228,\n",
      "         0.0280, -0.0229,  0.0249,  0.0012,  0.0495, -0.0369, -0.0593,  0.0225,\n",
      "         0.0312, -0.0625,  0.0130, -0.0029, -0.0475, -0.0556, -0.0030,  0.0392,\n",
      "        -0.0017, -0.0257, -0.0300,  0.0437, -0.0042, -0.0493, -0.0644, -0.0108,\n",
      "        -0.0267, -0.0232,  0.0668, -0.0216,  0.0085,  0.0396, -0.0143,  0.0099,\n",
      "        -0.0199, -0.0405,  0.0533,  0.0488, -0.0428, -0.0322, -0.0129,  0.0172,\n",
      "        -0.0054, -0.0062, -0.0377,  0.0025,  0.0131, -0.0118, -0.0477, -0.0016,\n",
      "         0.0145, -0.0300, -0.0413, -0.0050, -0.0441, -0.0002, -0.0232, -0.0096,\n",
      "         0.0375,  0.0242,  0.0130,  0.0081,  0.0194,  0.0199,  0.0289,  0.0190,\n",
      "        -0.0513, -0.0197, -0.0375, -0.0689,  0.0549, -0.0085, -0.0463,  0.0283,\n",
      "        -0.0262, -0.0158, -0.0162,  0.0567, -0.0191, -0.0368,  0.0478,  0.0231,\n",
      "         0.0231, -0.0540,  0.0366,  0.0094, -0.0408, -0.0675, -0.0067,  0.0089,\n",
      "        -0.0450, -0.0199, -0.0600,  0.0037, -0.0192,  0.0005, -0.0569, -0.0032,\n",
      "        -0.0049,  0.0150,  0.0424,  0.0389, -0.0417,  0.0022, -0.0324, -0.0135,\n",
      "        -0.0304, -0.0436,  0.0334,  0.0204,  0.0506, -0.0208, -0.0265, -0.0456,\n",
      "         0.0291, -0.0158, -0.0707, -0.0562, -0.0822, -0.0022, -0.0199,  0.0187,\n",
      "        -0.0248,  0.0216, -0.0093, -0.0033, -0.0198,  0.0036,  0.0293,  0.0505,\n",
      "         0.0038, -0.0133, -0.0625,  0.0333, -0.0020,  0.0366,  0.0278,  0.0276,\n",
      "        -0.0294, -0.0704, -0.0076, -0.0242,  0.0201,  0.0321,  0.0307,  0.0227])\n",
      "encoder.3.1.block.1.0.scale \t tensor(1.)\n",
      "encoder.3.1.block.1.0.zero_point \t tensor(0)\n",
      "encoder.3.1.block.1.0._packed_params.dtype \t torch.qint8\n",
      "encoder.3.1.block.1.0._packed_params._packed_params \t (tensor([[-0.0146, -0.0088, -0.0540,  ..., -0.0467, -0.0847, -0.0380],\n",
      "        [ 0.0146,  0.0453, -0.0190,  ..., -0.0292, -0.0482, -0.0088],\n",
      "        [ 0.0394, -0.0175,  0.0248,  ..., -0.0818, -0.0453, -0.0102],\n",
      "        ...,\n",
      "        [-0.0380, -0.0336,  0.0891,  ...,  0.0263,  0.0073, -0.0599],\n",
      "        [-0.0336,  0.0482,  0.0175,  ...,  0.0424, -0.0482,  0.0424],\n",
      "        [ 0.0716,  0.0482, -0.0234,  ..., -0.0336, -0.0175,  0.0190]],\n",
      "       size=(768, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0014605092583224177,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 3.3441e-02,  3.8430e-02, -7.1661e-02,  1.8153e-02,  1.7913e-02,\n",
      "         1.6649e-02,  2.7896e-02, -8.7044e-02, -1.1498e-02, -3.3865e-02,\n",
      "         1.5553e-03, -1.0537e-02, -4.1885e-02, -5.0961e-02, -7.7284e-02,\n",
      "        -8.8529e-02,  1.8900e-02, -2.5519e-02, -6.8470e-02,  3.9677e-02,\n",
      "         2.8829e-02, -9.1577e-02, -5.1707e-02, -7.6878e-02, -4.3976e-03,\n",
      "         2.9819e-02, -8.1328e-02, -4.5817e-02,  3.2524e-02, -7.1132e-02,\n",
      "        -3.1062e-02, -9.3084e-02, -5.5924e-02,  4.5697e-02,  2.7919e-02,\n",
      "         3.1967e-02, -8.6873e-02, -9.6734e-02, -1.0127e-01, -1.4408e-02,\n",
      "         1.0889e-02,  4.1143e-02, -4.8754e-02, -9.8918e-03, -1.3009e-02,\n",
      "         5.0141e-02,  3.5069e-03, -4.4662e-02, -3.1905e-02,  2.1381e-02,\n",
      "        -3.2227e-02,  2.1005e-02, -7.4122e-02, -6.8380e-02, -4.6641e-02,\n",
      "        -9.0629e-02, -9.0009e-02,  1.4431e-02, -8.8289e-02, -3.5326e-02,\n",
      "        -7.2763e-02,  1.6765e-02, -6.8302e-02, -7.0011e-02, -2.1894e-02,\n",
      "         2.5839e-02, -1.8000e-02, -5.1103e-02, -3.3540e-02, -6.7795e-02,\n",
      "        -1.9931e-02,  5.5793e-03,  2.0302e-02, -1.6584e-02, -7.4950e-02,\n",
      "        -8.5374e-02, -9.0080e-02, -5.0019e-02, -1.5877e-02, -9.6485e-02,\n",
      "        -6.6753e-04,  3.3127e-02, -8.2108e-03, -6.8637e-02,  4.4627e-03,\n",
      "         8.7925e-03,  2.1210e-02, -8.2811e-02, -7.0205e-02, -1.9657e-02,\n",
      "         9.8905e-03, -3.4725e-02, -9.5684e-02, -3.2369e-02, -4.4377e-03,\n",
      "        -2.9430e-02,  3.2112e-02, -3.1664e-02,  9.8534e-03, -1.1850e-02,\n",
      "        -3.0268e-02, -8.8512e-02, -7.3054e-02, -4.8161e-02, -1.6475e-03,\n",
      "        -8.4980e-02,  5.3006e-04,  1.4045e-02, -2.3867e-02, -8.3876e-02,\n",
      "        -9.3897e-02, -2.3258e-02, -4.4264e-02, -3.0870e-02, -3.0806e-02,\n",
      "        -2.6533e-02, -8.6030e-02, -7.0670e-02, -5.8013e-02,  9.9902e-03,\n",
      "        -7.9273e-02,  7.6808e-03,  3.6190e-02, -3.4774e-03, -8.8645e-02,\n",
      "        -9.8625e-02,  3.9855e-02, -6.3530e-02,  3.1181e-03, -6.0887e-02,\n",
      "        -1.1161e-01, -9.9400e-02, -2.2841e-02, -4.7200e-02,  1.7902e-02,\n",
      "        -8.1330e-02, -9.6801e-02, -3.0196e-02, -6.4598e-02, -9.1760e-02,\n",
      "        -4.9230e-02, -2.2816e-02,  2.3309e-02, -7.8388e-02, -7.8573e-02,\n",
      "        -8.8531e-02, -2.5693e-02, -6.3832e-02, -4.8289e-02,  1.7903e-02,\n",
      "        -2.6012e-02, -3.2069e-03, -9.4093e-02,  8.5007e-03, -2.2988e-02,\n",
      "        -2.3428e-02,  5.9329e-04, -7.7983e-02, -6.6312e-02,  3.2075e-03,\n",
      "         2.5644e-02, -8.0212e-02, -8.8107e-02, -8.7515e-03,  4.0728e-02,\n",
      "        -5.0416e-02,  4.7111e-02, -8.4791e-02, -5.4048e-02, -8.8282e-02,\n",
      "        -9.5644e-02, -8.8619e-02,  7.3811e-03,  2.7402e-02,  3.6557e-02,\n",
      "        -3.7604e-02, -7.4199e-02,  2.7139e-02,  1.6120e-02,  9.6329e-03,\n",
      "        -6.8838e-02, -1.2372e-02,  7.2094e-03, -3.7336e-02,  3.4935e-02,\n",
      "         3.0687e-02,  9.1444e-04, -9.3370e-03, -3.3904e-03,  2.5988e-02,\n",
      "        -8.1621e-02, -2.0207e-02, -8.1310e-02, -5.6275e-02,  2.2272e-02,\n",
      "        -8.5633e-02,  9.2056e-03, -2.7302e-02, -5.9395e-02,  3.2839e-02,\n",
      "        -5.4196e-02, -3.1749e-02, -1.0364e-01,  1.9338e-02,  2.2273e-02,\n",
      "         1.2328e-02,  1.6391e-03, -5.4641e-03, -7.7859e-02, -5.4960e-02,\n",
      "         1.7740e-02,  4.4753e-02, -6.7995e-02,  1.3576e-02, -8.2310e-02,\n",
      "         1.3889e-02, -2.0589e-02, -5.3974e-02,  9.8417e-03, -8.7330e-02,\n",
      "        -9.9583e-02, -3.7876e-02, -5.2796e-03,  1.9224e-02,  1.4179e-02,\n",
      "         1.1080e-02, -5.2723e-02,  2.1289e-02, -2.4432e-02, -8.9743e-02,\n",
      "        -7.6236e-02, -2.5705e-03, -6.5596e-02,  1.8978e-02, -4.9269e-02,\n",
      "         2.7551e-02,  2.5665e-03, -6.3687e-02, -2.3700e-02, -9.8900e-02,\n",
      "        -1.2857e-04, -9.3810e-02,  1.5482e-02, -7.1694e-03,  3.3307e-02,\n",
      "        -9.0199e-02, -8.0881e-02, -7.3295e-02, -3.1349e-02, -4.9192e-02,\n",
      "         2.2334e-02, -1.6943e-02,  1.3723e-02, -7.0845e-02, -9.9997e-02,\n",
      "         2.8440e-02,  1.6336e-02, -4.0416e-02, -2.8165e-03, -7.7386e-02,\n",
      "        -6.5318e-02, -8.4303e-02, -4.9964e-03, -9.1975e-02,  4.5618e-03,\n",
      "        -9.5014e-02, -4.8431e-03,  4.0951e-02,  4.0655e-02,  9.4432e-03,\n",
      "        -6.2935e-02,  2.5214e-02, -6.4543e-03, -4.2306e-02, -4.5869e-02,\n",
      "        -7.5739e-02, -8.3143e-02, -4.5556e-02, -5.6682e-02, -7.3457e-02,\n",
      "        -6.1315e-02, -1.0809e-02, -1.7806e-02,  1.2333e-03,  5.1923e-03,\n",
      "        -2.8557e-02, -4.9020e-02, -3.9339e-02,  4.8971e-02, -3.0262e-02,\n",
      "        -2.2337e-02, -1.0196e-01, -3.7748e-02, -9.3831e-02, -6.7889e-02,\n",
      "        -1.0619e-01, -7.1129e-02, -5.7592e-02, -9.5269e-02, -1.0721e-02,\n",
      "        -1.0152e-01, -1.7403e-02, -1.4064e-02,  6.4221e-03, -4.2807e-02,\n",
      "        -9.7154e-02, -5.4263e-02,  2.1896e-02, -1.1734e-02, -5.4799e-02,\n",
      "        -6.9422e-02,  5.3353e-03, -3.8527e-02, -5.9866e-02,  1.8368e-02,\n",
      "        -8.8982e-02,  2.6855e-02, -9.2068e-03, -2.2966e-02, -6.5606e-02,\n",
      "        -4.1881e-03, -3.6888e-02, -5.0803e-02, -5.7827e-02, -1.0067e-01,\n",
      "        -3.5818e-02, -5.1365e-02, -8.9642e-02, -5.3733e-02, -1.0399e-01,\n",
      "        -1.0929e-02,  9.0069e-03, -1.0012e-01,  2.3657e-02,  3.8638e-03,\n",
      "        -9.6186e-02, -1.1113e-02,  2.6285e-02,  1.4663e-02, -6.9535e-02,\n",
      "        -6.0718e-03,  6.3939e-03, -2.5461e-02, -2.7860e-02, -1.8090e-02,\n",
      "        -4.3109e-02, -1.5994e-02,  1.4864e-02, -2.9525e-02, -2.4714e-02,\n",
      "        -1.0707e-02, -4.2378e-02, -6.6208e-02, -2.2795e-02, -8.1938e-02,\n",
      "        -1.2115e-02, -5.1927e-02,  2.2454e-02, -7.8778e-02, -8.3510e-02,\n",
      "        -3.3089e-02,  1.2228e-02, -1.5605e-02, -3.9323e-02, -4.6221e-02,\n",
      "        -4.3302e-03,  2.3649e-02, -3.3788e-02, -1.9352e-02,  1.4964e-02,\n",
      "        -3.2664e-02,  3.2163e-02,  1.5573e-02, -6.0458e-02,  2.4822e-03,\n",
      "        -3.1644e-02, -3.6982e-02, -6.5191e-02,  8.5853e-03, -3.1106e-02,\n",
      "        -3.2412e-02, -1.5203e-02, -5.9668e-02, -2.7540e-02, -7.8562e-02,\n",
      "        -2.3305e-02,  2.1029e-02, -3.8998e-02, -1.4977e-02, -1.0522e-01,\n",
      "        -3.0929e-02, -1.4339e-02, -3.5542e-02, -3.6579e-02, -4.1272e-02,\n",
      "         2.1958e-02,  1.6541e-03, -3.2110e-02, -3.9092e-02, -5.9712e-02,\n",
      "        -1.0188e-01, -3.6707e-03, -1.0450e-03, -3.5000e-02, -8.1435e-02,\n",
      "        -4.8767e-02, -8.7150e-02, -8.0051e-02, -1.9912e-03,  2.4357e-02,\n",
      "        -8.5525e-02, -3.9574e-02, -8.7585e-02,  4.1714e-02, -3.4004e-02,\n",
      "         4.0280e-03, -7.3623e-02, -9.5764e-02, -7.6754e-02, -3.9082e-02,\n",
      "         1.9052e-03, -9.8114e-02, -5.6225e-02, -4.6219e-02,  3.9182e-02,\n",
      "        -9.6728e-02,  9.4538e-03, -6.9746e-02, -7.0175e-02, -6.4498e-02,\n",
      "        -6.9286e-02, -6.5739e-02, -6.2058e-02, -7.7012e-03, -1.0953e-01,\n",
      "        -6.3375e-02,  6.6056e-03, -2.3941e-02,  4.2272e-02, -1.2117e-02,\n",
      "        -6.1747e-02,  2.6530e-02, -4.8446e-02, -6.0966e-02,  2.2526e-02,\n",
      "        -6.8358e-02,  2.7224e-02,  6.7504e-03, -2.9032e-02, -2.6012e-02,\n",
      "        -2.3698e-02, -3.8934e-02, -9.4467e-02, -1.0349e-02, -3.1300e-02,\n",
      "        -7.7978e-02,  3.7067e-04, -6.5569e-02,  1.7830e-02, -7.5138e-02,\n",
      "         5.9299e-03, -1.9459e-03, -4.6472e-02, -7.0932e-02,  2.0597e-02,\n",
      "         2.3119e-02, -2.7020e-02, -3.3173e-02, -5.9831e-02, -7.2248e-02,\n",
      "        -5.1415e-02, -3.2829e-02, -4.5160e-02,  2.3913e-02, -5.6768e-02,\n",
      "        -6.8424e-02, -1.1249e-02, -2.1859e-02, -3.0401e-02, -1.1159e-02,\n",
      "        -7.0607e-02, -3.4970e-02,  2.8226e-03,  4.2129e-03,  8.0755e-03,\n",
      "         2.6925e-02, -1.1704e-01, -1.1700e-02, -9.7348e-02, -4.3125e-02,\n",
      "        -1.8860e-02, -2.1843e-02,  2.8692e-02,  1.1608e-02, -3.9565e-04,\n",
      "        -3.9528e-04, -5.2813e-02,  7.9253e-03,  3.2017e-02, -7.9881e-02,\n",
      "         2.5331e-02, -6.8579e-02, -2.0219e-02,  1.3410e-02,  2.5089e-02,\n",
      "        -2.5439e-02,  2.3273e-02, -4.7141e-02, -4.9935e-02, -8.0539e-02,\n",
      "        -7.5962e-02, -6.5641e-03, -5.9874e-02,  1.9584e-02, -7.0711e-02,\n",
      "         7.8002e-03, -3.1648e-02,  2.8595e-02,  5.1081e-02,  1.3754e-02,\n",
      "        -1.1641e-02,  5.1147e-02,  2.9803e-02, -9.5134e-02, -1.0099e-01,\n",
      "        -4.3209e-03,  2.4611e-02,  7.5309e-03, -2.4803e-02, -4.0680e-02,\n",
      "        -9.3661e-02,  3.8314e-02, -8.3644e-02, -4.8021e-02,  1.9035e-02,\n",
      "         3.0829e-02, -3.7275e-04, -7.4077e-02,  2.5226e-02,  2.8870e-02,\n",
      "         1.4766e-02,  2.2358e-02,  2.4847e-03, -8.5020e-02, -7.2200e-02,\n",
      "        -4.8243e-03, -5.8959e-02, -5.7697e-02, -9.3426e-02, -4.7220e-02,\n",
      "        -1.0181e-01, -4.2730e-02,  2.0634e-03, -2.2809e-02, -7.9139e-02,\n",
      "        -6.7606e-03, -3.2039e-02, -2.7061e-02, -6.1321e-02, -2.4493e-02,\n",
      "         2.0769e-02, -7.0282e-02,  3.6356e-02, -6.0010e-02, -3.2055e-02,\n",
      "         7.5805e-04, -8.6813e-02, -7.0397e-02, -6.5407e-02, -3.9945e-02,\n",
      "        -5.6572e-02, -6.7270e-02, -8.6002e-02, -4.1048e-02, -3.3401e-02,\n",
      "        -5.7577e-02, -3.8586e-02,  1.9643e-03, -5.9244e-02,  1.3922e-02,\n",
      "         2.0540e-02,  2.0141e-02, -5.3601e-02, -7.3935e-02,  2.8122e-02,\n",
      "        -3.1121e-02, -3.0290e-03, -1.0093e-01,  3.0509e-02,  3.9773e-02,\n",
      "        -2.6442e-02, -1.9202e-02,  3.4863e-02, -3.9349e-02, -2.9100e-02,\n",
      "        -8.8096e-02, -8.1039e-02, -9.1917e-02, -3.0813e-02, -9.9715e-02,\n",
      "        -9.3793e-03, -3.3948e-02, -1.8608e-02, -6.2622e-02,  1.9663e-02,\n",
      "         3.5373e-02, -1.4685e-02, -9.7672e-02, -6.0088e-02, -6.2356e-02,\n",
      "        -4.4013e-03, -8.4475e-02, -7.7166e-03, -7.5485e-02, -8.3846e-02,\n",
      "        -3.6766e-02, -7.2459e-02,  1.0944e-02,  1.9061e-02, -7.1653e-02,\n",
      "        -6.9866e-02, -2.1593e-02, -6.5706e-02, -7.1916e-03,  7.2179e-04,\n",
      "        -8.5615e-02, -3.6451e-02, -1.9299e-02, -4.8186e-02,  1.5110e-02,\n",
      "        -2.7953e-02, -5.5852e-02,  1.1687e-02, -3.2649e-02,  3.7290e-02,\n",
      "        -3.0685e-02, -9.8530e-02, -5.1073e-02,  1.8729e-02,  1.8008e-02,\n",
      "        -3.5330e-02, -5.1840e-02, -5.7237e-02, -1.0756e-05, -8.1662e-02,\n",
      "        -1.7157e-02,  6.3652e-03, -1.5074e-02, -1.2757e-02, -3.6156e-02,\n",
      "        -3.9825e-02,  8.7369e-03, -7.0702e-03, -4.1305e-02,  1.8006e-02,\n",
      "        -9.3407e-02, -4.5959e-04, -6.8399e-02, -7.0077e-02,  1.9552e-02,\n",
      "        -3.1802e-02, -1.1662e-02,  1.1397e-02,  2.1372e-02,  3.5546e-02,\n",
      "        -3.1894e-02, -4.9944e-02, -5.3372e-02,  2.5399e-02, -2.8569e-04,\n",
      "        -7.8378e-02,  1.0062e-03, -4.9141e-02, -3.6139e-03, -6.2803e-02,\n",
      "        -1.1202e-02,  3.8185e-03, -7.8598e-02, -9.1733e-02, -7.7255e-02,\n",
      "         3.3476e-02, -6.8725e-02, -3.5863e-02, -4.3473e-02,  2.7754e-02,\n",
      "         1.8694e-02, -4.2100e-02, -5.3840e-02, -5.4418e-02, -1.4064e-02,\n",
      "        -2.5241e-02, -2.7827e-02,  1.1537e-02,  2.0792e-02, -2.2233e-02,\n",
      "         2.1526e-02,  2.0989e-02,  1.8733e-02, -8.9651e-02,  4.9983e-03,\n",
      "        -4.2422e-02, -5.1920e-02, -5.2299e-03, -5.5330e-02, -1.6249e-03,\n",
      "         3.4618e-02, -2.1098e-03, -6.5688e-02, -9.0384e-02, -3.0208e-02,\n",
      "        -5.5144e-02, -3.1086e-02, -4.3077e-03,  1.3133e-02, -2.1231e-02,\n",
      "        -2.3042e-02, -8.2960e-02, -4.5219e-02, -1.7343e-02, -7.1771e-02,\n",
      "        -1.8579e-02, -5.4593e-03, -1.0340e-02, -4.5149e-02,  1.7131e-02,\n",
      "        -3.3945e-02,  1.7480e-02, -7.1708e-02, -5.8631e-02, -6.7172e-03,\n",
      "         3.5955e-02, -3.6470e-02, -5.1661e-02, -4.4575e-02,  1.6170e-02,\n",
      "         1.2500e-02, -2.4210e-02, -9.7514e-02, -5.7835e-02,  1.5479e-02,\n",
      "        -1.0234e-02,  1.1082e-02, -2.5356e-02,  1.1103e-02, -2.0862e-02,\n",
      "         2.5243e-02, -4.0036e-02,  1.9773e-02, -6.5612e-03,  7.6747e-03,\n",
      "        -8.5329e-02,  3.0811e-02, -9.7028e-03, -4.5356e-02, -2.3893e-02,\n",
      "        -4.8819e-02, -1.7807e-02, -3.1872e-02, -7.4933e-02,  2.4569e-02,\n",
      "        -2.7530e-02, -3.4258e-02, -1.0015e-01, -7.1280e-02,  3.0927e-02,\n",
      "        -3.3897e-02, -2.9214e-02,  1.1343e-02], requires_grad=True))\n",
      "encoder.3.1.block.1.2.scale \t tensor(1.)\n",
      "encoder.3.1.block.1.2.zero_point \t tensor(0)\n",
      "encoder.3.1.block.1.2._packed_params.dtype \t torch.qint8\n",
      "encoder.3.1.block.1.2._packed_params._packed_params \t (tensor([[-0.0489,  0.0359, -0.0272,  ..., -0.0098,  0.0457, -0.0196],\n",
      "        [ 0.0120,  0.0000, -0.0272,  ...,  0.0250,  0.0065, -0.0033],\n",
      "        [ 0.0207,  0.0141,  0.0740,  ..., -0.0348, -0.0044,  0.0131],\n",
      "        ...,\n",
      "        [ 0.0370,  0.0152,  0.0033,  ..., -0.0174, -0.0457, -0.0163],\n",
      "        [ 0.0402,  0.0000,  0.0076,  ..., -0.0272, -0.0283, -0.0305],\n",
      "        [ 0.0054, -0.0185,  0.0076,  ...,  0.0087,  0.0435, -0.0370]],\n",
      "       size=(192, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0010876706801354885,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-1.0037e-03, -8.5418e-03,  3.2087e-02,  2.8338e-02, -1.6778e-02,\n",
      "         2.5375e-02,  2.8864e-02, -1.1825e-02, -2.3556e-02,  2.6449e-02,\n",
      "         3.4684e-02,  3.4674e-03, -2.7540e-02,  1.9697e-02, -6.3831e-02,\n",
      "         2.0652e-02, -3.0529e-02,  1.1182e-02, -3.9847e-02,  3.1403e-02,\n",
      "        -8.8374e-03,  2.0049e-03, -2.3910e-02,  5.4574e-02, -2.7673e-02,\n",
      "         3.6172e-03,  1.3900e-02,  3.3865e-02, -3.0893e-02, -1.2857e-02,\n",
      "         3.3860e-02,  4.0392e-03,  1.8213e-02,  2.3025e-03,  1.2871e-02,\n",
      "        -1.1859e-02, -2.7720e-02, -4.2813e-02, -4.1133e-02,  6.6520e-03,\n",
      "         1.5337e-02, -2.8147e-03,  7.5340e-03, -1.1510e-02,  3.9856e-02,\n",
      "         1.0310e-02, -5.7408e-02,  9.6478e-04, -3.2182e-02,  3.4547e-02,\n",
      "        -4.0738e-03, -1.0236e-02, -1.3705e-02,  1.5009e-02, -3.5272e-02,\n",
      "        -2.1837e-02, -1.4615e-02, -6.1347e-03, -4.3597e-02, -3.0252e-02,\n",
      "        -9.3754e-03,  4.2047e-02, -1.2771e-02, -1.8173e-04, -4.1749e-02,\n",
      "        -1.4659e-02, -2.1413e-02, -1.3414e-02,  1.4907e-02, -2.4521e-02,\n",
      "         3.4788e-02,  1.6397e-02, -1.3721e-02, -2.2453e-02, -1.3973e-02,\n",
      "         1.7136e-03,  6.4993e-03, -9.5267e-03, -2.2603e-02, -6.4888e-03,\n",
      "         2.7936e-02,  9.8066e-03, -5.3320e-02, -2.7568e-02, -2.3378e-02,\n",
      "        -6.2156e-04, -1.1233e-02, -2.2080e-02,  2.4353e-02, -5.8303e-02,\n",
      "        -1.0803e-02, -3.8393e-02, -3.6945e-02,  6.2695e-03,  2.3310e-02,\n",
      "        -9.4642e-03, -2.7811e-02, -6.6660e-03,  1.9368e-02, -5.1195e-02,\n",
      "         1.7265e-02,  2.5570e-02,  3.9772e-02, -2.9609e-02, -3.8756e-02,\n",
      "         1.2818e-02, -3.7244e-03, -2.6336e-02,  3.4319e-03,  2.3178e-02,\n",
      "        -4.9157e-04, -3.7813e-02,  8.8967e-03,  3.2749e-03,  1.8660e-03,\n",
      "         4.1052e-02, -1.6205e-02, -1.6217e-05, -2.0909e-02, -4.3287e-02,\n",
      "        -8.3200e-03,  2.6677e-02, -2.4542e-03, -2.1259e-02, -1.4268e-02,\n",
      "        -5.3820e-04, -4.0703e-02, -2.2650e-03,  1.1175e-02,  4.7301e-03,\n",
      "        -6.3202e-02, -5.1364e-02,  4.1999e-02,  4.7264e-02, -9.3971e-03,\n",
      "        -1.1662e-02,  4.8651e-02, -6.7894e-04,  7.3210e-03,  2.1874e-02,\n",
      "        -4.1737e-02, -2.9020e-02, -8.0362e-03, -1.2904e-02, -5.3793e-04,\n",
      "        -5.6242e-02, -3.6868e-02, -7.1430e-02, -2.9752e-02, -1.1623e-02,\n",
      "        -1.3045e-02,  2.8129e-02, -1.8660e-02,  9.9071e-03, -2.6548e-02,\n",
      "        -1.3560e-02, -4.2182e-02,  1.5348e-02, -3.5814e-03, -2.2825e-03,\n",
      "        -1.5248e-02,  3.1960e-04, -1.7675e-02,  1.0479e-02, -7.9718e-03,\n",
      "         4.0915e-02,  3.7634e-02, -5.9756e-02,  2.3788e-02, -2.2634e-02,\n",
      "        -3.2624e-02, -2.8276e-02,  3.1671e-02, -3.3303e-02, -4.6724e-02,\n",
      "        -3.8836e-02, -4.1448e-02,  1.3491e-02, -2.0844e-02,  1.0571e-02,\n",
      "        -3.1206e-02, -5.6161e-02, -3.4224e-02,  2.2168e-04,  3.7798e-02,\n",
      "        -1.3469e-02,  9.4888e-03,  1.7160e-02,  6.7917e-04, -5.0891e-03,\n",
      "        -2.9269e-02, -4.6736e-02], requires_grad=True))\n",
      "encoder.4.0.block.0.weight \t tensor([1.0638, 1.0900, 1.0632, 1.0523, 1.0345, 1.0584, 1.1711, 1.0900, 1.0929,\n",
      "        1.0388, 1.0739, 1.0645, 1.1292, 1.0302, 1.0804, 1.0459, 1.0506, 1.0580,\n",
      "        1.0772, 1.0868, 1.0476, 1.0388, 1.0627, 1.1022, 1.0526, 1.0761, 1.0509,\n",
      "        1.0698, 1.0465, 1.0165, 1.0643, 1.0420, 1.0936, 1.0807, 1.0737, 1.0721,\n",
      "        1.0657, 1.0433, 1.1175, 1.0729, 1.1148, 1.0733, 1.0650, 1.1116, 1.0535,\n",
      "        1.0558, 1.0894, 1.0698, 1.0856, 1.0981, 1.1126, 1.0848, 1.1226, 1.0476,\n",
      "        1.0584, 1.0810, 1.0570, 1.0946, 1.0703, 1.1243, 1.0933, 1.0815, 1.0935,\n",
      "        1.0443, 1.0913, 1.0623, 1.0801, 1.0687, 1.1020, 1.0818, 1.0493, 1.0772,\n",
      "        1.0247, 1.0790, 1.0959, 1.0866, 1.0842, 1.0525, 1.0348, 1.0587, 1.0980,\n",
      "        1.0704, 1.0870, 1.0229, 1.0635, 1.0514, 1.1175, 1.1091, 1.0512, 1.1043,\n",
      "        1.0645, 1.0539, 1.0839, 1.0627, 1.0805, 1.1096, 1.0489, 1.0336, 1.0841,\n",
      "        1.0873, 1.0860, 1.1055, 1.0654, 1.0862, 0.9907, 1.1054, 1.0718, 1.0442,\n",
      "        1.0447, 1.1030, 1.0714, 1.0698, 1.0567, 1.0150, 1.0586, 1.0929, 1.0627,\n",
      "        1.0240, 1.0025, 1.0877, 1.0795, 1.0358, 1.0918, 1.1030, 1.0876, 1.0521,\n",
      "        1.0518, 1.0729, 1.1203, 1.0556, 1.1076, 1.0885, 1.0346, 1.0846, 1.0358,\n",
      "        1.0822, 1.1065, 1.0706, 1.0744, 1.0541, 1.0603, 1.0676, 1.0301, 1.0764,\n",
      "        1.0823, 1.1184, 1.0908, 1.0279, 1.0856, 1.0532, 1.0906, 1.0517, 1.0927,\n",
      "        1.0738, 1.1094, 1.0904, 1.0742, 1.0900, 1.0831, 1.0111, 1.0649, 1.1051,\n",
      "        1.0290, 1.1085, 1.0452, 1.0810, 1.0873, 1.0647, 1.0183, 1.0940, 1.0649,\n",
      "        1.0491, 1.0893, 1.0781, 1.0968, 1.1113, 1.0955, 1.1329, 1.0838, 0.9928,\n",
      "        1.0456, 1.0852, 1.0841, 1.0612, 1.0525, 1.0696, 1.0469, 1.0601, 1.0289,\n",
      "        1.0276, 1.0675, 1.0702])\n",
      "encoder.4.0.block.0.bias \t tensor([ 0.0025, -0.0152,  0.0046,  0.0125, -0.0015,  0.0121,  0.0283, -0.0158,\n",
      "         0.0164,  0.0263,  0.0274, -0.0307, -0.0117,  0.0182,  0.0039,  0.0200,\n",
      "         0.0053, -0.0218,  0.0104, -0.0067,  0.0150, -0.0097,  0.0143,  0.0314,\n",
      "        -0.0007, -0.0148,  0.0081,  0.0148, -0.0125,  0.0101,  0.0104, -0.0148,\n",
      "         0.0163, -0.0266,  0.0091, -0.0031,  0.0106,  0.0089, -0.0107,  0.0475,\n",
      "        -0.0235,  0.0269,  0.0096, -0.0053,  0.0292, -0.0071, -0.0025,  0.0017,\n",
      "        -0.0283, -0.0025, -0.0066, -0.0110,  0.0058, -0.0022,  0.0201,  0.0045,\n",
      "        -0.0054, -0.0313, -0.0120,  0.0080,  0.0276,  0.0128, -0.0242, -0.0236,\n",
      "        -0.0137, -0.0013,  0.0067,  0.0024,  0.0070, -0.0086,  0.0324,  0.0346,\n",
      "         0.0277,  0.0130, -0.0298,  0.0353, -0.0138, -0.0330,  0.0064,  0.0239,\n",
      "        -0.0037,  0.0066, -0.0197, -0.0247,  0.0072,  0.0247, -0.0259, -0.0261,\n",
      "        -0.0164, -0.0276,  0.0174, -0.0101,  0.0107,  0.0164, -0.0110, -0.0117,\n",
      "        -0.0224, -0.0163,  0.0094, -0.0267, -0.0085,  0.0215,  0.0015,  0.0090,\n",
      "        -0.0002,  0.0163, -0.0230, -0.0074, -0.0210,  0.0095, -0.0136,  0.0038,\n",
      "         0.0205,  0.0355, -0.0136,  0.0118, -0.0268, -0.0109,  0.0298,  0.0109,\n",
      "        -0.0132,  0.0100,  0.0162,  0.0112,  0.0021, -0.0150, -0.0495,  0.0154,\n",
      "        -0.0328,  0.0272, -0.0167, -0.0022,  0.0394,  0.0217,  0.0035,  0.0161,\n",
      "         0.0180,  0.0041,  0.0038, -0.0017,  0.0023,  0.0054, -0.0026,  0.0165,\n",
      "         0.0287, -0.0183, -0.0040, -0.0425,  0.0114,  0.0170,  0.0181, -0.0314,\n",
      "         0.0303,  0.0187, -0.0290,  0.0218, -0.0155, -0.0539, -0.0005,  0.0468,\n",
      "        -0.0350,  0.0081, -0.0224,  0.0235,  0.0143,  0.0298,  0.0201, -0.0353,\n",
      "        -0.0196, -0.0348, -0.0185, -0.0124, -0.0164, -0.0171, -0.0029, -0.0193,\n",
      "         0.0061,  0.0183, -0.0085, -0.0054,  0.0183, -0.0386,  0.0345, -0.0417,\n",
      "         0.0327,  0.0290, -0.0041,  0.0229, -0.0410, -0.0133,  0.0100,  0.0139])\n",
      "encoder.4.0.block.1.queries_projection.scale \t tensor(1.)\n",
      "encoder.4.0.block.1.queries_projection.zero_point \t tensor(0)\n",
      "encoder.4.0.block.1.queries_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.4.0.block.1.queries_projection._packed_params._packed_params \t (tensor([[-0.0258, -0.0223, -0.0120,  ...,  0.0567, -0.0602,  0.0756],\n",
      "        [-0.0636, -0.0223, -0.0413,  ..., -0.0378,  0.0602,  0.0155],\n",
      "        [ 0.0241, -0.0309,  0.0378,  ..., -0.0017, -0.0636,  0.0017],\n",
      "        ...,\n",
      "        [-0.1341,  0.0894, -0.0911,  ..., -0.0292,  0.0464, -0.1135],\n",
      "        [-0.0309, -0.0309,  0.0481,  ..., -0.0602, -0.1032,  0.0034],\n",
      "        [ 0.0653, -0.1083, -0.0860,  ..., -0.0894,  0.0585,  0.0172]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0017192050581797957,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0085,  0.0092, -0.0396, -0.0063,  0.0281, -0.0036,  0.0323, -0.0990,\n",
      "         0.0187, -0.0211,  0.0680, -0.0064, -0.0211,  0.0340, -0.0569,  0.0181,\n",
      "         0.0176, -0.0539,  0.0185,  0.0508,  0.0196,  0.0137,  0.0181,  0.0594,\n",
      "        -0.0042,  0.0049, -0.0661,  0.0049, -0.0078,  0.0640,  0.0274, -0.0091,\n",
      "        -0.0706, -0.0087, -0.0392, -0.0374, -0.0177,  0.0441,  0.0080, -0.0437,\n",
      "        -0.0271,  0.0129,  0.0684, -0.0127,  0.0569,  0.0456, -0.0177, -0.0010,\n",
      "        -0.0429, -0.0090, -0.0047, -0.0487,  0.0426,  0.0374,  0.0375, -0.0101,\n",
      "         0.0063,  0.0618,  0.0546,  0.0385,  0.0693,  0.0082,  0.0567, -0.0197,\n",
      "         0.0104, -0.0011, -0.0052, -0.0690,  0.0603, -0.0854,  0.0409, -0.0667,\n",
      "        -0.0396, -0.0572, -0.0090,  0.0229,  0.0051, -0.0180, -0.0222, -0.0316,\n",
      "        -0.0496,  0.0484, -0.0672, -0.0002,  0.0479,  0.0394, -0.0242,  0.0300,\n",
      "         0.0180, -0.0123,  0.0378,  0.0362, -0.0375, -0.0208, -0.0578, -0.0413,\n",
      "         0.0275,  0.0223, -0.0223, -0.0018,  0.0031, -0.0555,  0.0172, -0.0840,\n",
      "        -0.0387,  0.0616, -0.0461,  0.0025, -0.0764,  0.0410,  0.0519,  0.0071,\n",
      "         0.0673, -0.0582,  0.0280,  0.1032, -0.0269,  0.0555, -0.0426,  0.0143,\n",
      "        -0.0240,  0.1304,  0.0872, -0.0440,  0.0371,  0.1125, -0.0184, -0.0687,\n",
      "        -0.0144, -0.0787, -0.0543, -0.0146,  0.0086, -0.1326, -0.0531,  0.0690,\n",
      "        -0.0124,  0.1505, -0.0902, -0.0771, -0.0068, -0.0314, -0.0606, -0.0524,\n",
      "         0.0710,  0.0781,  0.0447,  0.0008, -0.0325,  0.0495,  0.0264,  0.0814,\n",
      "         0.0611, -0.0461,  0.0302,  0.0251, -0.0282,  0.0260, -0.0608, -0.0475,\n",
      "         0.0171, -0.0080, -0.0208, -0.0260, -0.0168,  0.0765,  0.0197,  0.0211,\n",
      "         0.0372, -0.0959, -0.0631,  0.0448,  0.0623,  0.0287,  0.0450,  0.0315,\n",
      "         0.0336, -0.0685,  0.0634, -0.0034, -0.0718,  0.0380,  0.0075, -0.0760,\n",
      "        -0.0020,  0.0143, -0.0317, -0.0266, -0.0432,  0.0066, -0.0974, -0.0178],\n",
      "       requires_grad=True))\n",
      "encoder.4.0.block.1.values_projection.scale \t tensor(1.)\n",
      "encoder.4.0.block.1.values_projection.zero_point \t tensor(0)\n",
      "encoder.4.0.block.1.values_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.4.0.block.1.values_projection._packed_params._packed_params \t (tensor([[-0.0247, -0.0449,  0.0314,  ...,  0.0650,  0.0000, -0.0493],\n",
      "        [ 0.0763, -0.0807, -0.0987,  ..., -0.0381, -0.0538,  0.0807],\n",
      "        [-0.0112,  0.0045, -0.0112,  ..., -0.0157, -0.0112, -0.0224],\n",
      "        ...,\n",
      "        [ 0.0359,  0.0292,  0.0067,  ...,  0.1076,  0.0359,  0.0852],\n",
      "        [ 0.0045,  0.0942, -0.1099,  ...,  0.0650, -0.1413,  0.0583],\n",
      "        [ 0.0135,  0.0224, -0.0673,  ..., -0.0314, -0.0292, -0.0224]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.002242662012577057,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-4.8011e-02, -5.7966e-04, -3.0337e-02, -4.8949e-02, -1.0374e-02,\n",
      "         4.9731e-02,  2.1908e-02,  3.0104e-02,  1.7515e-03,  6.3380e-02,\n",
      "        -1.0148e-02, -2.4967e-02,  3.6602e-02,  1.1918e-02,  2.8438e-03,\n",
      "        -3.5834e-03, -5.2405e-02,  9.9229e-03,  4.9465e-02,  6.9462e-02,\n",
      "        -3.6450e-02,  1.5328e-02, -1.4831e-02, -5.7250e-02,  1.9970e-02,\n",
      "        -6.1588e-02, -3.9940e-02,  1.0706e-03, -4.6214e-02,  2.3395e-02,\n",
      "        -4.7483e-02, -6.0955e-02, -5.7062e-02,  1.2593e-02, -1.8932e-03,\n",
      "         1.3098e-02, -2.3507e-02, -6.7928e-02, -3.9987e-02, -4.4460e-05,\n",
      "        -3.7199e-02, -2.3861e-02,  4.8201e-02, -8.8446e-03, -5.9407e-02,\n",
      "        -8.3707e-03,  2.6306e-02,  2.5476e-02,  1.3296e-02, -1.4447e-04,\n",
      "        -5.5265e-02,  4.1990e-02,  3.5565e-02,  4.4077e-02, -5.6877e-02,\n",
      "        -1.7514e-02,  4.1700e-02, -1.2039e-02, -1.5877e-02, -4.2384e-02,\n",
      "        -3.8398e-02, -3.6178e-02, -3.0639e-02, -3.3090e-02,  5.7623e-02,\n",
      "        -4.9679e-02, -3.5252e-02, -2.5294e-03,  5.1246e-02, -4.0382e-03,\n",
      "        -3.7326e-02, -7.1386e-02,  3.7522e-02,  9.0169e-03, -6.2248e-02,\n",
      "         1.6676e-02,  2.4347e-02, -4.4345e-02,  2.4759e-02, -4.5298e-02,\n",
      "        -2.5563e-02, -4.7392e-02,  7.2156e-02, -4.2536e-02, -6.3024e-02,\n",
      "        -5.3872e-02,  3.8378e-02, -1.3743e-03, -7.8411e-03, -1.1280e-02,\n",
      "         2.2352e-02, -2.5866e-02, -4.9475e-02, -5.3390e-03, -6.1177e-02,\n",
      "        -2.7095e-02,  3.4059e-02, -3.3110e-02, -1.0852e-02,  5.9587e-02,\n",
      "         5.7998e-02,  4.7020e-02,  3.4747e-02,  2.2949e-03, -4.0624e-02,\n",
      "        -6.6506e-02,  7.5745e-02,  5.8271e-02,  8.3864e-02, -4.7994e-02,\n",
      "         2.5960e-02,  1.3084e-03,  7.5474e-02, -5.0564e-02, -2.6862e-02,\n",
      "        -5.6518e-02, -3.2867e-02, -3.3478e-02,  6.7387e-02,  9.7678e-03,\n",
      "        -5.0131e-02,  2.0473e-02,  2.0286e-03, -1.5400e-03,  1.1513e-02,\n",
      "         4.0986e-02, -4.4965e-02, -1.8727e-02, -5.9600e-02,  2.5915e-02,\n",
      "        -6.3420e-02, -5.3580e-02,  3.0462e-02, -4.5679e-02,  1.8510e-02,\n",
      "         6.4449e-02, -2.0885e-03, -4.9504e-02,  3.3658e-02, -2.3439e-04,\n",
      "         6.2878e-03,  1.4228e-02, -6.1248e-02,  3.0127e-02,  5.2069e-02,\n",
      "        -7.2060e-02, -5.6753e-02, -1.7111e-02, -2.4345e-02, -3.9131e-02,\n",
      "         5.8179e-02, -9.1518e-03,  4.6909e-02, -1.8624e-03, -3.9532e-02,\n",
      "         1.8140e-02,  4.7332e-02,  3.7356e-02,  2.6141e-03,  2.5948e-02,\n",
      "        -2.5516e-02, -8.7879e-03, -6.5528e-02, -5.9725e-03,  3.0522e-02,\n",
      "         3.0969e-02, -5.1195e-02, -2.4461e-02, -1.6825e-02, -2.0799e-02,\n",
      "        -6.4294e-02,  1.7903e-02,  2.7853e-02, -3.0784e-02,  3.3337e-02,\n",
      "         6.2380e-02, -3.0738e-03, -3.6056e-03, -5.0158e-02, -3.9210e-02,\n",
      "         4.3848e-02,  7.4806e-02, -6.5757e-02, -1.5930e-02,  3.5785e-02,\n",
      "        -8.1346e-03,  3.9718e-02, -5.8220e-03, -2.2549e-02, -5.8619e-03,\n",
      "         1.6352e-02,  6.2813e-02], requires_grad=True))\n",
      "encoder.4.0.block.1.keys_projection.scale \t tensor(1.)\n",
      "encoder.4.0.block.1.keys_projection.zero_point \t tensor(0)\n",
      "encoder.4.0.block.1.keys_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.4.0.block.1.keys_projection._packed_params._packed_params \t (tensor([[-0.0172, -0.0329,  0.0925,  ..., -0.0549, -0.1410,  0.0501],\n",
      "        [ 0.0439,  0.1661,  0.0392,  ..., -0.0016,  0.0141, -0.0172],\n",
      "        [-0.0031, -0.0580,  0.0251,  ..., -0.0188, -0.0298, -0.0219],\n",
      "        ...,\n",
      "        [ 0.0345, -0.0596,  0.0016,  ...,  0.0439,  0.0094, -0.0596],\n",
      "        [-0.0752, -0.0157, -0.0172,  ..., -0.0407,  0.0831, -0.0549],\n",
      "        [-0.0533,  0.0533,  0.0407,  ..., -0.0345, -0.0266,  0.1034]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0015671472065150738,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-1.0895e-02,  6.3180e-02, -1.8824e-02,  4.2413e-02, -7.8060e-02,\n",
      "        -3.2297e-02,  5.0512e-02,  4.9495e-03,  7.0407e-02,  4.5429e-02,\n",
      "        -4.9630e-02, -1.4955e-02,  4.6166e-02,  6.3973e-02, -4.8292e-03,\n",
      "        -6.4452e-02, -1.0680e-02, -5.6293e-02, -5.1834e-02,  6.1032e-02,\n",
      "         9.9128e-05, -3.3448e-02, -1.2517e-02, -6.8648e-02, -2.8074e-02,\n",
      "         1.0510e-02,  2.7373e-02, -4.6975e-02, -1.2488e-02, -1.2333e-02,\n",
      "         4.8308e-02,  5.2956e-02,  5.7795e-02,  1.4634e-02, -6.1049e-02,\n",
      "         7.3451e-02,  5.6378e-03, -7.1948e-02,  4.8294e-02,  6.8591e-02,\n",
      "        -4.9159e-03, -2.3847e-02, -6.2545e-02, -6.4045e-02, -7.3728e-02,\n",
      "         4.8882e-02,  5.0522e-02,  3.7811e-02, -2.9981e-03, -6.6946e-03,\n",
      "         1.4618e-02, -1.9070e-02,  3.0041e-02, -2.2323e-02, -4.2609e-02,\n",
      "        -7.1109e-02, -4.4767e-02, -5.6789e-02,  3.2727e-02,  1.7662e-04,\n",
      "        -5.8862e-02, -2.3872e-03, -5.5267e-02, -6.6243e-02,  6.8357e-02,\n",
      "        -5.5105e-02,  5.2067e-02, -2.6038e-02,  8.4788e-03, -5.2105e-02,\n",
      "        -2.2646e-02, -4.2898e-02,  1.2597e-02,  1.2665e-02, -5.2620e-03,\n",
      "        -1.7778e-02,  3.3080e-02,  1.6150e-02, -5.9820e-02,  4.0392e-02,\n",
      "        -1.0021e-02, -5.5670e-02, -1.3502e-02, -1.7644e-02, -1.1280e-02,\n",
      "        -1.6667e-02, -1.3686e-03, -7.1891e-02, -5.5492e-02, -6.4565e-02,\n",
      "        -1.9459e-03,  3.1164e-02,  2.0624e-02,  1.7707e-02, -2.0263e-03,\n",
      "        -6.8946e-02, -4.9709e-03, -3.7231e-02,  4.6505e-02, -6.2440e-02,\n",
      "         1.1385e-02, -7.1312e-02, -7.1286e-02, -4.4293e-02, -6.2311e-02,\n",
      "         4.6200e-03,  2.2487e-02, -2.7934e-02,  3.9569e-02,  2.4856e-02,\n",
      "        -7.2979e-03,  2.7133e-02,  6.0515e-02,  5.6164e-02,  5.8726e-02,\n",
      "        -5.3629e-02,  6.9452e-02, -3.3255e-02, -4.1277e-02,  5.3302e-02,\n",
      "        -1.8683e-02, -1.6347e-02, -1.8198e-02,  5.1387e-02, -6.1411e-02,\n",
      "        -5.5332e-03,  4.3173e-02, -7.1052e-02, -5.1737e-02,  1.9461e-02,\n",
      "         6.9978e-03,  1.1267e-02, -4.1184e-02,  4.4253e-02,  4.8978e-02,\n",
      "         3.3327e-02, -4.8285e-02, -1.6034e-02,  6.6164e-03, -1.7011e-02,\n",
      "         5.0277e-02, -8.0391e-03,  1.1841e-02,  1.5540e-02, -2.4747e-02,\n",
      "         5.0144e-02, -2.6321e-02, -4.7094e-02,  7.4690e-02,  2.9800e-02,\n",
      "         3.0880e-02,  1.5931e-02,  5.1248e-02, -3.3680e-02, -3.4930e-02,\n",
      "        -4.2327e-02,  3.8513e-02, -1.5218e-03, -1.1628e-02,  5.8372e-02,\n",
      "        -5.7681e-02, -3.9470e-02, -2.9058e-02,  2.9030e-02,  1.1627e-04,\n",
      "        -1.6428e-02, -4.8160e-03,  3.8349e-02,  5.3687e-02, -2.6655e-03,\n",
      "         1.8334e-02, -7.7491e-02,  1.1893e-02, -1.8671e-02,  2.6421e-02,\n",
      "         7.6476e-02, -1.4163e-02,  2.6484e-02,  1.3270e-02,  2.0723e-03,\n",
      "         2.3009e-02,  6.0288e-03, -5.2416e-02,  6.9460e-02,  2.7248e-02,\n",
      "         7.5377e-02, -4.5069e-03,  7.1861e-02,  7.4475e-02,  4.9075e-02,\n",
      "         2.0328e-02, -5.2914e-02], requires_grad=True))\n",
      "encoder.4.0.block.1.final_projection.scale \t tensor(1.)\n",
      "encoder.4.0.block.1.final_projection.zero_point \t tensor(0)\n",
      "encoder.4.0.block.1.final_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.4.0.block.1.final_projection._packed_params._packed_params \t (tensor([[ 0.1461,  0.0411,  0.0548,  ..., -0.1324,  0.1119,  0.0160],\n",
      "        [ 0.1416, -0.0776,  0.0685,  ...,  0.0822, -0.1073,  0.0616],\n",
      "        [-0.0502,  0.1393, -0.0046,  ...,  0.0479, -0.1872, -0.1781],\n",
      "        ...,\n",
      "        [-0.0731,  0.0753, -0.1256,  ..., -0.0890,  0.1142,  0.0616],\n",
      "        [-0.0502,  0.0434, -0.0479,  ...,  0.0822, -0.1484, -0.1370],\n",
      "        [ 0.0845,  0.0000, -0.0388,  ...,  0.0571,  0.0959,  0.1530]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0022832865361124277,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-1.5919e-02, -5.3035e-02,  6.2691e-02,  6.5314e-02, -3.6672e-02,\n",
      "         6.1238e-02,  5.3430e-02,  5.5790e-03,  4.0288e-02,  5.9723e-02,\n",
      "        -2.0820e-02,  4.1051e-02, -4.3547e-02, -8.0800e-03, -6.7717e-02,\n",
      "        -3.6644e-02,  3.0902e-02, -4.1565e-02,  5.1769e-02,  1.4633e-02,\n",
      "         8.4142e-02, -3.5333e-02,  1.2511e-02, -2.5276e-02, -1.1012e-02,\n",
      "        -1.4448e-02, -7.5029e-03,  2.7096e-02, -6.1842e-02,  1.8886e-02,\n",
      "        -4.0963e-02,  4.4535e-02,  4.1757e-02, -2.4691e-02, -3.6748e-02,\n",
      "        -8.6405e-02,  1.6089e-02,  4.1019e-02, -5.1069e-02,  7.6108e-02,\n",
      "         3.3944e-02,  3.8781e-02, -2.5156e-02,  2.2545e-02, -3.5051e-02,\n",
      "         2.3562e-02, -1.3691e-02,  6.6332e-03,  3.1747e-02,  3.5275e-02,\n",
      "         5.7280e-03,  8.5732e-04, -1.6917e-03, -1.6674e-03,  3.5992e-02,\n",
      "        -4.3428e-02,  3.0393e-02, -5.8704e-02,  1.0400e-02,  6.1094e-03,\n",
      "         7.0711e-02,  3.0088e-02,  1.9951e-02, -8.6785e-02, -1.0105e-02,\n",
      "         4.1660e-02, -2.6266e-02,  4.4509e-02,  3.3581e-02,  7.3850e-02,\n",
      "        -5.8818e-02, -3.5848e-02,  1.0716e-02,  1.5680e-02,  4.3402e-02,\n",
      "         4.2596e-02, -5.8214e-02, -3.2845e-02, -6.2304e-02, -4.7846e-02,\n",
      "        -4.6890e-02,  1.0747e-02, -2.3848e-02,  3.6780e-02,  3.1003e-02,\n",
      "         9.6878e-02, -7.3008e-04, -1.8087e-02,  9.0084e-02, -7.2326e-02,\n",
      "        -6.0185e-03, -3.1969e-02,  7.0707e-03,  5.5190e-02,  8.2620e-03,\n",
      "        -9.0914e-02,  3.2264e-02, -5.0940e-02, -2.3014e-02,  4.6276e-02,\n",
      "         2.3046e-02, -1.7860e-02, -4.9597e-02,  3.9318e-02,  4.0888e-02,\n",
      "        -3.4710e-02,  6.5137e-02, -4.6865e-02, -3.5179e-02, -1.2095e-02,\n",
      "        -6.8766e-02, -7.6698e-02, -2.6189e-02, -2.8482e-02, -2.4007e-02,\n",
      "         5.2600e-02, -7.0855e-02,  5.6326e-02,  5.4250e-02, -1.0432e-02,\n",
      "         2.6315e-02,  4.5576e-02, -3.1834e-02, -4.0264e-02, -2.1831e-02,\n",
      "        -3.1567e-02,  2.2332e-02, -5.2979e-02, -2.5792e-02, -5.9969e-02,\n",
      "        -8.0305e-02, -6.5776e-02, -3.5268e-02, -2.3680e-02,  8.4593e-03,\n",
      "        -6.1291e-02, -4.6596e-02, -1.3383e-02, -2.5937e-02,  7.9925e-02,\n",
      "        -7.0023e-02, -2.6153e-02,  5.2773e-02,  4.2739e-02, -1.6476e-02,\n",
      "        -8.0404e-02, -5.6962e-02, -8.6923e-02, -5.1331e-02, -4.2846e-02,\n",
      "         6.3720e-02, -7.0276e-02,  4.0364e-02,  6.9865e-03,  3.6035e-02,\n",
      "         2.4611e-02, -6.8759e-02, -7.1753e-02,  4.2266e-02, -2.0350e-02,\n",
      "         3.0384e-02, -3.0187e-02,  3.9661e-02, -2.3756e-03, -3.0569e-02,\n",
      "         6.5463e-02, -1.3613e-02, -1.0444e-01, -6.0485e-03,  2.4944e-02,\n",
      "        -4.8350e-02, -2.0102e-02, -1.0401e-04, -2.1513e-02, -6.9843e-02,\n",
      "         3.8748e-02, -8.5604e-02, -3.3416e-02, -8.3617e-02,  4.4005e-02,\n",
      "        -4.1236e-02, -3.1184e-02, -6.6378e-02, -4.5333e-02, -2.9144e-02,\n",
      "        -5.0811e-02,  4.2519e-02,  6.2706e-02, -7.6003e-02,  7.2219e-02,\n",
      "        -5.2235e-02, -6.1404e-02], requires_grad=True))\n",
      "encoder.4.1.block.0.weight \t tensor([0.8385, 0.8729, 0.8614, 0.9557, 0.9089, 0.9396, 0.9707, 0.9610, 0.8626,\n",
      "        0.8980, 0.8756, 0.9733, 0.8909, 0.9172, 0.8736, 0.9056, 0.8776, 0.9417,\n",
      "        0.9588, 0.9246, 0.9377, 0.8978, 0.8972, 0.8855, 0.8216, 0.8997, 0.9275,\n",
      "        0.8598, 0.8622, 0.8710, 0.9307, 0.8626, 0.9285, 0.8541, 0.8815, 0.9460,\n",
      "        0.9331, 0.8715, 0.8788, 0.8776, 0.8782, 0.8623, 0.8724, 0.9348, 0.9126,\n",
      "        0.8783, 0.8718, 0.8721, 1.0108, 0.8399, 0.8359, 0.9153, 0.8770, 0.9188,\n",
      "        0.8759, 0.8592, 0.8911, 0.8944, 0.8594, 0.9404, 0.9097, 0.9237, 0.9029,\n",
      "        0.9585, 0.9252, 0.9094, 0.9308, 0.9252, 0.9353, 0.9370, 0.9915, 0.9212,\n",
      "        0.8549, 0.9098, 0.9347, 0.8771, 0.8864, 0.8992, 0.9496, 0.9144, 0.8847,\n",
      "        0.9492, 0.9383, 0.8932, 0.9183, 0.9333, 0.9471, 0.9240, 0.8692, 0.8388,\n",
      "        0.9197, 0.8932, 0.8780, 0.8864, 0.9304, 0.8962, 0.8437, 0.8946, 0.9136,\n",
      "        0.8573, 0.9171, 0.8501, 0.9585, 0.8601, 0.9281, 0.9313, 0.9301, 0.9406,\n",
      "        0.9368, 0.9016, 0.9110, 0.9369, 0.9205, 0.9038, 0.9001, 0.8786, 0.9471,\n",
      "        0.9333, 0.9534, 0.9131, 0.8604, 0.9343, 0.8587, 0.9129, 0.8458, 0.9173,\n",
      "        0.8971, 0.8816, 0.9070, 0.8510, 0.9100, 0.9398, 0.9074, 0.8765, 0.9260,\n",
      "        0.8522, 0.8750, 0.8942, 0.9066, 0.8622, 0.8860, 0.8762, 0.9268, 0.8550,\n",
      "        0.9161, 0.9467, 0.9027, 0.9041, 0.9509, 0.9260, 0.8790, 0.8511, 0.8736,\n",
      "        0.9153, 0.8671, 0.8527, 0.9455, 0.8596, 0.8448, 0.9190, 0.9242, 0.8798,\n",
      "        0.9401, 0.8828, 0.8780, 0.8835, 0.9271, 0.9660, 0.9342, 0.9509, 0.9452,\n",
      "        0.9290, 0.9359, 0.8991, 0.8639, 0.8667, 0.9374, 0.9321, 0.8572, 0.9280,\n",
      "        0.9599, 0.9640, 0.9253, 0.8569, 0.9147, 0.9371, 0.9393, 0.8833, 0.9399,\n",
      "        0.8802, 0.8939, 0.8750])\n",
      "encoder.4.1.block.0.bias \t tensor([ 0.0807, -0.0234, -0.0325, -0.0062, -0.0243,  0.0284, -0.0043,  0.0654,\n",
      "        -0.0232,  0.0229, -0.0478,  0.0081, -0.0369, -0.0519, -0.0039, -0.0131,\n",
      "        -0.0307, -0.0584,  0.0418,  0.0549, -0.0339,  0.0116, -0.0144, -0.0388,\n",
      "         0.0113, -0.0276, -0.0366, -0.0021,  0.0181,  0.0022, -0.0350,  0.0040,\n",
      "        -0.0044,  0.0394, -0.0175,  0.0222,  0.0026, -0.0273,  0.0370, -0.0144,\n",
      "         0.0048, -0.0335, -0.0179,  0.0184, -0.0301,  0.0088,  0.0386, -0.0112,\n",
      "         0.0385,  0.0080,  0.0232, -0.0008,  0.0174, -0.0393, -0.0208, -0.0079,\n",
      "         0.0407, -0.0420, -0.0413, -0.0227, -0.0356, -0.0010, -0.0087,  0.0058,\n",
      "         0.0498, -0.0033, -0.0274,  0.0190,  0.0084, -0.0378, -0.0522, -0.0427,\n",
      "        -0.0280, -0.0230,  0.0264, -0.0439, -0.0116,  0.0088, -0.0210,  0.0270,\n",
      "        -0.0090, -0.0282,  0.0128,  0.0359, -0.0238, -0.0303,  0.0096,  0.0085,\n",
      "        -0.0030,  0.0396, -0.0335,  0.0319,  0.0178,  0.0436, -0.0205,  0.0330,\n",
      "         0.0577, -0.0139, -0.0467, -0.0256, -0.0259, -0.0048, -0.0350, -0.0020,\n",
      "         0.0403,  0.0376,  0.0160,  0.0209,  0.0228,  0.0648,  0.0359, -0.0138,\n",
      "        -0.0022, -0.0552, -0.0127,  0.0054,  0.0187,  0.0340, -0.0443,  0.0375,\n",
      "        -0.0437,  0.0013, -0.0228,  0.0199, -0.0192, -0.0387,  0.0107, -0.0092,\n",
      "         0.0106, -0.0133,  0.0120, -0.0340, -0.0258, -0.0424,  0.0133, -0.0023,\n",
      "        -0.0360,  0.0329, -0.0255, -0.0048, -0.0396,  0.0054,  0.0171, -0.0356,\n",
      "        -0.0440,  0.0184,  0.0517,  0.0408, -0.0132,  0.0081, -0.0655, -0.0013,\n",
      "        -0.0409, -0.0433,  0.0026,  0.0060,  0.0031, -0.0388,  0.0090, -0.0461,\n",
      "         0.0380,  0.0024, -0.0332, -0.0176, -0.0263, -0.0145, -0.0332,  0.0185,\n",
      "        -0.0048,  0.0418, -0.0362,  0.0054, -0.0210, -0.0085,  0.0601,  0.0263,\n",
      "         0.0133, -0.0051, -0.0084,  0.0340, -0.0205,  0.0466,  0.0080,  0.0484,\n",
      "        -0.0058, -0.0311, -0.0158, -0.0391,  0.0375,  0.0560, -0.0085, -0.0150])\n",
      "encoder.4.1.block.1.0.scale \t tensor(1.)\n",
      "encoder.4.1.block.1.0.zero_point \t tensor(0)\n",
      "encoder.4.1.block.1.0._packed_params.dtype \t torch.qint8\n",
      "encoder.4.1.block.1.0._packed_params._packed_params \t (tensor([[ 0.0123, -0.0055, -0.0219,  ...,  0.0096,  0.0507, -0.0096],\n",
      "        [-0.0137,  0.0493, -0.0740,  ...,  0.0438, -0.0164, -0.0603],\n",
      "        [-0.0562, -0.0288,  0.0315,  ...,  0.0027, -0.0192, -0.0356],\n",
      "        ...,\n",
      "        [-0.0329, -0.0082,  0.0014,  ..., -0.0288, -0.0206, -0.0617],\n",
      "        [ 0.0027,  0.0164,  0.0096,  ..., -0.0480, -0.0288, -0.0658],\n",
      "        [-0.0781, -0.0027, -0.0027,  ...,  0.0356, -0.0671,  0.0685]],\n",
      "       size=(768, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0013701356947422028,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 5.9283e-03, -7.6263e-02,  7.6230e-03, -2.5546e-02, -1.4180e-03,\n",
      "        -3.8100e-02,  5.9794e-03,  1.9749e-02, -3.0851e-02,  3.3257e-03,\n",
      "        -5.3829e-03, -6.1530e-02,  1.0541e-02, -8.0617e-02,  2.2182e-02,\n",
      "        -4.6406e-02, -3.1358e-02,  1.5187e-02, -6.7081e-02,  7.3424e-03,\n",
      "        -5.9086e-02,  1.6962e-02, -4.2203e-02, -5.7394e-02, -8.5536e-02,\n",
      "         8.2324e-03,  1.7470e-02, -1.0911e-04, -1.0162e-02, -4.3864e-02,\n",
      "         2.0036e-03, -6.5606e-02, -3.6355e-02,  8.8280e-03,  2.6890e-02,\n",
      "        -2.2328e-02, -1.0044e-01, -3.1594e-02, -9.8318e-02, -2.6969e-02,\n",
      "        -3.0436e-02, -8.1517e-02, -2.3958e-02, -5.2485e-02,  5.8960e-03,\n",
      "        -9.7464e-02,  1.0344e-02, -4.7023e-02, -1.2209e-02, -1.7607e-02,\n",
      "        -4.0378e-02,  5.9255e-03,  2.0205e-02, -6.7333e-03, -2.8638e-03,\n",
      "        -7.6838e-02, -6.8991e-02,  2.9285e-03, -3.5251e-02, -5.2925e-02,\n",
      "        -8.4744e-02, -6.1017e-02,  1.9469e-02, -3.8539e-02,  9.1911e-03,\n",
      "         5.9330e-03, -1.1411e-02, -4.1730e-02, -7.7774e-02,  1.3910e-03,\n",
      "        -1.0504e-01, -8.8088e-02, -4.8445e-02, -1.1491e-01, -6.6753e-02,\n",
      "        -9.8080e-02, -1.0976e-01, -8.0398e-02, -8.0631e-02, -6.1477e-02,\n",
      "        -6.9906e-02, -6.2359e-04, -7.2160e-02, -1.6936e-02, -4.1368e-02,\n",
      "         3.3375e-02,  5.5508e-03, -1.4550e-02, -2.7933e-02, -1.5097e-03,\n",
      "        -5.8410e-02, -9.2768e-02, -4.0278e-02,  2.3524e-02,  7.0424e-03,\n",
      "        -1.5458e-02, -8.7848e-02,  6.3445e-03, -7.4892e-02, -6.0130e-02,\n",
      "         4.3760e-02, -1.6118e-03, -6.7975e-02, -2.4170e-02, -9.4648e-03,\n",
      "        -6.6331e-02, -7.2707e-02, -3.7388e-02,  1.4013e-02,  2.6490e-02,\n",
      "         4.1121e-03, -4.2711e-02, -5.2511e-02,  8.7939e-03, -4.6624e-02,\n",
      "        -8.5318e-02, -2.9740e-03,  1.8782e-02, -7.7350e-02, -1.8091e-02,\n",
      "        -3.6270e-02, -5.7535e-02, -2.7202e-02, -5.7586e-02,  1.7391e-02,\n",
      "         1.3774e-02,  3.4208e-02, -1.4145e-02, -1.7288e-02, -3.0912e-02,\n",
      "         2.1739e-02, -8.3228e-02,  2.6760e-03, -1.8404e-02, -1.0363e-03,\n",
      "        -1.9665e-02, -2.0569e-02, -5.6672e-02,  3.1975e-02,  2.1403e-02,\n",
      "        -9.7199e-02, -5.7248e-02,  2.6143e-02, -6.5446e-02, -4.9618e-02,\n",
      "         4.4070e-02, -5.0671e-02, -6.2031e-02, -5.8897e-02,  2.9958e-02,\n",
      "         3.1380e-02, -9.4416e-02, -6.0827e-03,  3.1590e-02, -5.6856e-02,\n",
      "        -9.7435e-02, -1.8695e-02, -4.6817e-03,  1.7288e-02,  1.8753e-02,\n",
      "        -5.3807e-03, -3.1851e-02, -7.0818e-02, -6.1314e-02, -4.8082e-02,\n",
      "         4.4050e-03, -3.3280e-02,  1.2370e-02, -4.4692e-02,  4.1314e-03,\n",
      "        -2.4631e-02, -2.4769e-02, -1.0933e-01,  3.6022e-02, -1.3539e-02,\n",
      "         1.9828e-02, -8.4821e-02, -5.3274e-02, -4.6174e-02, -4.9750e-02,\n",
      "        -4.9337e-02, -4.7188e-02, -5.1426e-02,  7.8221e-03, -2.0532e-02,\n",
      "        -3.5128e-03,  1.2057e-02, -2.7691e-02,  1.7865e-02, -1.5146e-03,\n",
      "         2.2433e-02, -9.1100e-02,  1.2246e-02,  3.0230e-02,  2.2457e-03,\n",
      "        -3.8582e-02,  3.2134e-02, -1.0369e-01, -7.0708e-02, -5.5700e-03,\n",
      "        -9.8836e-02, -8.3797e-02, -2.6666e-02, -6.6059e-02, -8.1511e-02,\n",
      "        -3.0031e-02, -1.7330e-02, -1.1283e-01, -6.8887e-02, -6.8145e-02,\n",
      "        -1.7866e-02, -9.5589e-02, -9.6811e-02, -9.4041e-02, -4.1251e-02,\n",
      "        -9.6980e-02, -2.4950e-02, -9.9772e-02, -2.6645e-02, -7.4392e-02,\n",
      "         7.4878e-03, -6.8282e-02,  1.2673e-02, -2.1025e-02, -6.6168e-02,\n",
      "        -9.1036e-02, -7.0887e-02, -1.5157e-02,  5.1705e-03, -3.6150e-02,\n",
      "        -2.4950e-02, -8.8856e-02,  8.4670e-03, -4.3265e-02, -8.6488e-02,\n",
      "         2.3719e-02, -9.1754e-02,  5.7884e-03, -1.6800e-02,  1.5435e-02,\n",
      "        -7.9444e-02, -7.2321e-02, -1.2671e-02,  2.9205e-02,  1.8932e-02,\n",
      "        -3.7559e-02, -1.7584e-02,  8.8594e-04, -2.7016e-03, -1.9094e-02,\n",
      "        -4.7287e-02,  2.9358e-02, -1.7833e-02, -1.0649e-03, -2.2165e-02,\n",
      "        -6.2967e-02, -1.6179e-02, -6.8927e-02, -1.1954e-02, -5.9513e-02,\n",
      "        -7.1871e-02, -6.5097e-02, -1.8029e-02, -3.7327e-03, -7.3750e-02,\n",
      "        -1.0143e-02, -9.2270e-02, -8.0312e-02, -4.1155e-02, -4.0370e-02,\n",
      "         1.0189e-02,  1.7982e-02,  1.1024e-02, -5.2531e-02,  6.4183e-03,\n",
      "        -1.0404e-02,  1.2592e-02, -7.3922e-02, -3.5292e-02, -7.1487e-02,\n",
      "        -3.8851e-02, -2.6994e-03, -9.3060e-02,  2.2503e-02,  1.0675e-02,\n",
      "        -3.8161e-02, -3.7928e-03,  1.9259e-02, -9.3371e-02, -3.7610e-02,\n",
      "         2.3730e-02, -5.2844e-02, -4.7204e-02, -5.9031e-02,  2.3028e-02,\n",
      "         1.7253e-02,  1.2088e-02, -4.2676e-02, -5.6987e-02, -4.3782e-02,\n",
      "        -9.4581e-02, -6.3570e-02, -8.2317e-02, -3.8757e-02, -7.4511e-02,\n",
      "        -8.4827e-02, -9.9511e-02, -5.4494e-02, -9.1144e-02, -4.6056e-02,\n",
      "        -5.8651e-02, -8.6591e-03, -1.5978e-02,  1.2825e-02, -4.5897e-02,\n",
      "        -2.6543e-02, -3.8533e-02, -1.0317e-01,  3.8587e-03,  1.3927e-02,\n",
      "        -9.6195e-02,  2.7525e-03,  1.8611e-02, -5.9915e-02, -7.6725e-02,\n",
      "        -3.2872e-02, -8.4358e-02, -3.1767e-02, -1.5878e-02, -9.4168e-02,\n",
      "         3.4407e-04, -3.5816e-02, -7.0708e-02, -6.5007e-02, -9.2710e-02,\n",
      "         2.1389e-02, -1.4068e-02, -3.5207e-02, -8.4330e-02, -8.9934e-02,\n",
      "         2.1728e-03, -8.9939e-02, -1.3933e-02,  4.2125e-04, -1.2097e-02,\n",
      "        -7.2461e-02, -6.4872e-02,  3.0688e-02,  5.7413e-03, -8.4497e-02,\n",
      "        -7.4515e-02, -9.4635e-03, -6.4947e-02,  7.3993e-03, -2.3873e-02,\n",
      "        -9.0003e-02,  2.8305e-02, -1.0518e-01, -9.3847e-02, -2.8033e-02,\n",
      "        -2.7245e-02, -7.2262e-02,  1.4790e-02, -9.0215e-02, -3.0856e-02,\n",
      "         1.4033e-02,  4.1112e-02, -3.2966e-02,  1.8126e-02, -6.8341e-02,\n",
      "        -8.0056e-03, -4.9894e-02, -1.6712e-03,  2.4116e-02, -3.6600e-02,\n",
      "        -6.0905e-02, -1.1800e-01, -4.5238e-02, -3.6913e-02, -5.1338e-02,\n",
      "         2.9368e-02, -6.0366e-02,  1.2541e-02, -2.9498e-02, -6.1825e-03,\n",
      "         6.3483e-04, -6.4825e-02, -8.5272e-02,  1.2152e-02,  2.1771e-02,\n",
      "        -1.9930e-02, -1.9102e-02, -8.2426e-03,  1.2199e-02, -3.4708e-02,\n",
      "         2.4042e-02, -6.9987e-02, -1.0069e-02, -5.9013e-02, -3.8488e-02,\n",
      "        -7.3505e-02,  1.4056e-02, -1.7114e-02, -1.6265e-02, -3.2868e-02,\n",
      "         2.1971e-02,  2.7929e-02, -6.8524e-02, -9.6882e-02, -9.2641e-02,\n",
      "         7.3746e-03, -8.4991e-02,  6.5533e-04, -2.5810e-02, -3.5264e-02,\n",
      "         9.2928e-03,  2.9091e-03, -2.9388e-02, -6.4140e-02,  2.5339e-02,\n",
      "        -6.8475e-02, -7.5150e-02, -6.2811e-02, -1.0230e-02, -3.6128e-02,\n",
      "        -5.2560e-02, -8.6828e-02, -6.2875e-02, -5.2734e-02, -5.3111e-02,\n",
      "         2.3702e-02, -1.3022e-03, -3.5522e-02,  2.9803e-02,  2.7291e-03,\n",
      "        -4.0458e-02, -9.0759e-02, -1.0116e-02, -3.3206e-02, -4.0259e-02,\n",
      "        -1.0116e-02, -3.1774e-02,  1.0121e-02,  2.2708e-02, -6.5602e-02,\n",
      "        -3.7625e-02, -1.0404e-01,  8.1030e-03,  3.6004e-02, -8.6032e-02,\n",
      "        -7.1032e-02, -6.2373e-02, -5.6406e-02,  1.1899e-02, -3.4560e-02,\n",
      "         8.0458e-03, -8.7768e-02, -6.7496e-02, -4.8509e-02,  1.6495e-02,\n",
      "        -2.8919e-02, -8.4653e-03,  2.4843e-02, -1.0669e-01,  2.0702e-02,\n",
      "        -3.8343e-02, -4.9689e-02,  2.3289e-03, -8.4555e-02, -7.3414e-02,\n",
      "        -3.6200e-02, -2.9743e-02, -3.9726e-02,  6.7178e-03, -5.9332e-02,\n",
      "        -6.2196e-02, -9.0183e-02, -8.4253e-02, -6.6069e-03,  3.0811e-02,\n",
      "         2.6415e-02,  2.7345e-02,  6.1846e-03,  1.9950e-02, -7.3696e-02,\n",
      "        -4.9806e-02, -6.9620e-02, -3.3029e-02, -2.9896e-02,  2.2923e-02,\n",
      "        -6.2340e-02, -6.8387e-02,  2.8303e-02, -6.2487e-02, -2.3818e-03,\n",
      "        -6.3737e-02,  1.5163e-02, -6.6810e-02,  3.0828e-02,  7.1262e-03,\n",
      "        -3.8654e-02,  2.5211e-02, -7.2221e-02, -7.4758e-02, -3.6787e-02,\n",
      "        -4.5481e-02, -1.5001e-02, -7.8471e-02,  3.5744e-03,  2.5067e-02,\n",
      "        -9.8390e-02, -6.2930e-02, -8.3106e-03,  7.1775e-03, -2.1932e-02,\n",
      "         1.9272e-02, -8.6708e-02, -1.3848e-02, -1.7821e-02, -2.0627e-02,\n",
      "        -2.7963e-02, -2.5542e-02,  1.4125e-03, -3.3937e-02, -6.5076e-02,\n",
      "        -2.1079e-02, -4.3083e-02, -4.8611e-02, -5.4046e-02, -7.7959e-03,\n",
      "        -8.5239e-02, -4.3522e-02, -8.0764e-02, -9.6838e-02, -4.9773e-02,\n",
      "        -1.7562e-02, -4.8222e-02, -1.1585e-02, -5.7397e-03, -9.0427e-02,\n",
      "         8.2807e-03, -1.3722e-02,  2.3991e-02, -5.9268e-02, -4.0964e-02,\n",
      "        -9.4298e-02, -9.3542e-02, -3.2900e-02,  2.2641e-03, -8.0868e-02,\n",
      "         1.3159e-02,  1.9598e-02, -9.2014e-03, -2.0284e-02, -3.3705e-02,\n",
      "        -4.0358e-02,  1.2645e-02, -7.0626e-02, -9.7005e-02, -7.6351e-02,\n",
      "        -5.7155e-02, -2.6777e-02, -1.4637e-03, -7.2865e-02, -3.6084e-02,\n",
      "        -8.9764e-02,  3.4748e-03,  2.2887e-02, -6.0434e-02, -3.2527e-03,\n",
      "        -1.3261e-02, -7.2077e-02, -1.0377e-01, -7.8646e-02, -7.0744e-02,\n",
      "         2.8411e-03,  1.7460e-02, -1.3391e-02, -2.7954e-02, -6.1522e-02,\n",
      "        -3.5179e-02,  6.6121e-03, -1.2621e-02, -3.8751e-02, -6.1115e-02,\n",
      "        -7.8686e-02,  5.1379e-03, -1.0126e-02, -5.2801e-02, -6.9202e-02,\n",
      "         2.1487e-02,  8.1026e-03, -1.0118e-02, -8.9952e-03, -4.6288e-02,\n",
      "         1.9066e-02,  2.2886e-02, -7.0530e-02, -6.8495e-02, -9.4693e-02,\n",
      "         1.7523e-02, -5.4683e-03, -2.0616e-02, -3.6564e-02, -5.3499e-02,\n",
      "        -7.7889e-02,  1.6808e-02, -3.7823e-02, -5.9270e-02,  2.0330e-02,\n",
      "        -7.3746e-02, -3.9638e-02, -6.8944e-02, -1.2875e-02, -4.2731e-03,\n",
      "        -2.9531e-02, -2.9305e-03, -5.3942e-02, -8.9243e-04, -8.5041e-02,\n",
      "        -1.4467e-02, -3.0631e-02, -6.7832e-02, -1.6736e-02,  2.0105e-02,\n",
      "        -7.8215e-04, -4.2794e-02, -5.2724e-02, -4.0637e-02, -8.5175e-02,\n",
      "         1.6653e-02, -7.4964e-03, -9.4389e-03, -6.4213e-02,  1.1975e-02,\n",
      "        -6.3527e-02,  1.2592e-02, -7.8611e-02,  1.6907e-02, -3.3881e-02,\n",
      "        -6.3813e-02,  1.0997e-02,  7.8993e-03, -1.0325e-01, -1.0605e-01,\n",
      "        -5.6901e-02,  9.5511e-03, -8.8506e-03,  1.5537e-02, -5.6861e-02,\n",
      "        -7.0053e-02, -2.5571e-02, -1.6563e-03, -3.7488e-02, -1.7555e-02,\n",
      "        -7.7019e-02, -2.0825e-02, -1.0082e-01, -4.7115e-02, -3.3478e-02,\n",
      "        -3.6238e-02, -9.6538e-02, -3.3713e-02,  9.0900e-03, -1.0476e-01,\n",
      "        -6.5003e-03,  3.5205e-02,  6.8724e-03, -4.5064e-03, -2.0967e-02,\n",
      "        -8.4263e-02, -5.0193e-02, -1.0756e-01, -2.6459e-02, -6.8015e-02,\n",
      "        -4.4976e-02, -4.5251e-02, -2.5928e-02, -4.8616e-03, -5.2000e-02,\n",
      "        -4.7593e-02, -2.8106e-02,  2.6782e-02, -6.1393e-02, -5.9589e-02,\n",
      "        -7.3059e-02, -1.0096e-01, -3.0381e-02, -7.5148e-02, -6.4836e-02,\n",
      "        -4.7744e-02, -5.3063e-02, -7.8869e-02, -3.8349e-02, -5.8375e-02,\n",
      "        -4.6103e-02, -2.9820e-02, -1.7967e-02,  8.1730e-03, -8.7416e-02,\n",
      "        -4.5638e-02,  2.4327e-02,  7.9205e-03,  2.7178e-02,  1.4676e-02,\n",
      "        -6.3593e-02, -5.6491e-04, -8.1712e-02, -6.9300e-03,  1.2577e-02,\n",
      "        -6.1516e-02, -1.4638e-02, -1.0198e-01, -7.5524e-03, -3.7102e-02,\n",
      "         1.7004e-02, -2.5362e-02, -3.8859e-02, -5.6853e-02, -5.0830e-02,\n",
      "        -3.9419e-02, -7.8377e-02, -9.8076e-03, -4.2423e-02, -6.9248e-02,\n",
      "        -6.5302e-02, -7.1933e-02,  2.5426e-02, -1.6649e-02, -1.0216e-01,\n",
      "        -6.1570e-02, -4.9148e-02, -4.4837e-02, -3.5629e-02,  2.2725e-02,\n",
      "        -4.6060e-02, -7.7105e-02,  2.2409e-02, -6.9672e-02, -6.3325e-02,\n",
      "        -3.4540e-02, -5.3388e-02, -8.8525e-02, -8.4924e-02, -3.8903e-02,\n",
      "        -1.2946e-02,  3.2330e-02,  3.2452e-02, -8.8830e-02,  1.0759e-03,\n",
      "        -3.8236e-03, -9.7457e-02, -3.6999e-02, -1.0790e-01, -7.3766e-02,\n",
      "        -4.1789e-02,  1.8265e-02, -7.3338e-02, -4.9835e-02,  4.8786e-03,\n",
      "        -7.9645e-03, -5.4261e-02,  4.5548e-04, -9.0008e-02, -2.7855e-02,\n",
      "         3.2485e-02, -5.0318e-02, -1.0287e-02], requires_grad=True))\n",
      "encoder.4.1.block.1.2.scale \t tensor(1.)\n",
      "encoder.4.1.block.1.2.zero_point \t tensor(0)\n",
      "encoder.4.1.block.1.2._packed_params.dtype \t torch.qint8\n",
      "encoder.4.1.block.1.2._packed_params._packed_params \t (tensor([[-0.0096,  0.0376, -0.0029,  ...,  0.0067,  0.0096, -0.0087],\n",
      "        [ 0.0144, -0.0116,  0.0356,  ..., -0.0096,  0.0096,  0.0096],\n",
      "        [-0.0125,  0.0212,  0.0356,  ...,  0.0279, -0.0289,  0.0472],\n",
      "        ...,\n",
      "        [-0.0356, -0.0125, -0.0039,  ...,  0.0231, -0.0279,  0.0029],\n",
      "        [ 0.0298,  0.0173, -0.0279,  ...,  0.0298,  0.0096, -0.0327],\n",
      "        [-0.0279, -0.0270, -0.0260,  ...,  0.0366, -0.0587,  0.0241]],\n",
      "       size=(192, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0009628683910705149,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-1.1114e-02,  4.4044e-02, -6.8559e-03,  3.2829e-02, -2.8763e-02,\n",
      "        -1.0809e-02, -1.6793e-02, -1.5005e-05,  4.9486e-03,  4.2160e-03,\n",
      "         3.1879e-02, -1.5699e-02, -5.1480e-03, -1.0420e-02, -5.3506e-03,\n",
      "         3.3755e-02, -1.1466e-03,  2.8462e-02, -9.0816e-03, -3.8818e-03,\n",
      "         2.6433e-02,  5.4014e-04,  1.2206e-02,  2.2696e-02, -5.4873e-02,\n",
      "        -3.0140e-03,  4.6939e-03, -5.8922e-04, -1.1779e-02, -1.2534e-02,\n",
      "         8.4912e-03, -3.2522e-02,  2.7469e-02,  9.8847e-03, -1.0277e-02,\n",
      "        -2.4678e-02, -6.0033e-03,  2.4877e-02,  7.8528e-03,  1.7827e-02,\n",
      "         3.4093e-02,  1.0010e-02, -7.6997e-03, -3.2211e-02,  1.0359e-02,\n",
      "        -1.1746e-02, -3.1118e-02, -2.9392e-02, -4.4928e-02,  1.5190e-02,\n",
      "        -3.5161e-02, -1.0174e-02,  3.2068e-03, -2.1357e-03, -1.2839e-02,\n",
      "         1.6324e-02, -2.7349e-02,  2.4000e-02, -1.8802e-02, -2.3931e-02,\n",
      "         6.2544e-03,  3.6548e-03,  1.9159e-02,  4.9814e-04, -1.5408e-02,\n",
      "        -2.1665e-02, -3.1555e-02,  1.0654e-02,  3.0881e-02,  4.7831e-02,\n",
      "        -1.5636e-03, -3.9303e-03, -1.1442e-03, -1.7787e-02, -5.0670e-02,\n",
      "         3.1011e-02, -4.3246e-02, -3.3107e-02,  3.1526e-02, -1.7126e-02,\n",
      "         4.3600e-02,  9.2230e-04, -1.7284e-02, -3.0212e-02,  1.4462e-02,\n",
      "         3.6810e-02, -5.9524e-02, -4.6589e-02,  1.5605e-02, -1.2762e-02,\n",
      "        -1.2596e-02, -3.1607e-02, -2.2193e-02,  2.8565e-03,  3.1358e-02,\n",
      "        -1.1288e-02, -6.4377e-03, -1.3729e-02,  2.3797e-02, -3.6342e-02,\n",
      "        -3.3878e-02, -1.8079e-04,  1.8737e-02,  3.9885e-02, -3.6884e-02,\n",
      "        -2.0722e-02, -1.2636e-02, -4.0199e-02, -3.7782e-02,  9.6052e-03,\n",
      "        -3.2802e-02, -1.7876e-02,  1.4767e-02, -1.3826e-02,  1.3412e-02,\n",
      "         3.9606e-03,  9.4661e-03, -1.4373e-02,  2.1388e-02, -3.1818e-02,\n",
      "        -1.6600e-02,  2.3035e-02,  1.7363e-02, -5.7977e-03, -1.2182e-02,\n",
      "         7.8174e-03, -3.9234e-02, -4.0916e-02, -1.2181e-02, -1.8883e-02,\n",
      "        -1.3769e-02,  1.0117e-02, -1.9311e-02, -1.1695e-02, -1.2035e-02,\n",
      "        -2.5254e-02,  3.6546e-02,  7.2212e-03,  1.4268e-02,  1.8659e-02,\n",
      "        -3.8119e-02, -4.4812e-02,  2.0951e-02,  3.6302e-02,  1.6313e-02,\n",
      "        -1.1175e-02, -3.4187e-02, -4.6747e-02, -2.6137e-02,  5.2322e-03,\n",
      "         3.4701e-02, -1.8425e-02, -1.1336e-02,  1.9138e-02, -1.8441e-02,\n",
      "        -9.7480e-03,  2.2057e-02,  2.4840e-02, -9.9632e-03, -1.2612e-02,\n",
      "        -5.9567e-02,  1.8888e-02,  9.0677e-03, -2.9870e-03,  1.7685e-02,\n",
      "         4.1237e-02,  9.8927e-03, -7.2057e-02,  1.2730e-03, -1.9818e-02,\n",
      "        -4.2360e-02,  6.2983e-03, -1.2706e-02, -1.2052e-03, -8.9860e-02,\n",
      "        -5.9519e-02, -9.9133e-03, -1.8237e-02, -3.5209e-02,  7.4250e-03,\n",
      "         1.5901e-02, -2.0540e-02,  1.1005e-03, -3.7816e-02,  1.4934e-02,\n",
      "         1.3069e-02, -3.3887e-02, -3.6516e-03, -2.0785e-02,  1.2890e-02,\n",
      "         2.9887e-02,  1.2980e-02], requires_grad=True))\n",
      "encoder.5.0.block.0.weight \t tensor([1.0040, 1.0196, 1.0704, 1.1298, 1.0712, 1.0603, 1.0917, 1.0490, 1.0462,\n",
      "        1.0612, 1.0613, 1.0652, 1.0652, 1.0995, 1.0740, 1.0476, 1.0782, 1.0395,\n",
      "        1.0467, 1.0482, 1.0595, 1.0212, 1.0892, 1.0564, 1.0591, 1.0401, 1.0688,\n",
      "        1.0644, 1.0795, 1.0583, 1.0961, 1.0673, 1.0423, 1.0371, 1.0704, 1.0984,\n",
      "        1.0560, 1.0819, 1.1150, 1.0513, 1.0395, 1.0705, 1.0471, 1.1171, 1.0756,\n",
      "        1.0950, 1.0425, 1.0132, 1.0667, 1.0646, 1.0535, 1.0627, 1.0746, 1.0333,\n",
      "        1.0406, 1.0506, 1.0950, 1.0659, 1.0542, 1.1152, 1.1051, 1.0496, 1.0604,\n",
      "        1.0909, 1.1683, 1.0517, 1.0722, 1.0735, 1.0947, 1.0375, 1.0749, 1.0275,\n",
      "        1.0235, 1.0754, 1.0849, 1.0203, 1.1024, 1.0624, 1.0068, 1.0479, 1.0372,\n",
      "        1.0356, 1.0895, 1.0444, 1.0799, 1.0163, 1.0800, 1.0689, 1.0467, 1.0579,\n",
      "        1.0647, 1.0638, 1.0726, 1.1013, 1.0186, 1.0575, 1.0328, 1.0457, 1.0428,\n",
      "        1.0432, 1.0654, 1.0768, 1.0599, 1.0159, 1.1010, 1.0476, 1.0689, 1.0837,\n",
      "        1.0496, 1.0630, 1.0956, 1.0832, 1.0394, 1.0422, 1.0577, 1.0625, 1.0778,\n",
      "        1.0391, 1.0598, 1.0762, 1.0847, 1.0916, 1.1236, 1.0519, 1.0955, 1.0524,\n",
      "        1.0737, 1.0769, 1.1136, 1.0508, 1.0745, 1.0555, 1.0512, 1.0406, 1.0306,\n",
      "        1.0552, 1.0989, 1.0317, 1.0444, 1.0923, 1.0401, 1.0403, 1.0319, 1.1142,\n",
      "        1.0648, 1.0799, 1.0375, 1.0953, 1.0798, 1.0414, 1.0859, 1.0929, 1.0831,\n",
      "        1.0774, 1.0578, 1.0532, 1.0767, 1.0957, 1.1136, 1.0403, 1.0819, 1.0746,\n",
      "        1.0552, 1.0697, 1.0258, 1.0947, 1.0903, 1.0740, 1.1220, 1.0647, 1.0678,\n",
      "        1.1130, 1.0924, 1.0170, 1.0599, 1.0582, 1.0503, 1.0619, 1.0062, 1.0201,\n",
      "        1.0889, 1.0654, 1.0477, 1.0637, 1.0801, 1.0709, 1.0788, 1.0638, 1.0835,\n",
      "        1.0617, 1.0874, 1.0522])\n",
      "encoder.5.0.block.0.bias \t tensor([-1.6078e-02, -1.6273e-02,  1.2383e-02, -2.3829e-02,  1.8708e-02,\n",
      "         1.6946e-02,  1.4111e-02, -1.2017e-02,  1.7149e-02, -3.2132e-02,\n",
      "         1.2410e-02, -4.0590e-02,  5.3764e-03,  6.9027e-03, -2.3430e-02,\n",
      "         8.9716e-03,  1.2777e-02,  3.0438e-02, -6.2387e-02, -2.0528e-02,\n",
      "         1.7131e-02, -2.6253e-02, -3.1117e-02,  1.5881e-02,  3.3949e-03,\n",
      "         7.9496e-03,  9.9044e-03, -1.5853e-02,  2.6235e-02, -2.7324e-03,\n",
      "        -3.7921e-02, -5.8391e-03, -1.7872e-02, -4.7602e-02,  2.9936e-02,\n",
      "        -1.5696e-02, -1.5856e-02,  2.4063e-03,  1.3723e-02,  5.0936e-03,\n",
      "        -1.6100e-02,  2.6463e-02, -1.1127e-02,  1.1221e-03, -2.4758e-02,\n",
      "         6.5538e-02,  1.0533e-02, -4.7662e-03, -1.3937e-02,  3.3054e-03,\n",
      "         2.5036e-02, -3.0782e-03, -1.5611e-02,  2.8689e-03,  1.1901e-02,\n",
      "         1.6200e-02,  3.2648e-02, -2.1268e-02,  1.2268e-02,  1.2687e-02,\n",
      "        -1.7268e-02, -2.3912e-02, -1.5457e-02,  1.9240e-02, -8.9838e-03,\n",
      "        -1.8814e-02,  3.3694e-02,  3.2360e-05, -3.4748e-04,  2.3958e-03,\n",
      "         2.7666e-02, -1.9382e-02, -1.9382e-02, -1.3705e-02,  2.1839e-03,\n",
      "        -2.3970e-02,  2.0861e-02, -1.2946e-02,  1.0923e-04, -4.0910e-03,\n",
      "        -6.1317e-03,  6.5413e-02, -2.6748e-02, -1.6659e-02,  9.2704e-03,\n",
      "         1.0450e-02,  4.4999e-03,  1.0988e-02, -1.2671e-02, -4.1485e-02,\n",
      "        -3.3754e-02,  1.4674e-03,  9.4934e-03,  2.1821e-02,  2.9225e-04,\n",
      "         3.1063e-02,  1.2220e-02, -4.4459e-03,  1.0390e-02,  1.8050e-02,\n",
      "        -2.3337e-02, -1.8241e-02,  8.2383e-03,  1.9018e-02,  2.0319e-02,\n",
      "         1.3866e-02, -4.3113e-03, -3.8966e-02,  3.4529e-02, -5.3830e-03,\n",
      "         1.3873e-02,  6.7009e-02, -1.2489e-02, -1.1384e-03, -2.8559e-03,\n",
      "        -2.1934e-02, -1.2523e-03, -2.0337e-02,  1.1227e-02,  1.4287e-02,\n",
      "         1.0162e-02,  9.7022e-03, -1.2539e-03,  4.3092e-03,  1.7934e-02,\n",
      "         3.9632e-02,  1.1429e-02,  2.0224e-02, -3.7302e-04, -7.2591e-03,\n",
      "        -1.1631e-02, -7.0199e-03, -4.1661e-03, -1.8633e-02, -9.8691e-04,\n",
      "        -3.5259e-02,  2.7380e-02,  1.1591e-02, -3.4748e-02, -3.2195e-02,\n",
      "        -1.1440e-02, -3.1906e-02, -1.8460e-02, -1.7123e-02,  3.7281e-02,\n",
      "        -9.0607e-03, -4.3940e-02, -5.0359e-02,  1.0282e-02, -1.2650e-02,\n",
      "         1.3500e-02, -1.5335e-02,  2.9392e-02, -1.1979e-02, -1.5790e-03,\n",
      "        -7.6614e-03,  1.8989e-02, -1.8019e-02, -1.4556e-02, -9.5855e-03,\n",
      "        -3.8630e-03, -1.3162e-02,  6.4234e-02, -5.3949e-03, -2.0051e-02,\n",
      "        -8.4334e-03,  3.9684e-02, -1.9391e-03,  1.6300e-02, -1.8553e-02,\n",
      "        -1.1218e-03,  1.3178e-03, -4.1242e-03,  8.2383e-03, -2.5318e-02,\n",
      "         1.8986e-02,  1.9129e-02, -6.3024e-03,  1.9254e-02, -1.8060e-02,\n",
      "         4.9744e-02, -2.9559e-03,  8.1625e-03,  1.4531e-02,  3.0216e-02,\n",
      "        -1.0660e-02,  1.0798e-03,  2.0625e-02, -1.2890e-02, -3.2998e-03,\n",
      "         4.4649e-03,  3.8908e-02])\n",
      "encoder.5.0.block.1.queries_projection.scale \t tensor(1.)\n",
      "encoder.5.0.block.1.queries_projection.zero_point \t tensor(0)\n",
      "encoder.5.0.block.1.queries_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.5.0.block.1.queries_projection._packed_params._packed_params \t (tensor([[-0.0127,  0.1235, -0.0381,  ...,  0.0963,  0.0926, -0.0763],\n",
      "        [ 0.0690,  0.0091,  0.0036,  ...,  0.0490,  0.0327, -0.0036],\n",
      "        [ 0.0054, -0.1054,  0.0073,  ..., -0.0018, -0.0690, -0.0527],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0509, -0.0091,  ...,  0.0036, -0.0291, -0.0091],\n",
      "        [ 0.0036,  0.0400, -0.0527,  ...,  0.1090,  0.0945,  0.0036],\n",
      "        [ 0.0272,  0.0763, -0.0472,  ..., -0.0781, -0.0563,  0.0345]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.001816588919609785,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0582,  0.0707,  0.0011,  0.0647, -0.0917,  0.0399,  0.0052, -0.0059,\n",
      "        -0.0688, -0.0395, -0.0314, -0.0218,  0.0510,  0.0398, -0.0179, -0.0484,\n",
      "        -0.0135,  0.0802, -0.0442, -0.0401, -0.0159, -0.0567,  0.0141,  0.0041,\n",
      "        -0.0494, -0.0640, -0.0183, -0.0905,  0.0279,  0.0230, -0.0517, -0.0658,\n",
      "        -0.0373,  0.0302, -0.0377,  0.0237, -0.0276, -0.0310, -0.0360, -0.0247,\n",
      "         0.0537, -0.0241,  0.0141, -0.0379, -0.0322,  0.0285,  0.0116, -0.0420,\n",
      "        -0.0296,  0.0333,  0.0656, -0.0004,  0.0389, -0.0396, -0.0684, -0.0640,\n",
      "         0.0368,  0.0558, -0.0199,  0.0476, -0.0495,  0.0126,  0.0807, -0.0327,\n",
      "        -0.0358, -0.0068, -0.0571, -0.0261,  0.0277, -0.0615,  0.0187,  0.0391,\n",
      "         0.0090, -0.0197,  0.0586,  0.0081,  0.0174,  0.0683, -0.0480,  0.0533,\n",
      "        -0.0506, -0.0203,  0.0148,  0.0406, -0.0121, -0.0147,  0.0695,  0.0118,\n",
      "        -0.0530, -0.0549, -0.0076, -0.0553, -0.0524,  0.0011,  0.0578, -0.0095,\n",
      "        -0.0728,  0.0364,  0.0924, -0.0075,  0.0206, -0.0147, -0.0668,  0.0003,\n",
      "         0.0020, -0.0147, -0.0292, -0.0354,  0.0647, -0.0298,  0.0833, -0.0954,\n",
      "        -0.0692, -0.0662, -0.0454, -0.0252,  0.0222, -0.0060,  0.0550, -0.0065,\n",
      "        -0.0068, -0.0451,  0.0272, -0.0539, -0.0407, -0.0142, -0.0719, -0.0531,\n",
      "        -0.0618, -0.0240,  0.0407, -0.0241, -0.0513, -0.0216,  0.0529,  0.0593,\n",
      "        -0.0353,  0.0345,  0.0371, -0.0040,  0.0152,  0.0547,  0.0362,  0.0072,\n",
      "        -0.0554,  0.0013,  0.0592, -0.0228, -0.0911,  0.0444, -0.0017,  0.0495,\n",
      "        -0.0906,  0.0425,  0.0584,  0.0182, -0.0526,  0.0221,  0.0682,  0.0274,\n",
      "        -0.0503,  0.0669, -0.0590, -0.0019, -0.0916,  0.0187, -0.0224, -0.0472,\n",
      "        -0.0579, -0.0174, -0.0422,  0.0282, -0.0825,  0.0281, -0.0143,  0.0170,\n",
      "        -0.0215, -0.0275,  0.0086, -0.0136,  0.0369, -0.1071, -0.0044, -0.0259,\n",
      "         0.0055, -0.0438, -0.0769,  0.0614, -0.0036, -0.0522, -0.1011,  0.0177],\n",
      "       requires_grad=True))\n",
      "encoder.5.0.block.1.values_projection.scale \t tensor(1.)\n",
      "encoder.5.0.block.1.values_projection.zero_point \t tensor(0)\n",
      "encoder.5.0.block.1.values_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.5.0.block.1.values_projection._packed_params._packed_params \t (tensor([[-0.0350,  0.0612, -0.0044,  ..., -0.0787, -0.0809, -0.0743],\n",
      "        [-0.0809, -0.1311,  0.0634,  ..., -0.0415,  0.0874,  0.0066],\n",
      "        [-0.0437, -0.0721, -0.0131,  ..., -0.0328,  0.1136, -0.0284],\n",
      "        ...,\n",
      "        [ 0.0022, -0.0524,  0.0197,  ...,  0.0087, -0.0197, -0.0437],\n",
      "        [ 0.0852,  0.0044, -0.0656,  ...,  0.0131,  0.0830, -0.0546],\n",
      "        [-0.0503,  0.0328, -0.0262,  ...,  0.0284,  0.0765, -0.0109]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0021851519122719765,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0380, -0.0108, -0.0142, -0.0630, -0.0421, -0.0502, -0.0521,  0.0352,\n",
      "         0.0027,  0.0153,  0.0021,  0.0417, -0.0048, -0.0172,  0.0264, -0.0535,\n",
      "         0.0556,  0.0261, -0.0549,  0.0075, -0.0627,  0.0046, -0.0313,  0.0085,\n",
      "        -0.0700,  0.0666,  0.0362, -0.0340, -0.0200, -0.0169,  0.0176,  0.0124,\n",
      "        -0.0085, -0.0239, -0.0457, -0.0653, -0.0414, -0.0438,  0.0235, -0.0342,\n",
      "         0.0265,  0.0013, -0.0506,  0.0563, -0.0530,  0.0079, -0.0149,  0.0037,\n",
      "         0.0195, -0.0548, -0.0048, -0.0277,  0.0005, -0.0225, -0.0505,  0.0220,\n",
      "         0.0223, -0.0088, -0.0068,  0.0446, -0.0044,  0.0523, -0.0546, -0.0016,\n",
      "         0.0599,  0.0487,  0.0629, -0.0444, -0.0544,  0.0725, -0.0466, -0.0460,\n",
      "         0.0541,  0.0352,  0.0460,  0.0555,  0.0398,  0.0423,  0.0064,  0.0439,\n",
      "        -0.0577,  0.0561, -0.0086,  0.0528,  0.0057,  0.0019, -0.0079,  0.0050,\n",
      "         0.0296, -0.0218, -0.0513, -0.0554, -0.0377, -0.0017,  0.0507,  0.0138,\n",
      "         0.0234,  0.0035,  0.0516,  0.0314, -0.0418, -0.0332, -0.0592,  0.0726,\n",
      "        -0.0603, -0.0556, -0.0636, -0.0633,  0.0465,  0.0323, -0.0490,  0.0223,\n",
      "        -0.0150, -0.0351, -0.0436, -0.0569,  0.0578,  0.0200,  0.0353,  0.0200,\n",
      "         0.0717,  0.0489,  0.0577, -0.0262,  0.0249, -0.0295, -0.0192,  0.0106,\n",
      "         0.0161, -0.0489, -0.0087,  0.0293,  0.0716,  0.0674,  0.0670, -0.0508,\n",
      "         0.0573,  0.0653, -0.0616,  0.0365,  0.0111, -0.0501,  0.0367,  0.0471,\n",
      "        -0.0242, -0.0042,  0.0459, -0.0484, -0.0347,  0.0391, -0.0364, -0.0146,\n",
      "        -0.0646, -0.0206,  0.0265, -0.0485,  0.0067, -0.0627, -0.0298, -0.0246,\n",
      "         0.0466, -0.0267, -0.0251, -0.0330,  0.0752,  0.0777,  0.0531, -0.0613,\n",
      "        -0.0499, -0.0334, -0.0521, -0.0223,  0.0512, -0.0260, -0.0019, -0.0329,\n",
      "        -0.0039, -0.0241,  0.0518, -0.0205, -0.0309,  0.0420,  0.0471,  0.0353,\n",
      "        -0.0217,  0.0186,  0.0647,  0.0449, -0.0517,  0.0084,  0.0244, -0.0577],\n",
      "       requires_grad=True))\n",
      "encoder.5.0.block.1.keys_projection.scale \t tensor(1.)\n",
      "encoder.5.0.block.1.keys_projection.zero_point \t tensor(0)\n",
      "encoder.5.0.block.1.keys_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.5.0.block.1.keys_projection._packed_params._packed_params \t (tensor([[ 0.0052,  0.0643, -0.0782,  ..., -0.0504,  0.1061,  0.0191],\n",
      "        [-0.1095, -0.0035,  0.0626,  ...,  0.0226,  0.0504,  0.0000],\n",
      "        [-0.0504, -0.0852,  0.0800,  ...,  0.0661,  0.0000,  0.0383],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0104,  0.0417,  ..., -0.0504, -0.0383,  0.0539],\n",
      "        [ 0.0974, -0.0122, -0.0383,  ...,  0.0452,  0.0522, -0.0243],\n",
      "        [ 0.0574,  0.0869,  0.0626,  ...,  0.0417,  0.0713,  0.0313]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0017387830885127187,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0100,  0.0060, -0.0585,  0.0557, -0.0274,  0.0588,  0.0534,  0.0555,\n",
      "        -0.0557,  0.0240,  0.0659,  0.0448, -0.0073,  0.0233,  0.0191, -0.0189,\n",
      "        -0.0125, -0.0468, -0.0526, -0.0213,  0.0272, -0.0198,  0.0296, -0.0176,\n",
      "         0.0001, -0.0178, -0.0574,  0.0306, -0.0668,  0.0714,  0.0002, -0.0646,\n",
      "         0.0095, -0.0468,  0.0599,  0.0439,  0.0612,  0.0372, -0.0149,  0.0397,\n",
      "         0.0421, -0.0227, -0.0450, -0.0337, -0.0109,  0.0396, -0.0551,  0.0387,\n",
      "         0.0143,  0.0100, -0.0707, -0.0422,  0.0537, -0.0062,  0.0727, -0.0175,\n",
      "        -0.0317, -0.0637, -0.0191, -0.0307,  0.0165, -0.0136, -0.0430, -0.0406,\n",
      "        -0.0182, -0.0052,  0.0112, -0.0487,  0.0055,  0.0393, -0.0131,  0.0625,\n",
      "         0.0147, -0.0061, -0.0496, -0.0695, -0.0499, -0.0569, -0.0632, -0.0636,\n",
      "        -0.0219, -0.0234,  0.0088,  0.0696,  0.0236,  0.0133,  0.0148,  0.0244,\n",
      "         0.0345, -0.0573, -0.0399, -0.0423, -0.0237,  0.0098, -0.0676, -0.0419,\n",
      "         0.0445, -0.0070,  0.0284, -0.0305,  0.0393, -0.0472,  0.0350,  0.0014,\n",
      "         0.0623,  0.0331, -0.0604,  0.0659, -0.0143, -0.0720, -0.0726,  0.0683,\n",
      "        -0.0590,  0.0308, -0.0242, -0.0654, -0.0198, -0.0149, -0.0389, -0.0013,\n",
      "         0.0459,  0.0480,  0.0484,  0.0124,  0.0197, -0.0634, -0.0109, -0.0704,\n",
      "        -0.0282, -0.0140, -0.0136,  0.0569,  0.0603,  0.0613,  0.0568, -0.0423,\n",
      "         0.0698, -0.0500, -0.0394, -0.0300,  0.0678,  0.0067,  0.0234, -0.0106,\n",
      "         0.0530, -0.0002,  0.0059, -0.0773,  0.0697,  0.0113, -0.0184,  0.0334,\n",
      "        -0.0426, -0.0697, -0.0415,  0.0451, -0.0435,  0.0787,  0.0247,  0.0201,\n",
      "        -0.0035, -0.0068,  0.0272, -0.0099, -0.0615,  0.0590, -0.0133,  0.0381,\n",
      "         0.0696, -0.0613,  0.0309,  0.0403,  0.0014, -0.0501,  0.0620, -0.0065,\n",
      "         0.0261,  0.0453, -0.0661, -0.0620, -0.0325,  0.0269,  0.0105, -0.0444,\n",
      "         0.0197,  0.0077, -0.0619, -0.0488,  0.0200, -0.0446,  0.0664, -0.0463],\n",
      "       requires_grad=True))\n",
      "encoder.5.0.block.1.final_projection.scale \t tensor(1.)\n",
      "encoder.5.0.block.1.final_projection.zero_point \t tensor(0)\n",
      "encoder.5.0.block.1.final_projection._packed_params.dtype \t torch.qint8\n",
      "encoder.5.0.block.1.final_projection._packed_params._packed_params \t (tensor([[ 0.0099, -0.0123,  0.0444,  ..., -0.0863, -0.0518,  0.1528],\n",
      "        [ 0.1553, -0.1208, -0.1158,  ...,  0.0518,  0.1282,  0.0887],\n",
      "        [-0.0148, -0.0246,  0.0789,  ...,  0.0961,  0.0690,  0.0049],\n",
      "        ...,\n",
      "        [ 0.1503,  0.0320,  0.0838,  ...,  0.0025, -0.0813,  0.0986],\n",
      "        [ 0.0345, -0.0863, -0.0025,  ...,  0.0690, -0.0173,  0.0542],\n",
      "        [ 0.0641, -0.0592,  0.0296,  ..., -0.1060,  0.0641, -0.0025]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0024645987432450056,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-4.5061e-02,  5.1251e-02, -1.1845e-02,  7.5974e-02,  1.6666e-02,\n",
      "        -1.9714e-02, -5.8005e-03,  3.7652e-02,  1.4640e-02, -6.2749e-02,\n",
      "         6.4713e-03,  1.2876e-02, -2.0077e-02,  6.2732e-02,  1.6301e-02,\n",
      "        -2.5795e-03, -2.6519e-02,  2.2310e-02, -4.3505e-02, -5.1138e-02,\n",
      "        -3.5702e-02, -6.4713e-02,  3.2990e-02, -1.9963e-02, -2.8530e-02,\n",
      "        -9.5469e-04,  6.7655e-02,  4.6615e-02,  6.4175e-02, -2.3270e-02,\n",
      "        -2.0380e-02,  3.7938e-02,  4.5630e-02, -8.0157e-02,  3.3612e-02,\n",
      "        -1.9522e-02,  2.5804e-02, -5.0788e-02,  1.8344e-02,  2.3505e-02,\n",
      "        -4.1560e-02, -4.6803e-02,  2.4042e-02,  5.3411e-03,  5.8957e-02,\n",
      "        -4.1081e-02,  3.7734e-04,  6.1145e-02, -8.2095e-02,  2.2657e-02,\n",
      "         4.9341e-02,  4.9735e-02,  4.5169e-02,  5.6259e-02,  5.2708e-02,\n",
      "        -1.4060e-05, -2.4693e-02, -9.4351e-03,  3.7444e-02,  7.0044e-02,\n",
      "         5.2069e-02, -3.6270e-02,  4.7126e-02, -7.4498e-02,  5.6346e-02,\n",
      "         2.6131e-02, -7.5128e-02, -4.9148e-02, -5.5425e-02,  1.6000e-03,\n",
      "        -1.4877e-03, -3.2502e-02, -6.2955e-03,  4.3695e-02,  9.2821e-03,\n",
      "         3.3934e-02, -4.1643e-02, -3.6633e-03,  1.1566e-03,  3.5893e-02,\n",
      "        -2.3276e-02,  4.1729e-02,  1.7800e-02, -8.8203e-02, -6.7972e-02,\n",
      "        -3.7766e-02, -6.4942e-02,  9.0076e-03, -1.2069e-02, -2.6159e-02,\n",
      "        -4.0899e-02,  4.7624e-02, -6.8657e-02,  6.3156e-02, -1.8354e-02,\n",
      "        -7.4172e-02, -5.7429e-02, -6.4990e-02,  9.0046e-02,  5.6068e-03,\n",
      "         3.9520e-02,  8.5049e-02,  3.9121e-02, -2.1824e-02, -1.2761e-02,\n",
      "         2.7298e-02, -6.8872e-02,  9.1534e-03, -4.2241e-02, -6.2870e-02,\n",
      "        -3.1967e-02, -6.6741e-03,  6.0484e-02,  4.0468e-04, -6.0060e-02,\n",
      "        -8.3399e-03, -7.0721e-02, -6.6144e-02,  4.3157e-02, -2.6243e-02,\n",
      "         1.4249e-02, -2.8142e-02, -6.3604e-02, -3.9538e-02,  4.0668e-02,\n",
      "         5.4459e-02, -4.7331e-02,  3.0552e-02, -4.8171e-02, -5.3202e-02,\n",
      "        -2.6586e-02,  4.9577e-02, -4.0971e-02, -1.4920e-02,  2.1770e-02,\n",
      "         3.9469e-02,  8.2398e-02, -1.8872e-02,  7.8470e-02, -3.3627e-02,\n",
      "         9.8868e-03,  8.3949e-03, -4.2282e-02, -2.3824e-02,  4.7076e-02,\n",
      "        -7.5572e-03,  5.8071e-02, -1.6971e-02, -4.3051e-02,  4.5807e-02,\n",
      "         8.0403e-02,  6.0236e-02, -3.3519e-02,  6.4799e-02, -6.4273e-02,\n",
      "        -4.1404e-02, -4.8378e-02,  6.6074e-02,  4.8741e-02,  6.7132e-02,\n",
      "         4.5757e-03,  8.8207e-03, -5.1363e-02, -2.4550e-02,  5.6278e-03,\n",
      "        -4.2449e-02, -3.5725e-02, -2.8504e-02, -3.9445e-02, -1.8112e-03,\n",
      "         4.1035e-02,  1.4346e-02, -4.1535e-03,  3.1990e-03, -3.5299e-02,\n",
      "         2.8565e-02, -7.4803e-02,  5.1403e-02, -4.5531e-02, -7.5480e-02,\n",
      "        -6.6026e-02, -6.6536e-02, -1.2084e-02,  8.6877e-03,  2.9733e-02,\n",
      "        -3.0511e-02,  2.6170e-03,  3.8643e-02, -8.2236e-02, -7.4078e-02,\n",
      "         2.5381e-02, -5.1540e-02], requires_grad=True))\n",
      "encoder.5.1.block.0.weight \t tensor([0.8664, 0.9132, 0.9032, 0.9271, 0.9020, 0.9413, 0.9585, 0.9339, 0.9020,\n",
      "        0.9248, 0.8638, 0.9689, 0.9115, 0.8984, 0.9297, 0.9144, 0.8346, 0.8713,\n",
      "        0.9432, 0.8894, 0.9235, 0.9130, 0.9011, 0.8911, 0.8200, 0.8763, 0.8905,\n",
      "        0.8848, 0.8987, 0.8596, 0.9266, 0.8857, 0.9264, 0.8507, 0.8829, 0.9521,\n",
      "        0.9357, 0.9031, 0.8799, 0.8703, 0.8927, 0.8524, 0.9122, 0.9331, 0.9448,\n",
      "        0.8858, 0.8946, 0.8902, 0.9670, 0.8493, 0.8737, 0.9203, 0.9080, 0.9065,\n",
      "        0.9281, 0.9128, 0.9391, 0.9202, 0.9260, 0.9437, 0.9063, 0.9039, 0.8987,\n",
      "        0.9167, 0.9048, 0.8761, 0.9341, 0.9169, 0.9346, 0.8983, 1.0289, 0.9326,\n",
      "        0.8718, 0.9185, 0.9172, 0.8949, 0.9005, 0.8274, 0.9436, 0.9162, 0.8792,\n",
      "        0.9154, 0.8992, 0.9007, 0.9355, 0.9067, 0.9022, 0.9246, 0.9331, 0.8889,\n",
      "        0.8484, 0.9155, 0.9009, 0.8668, 0.9483, 0.9378, 0.8251, 0.9083, 0.8894,\n",
      "        0.8763, 0.8766, 0.9206, 0.9368, 0.9215, 0.9547, 0.9055, 0.9199, 0.9558,\n",
      "        0.9304, 0.8626, 0.8949, 0.9278, 0.8908, 0.9164, 0.8805, 0.9210, 0.9259,\n",
      "        0.9149, 0.9556, 0.9071, 0.8401, 0.9187, 0.9245, 0.9060, 0.8543, 0.8859,\n",
      "        0.9084, 0.8214, 0.8903, 0.9322, 0.9181, 0.9280, 0.8711, 0.8934, 0.9592,\n",
      "        0.8819, 0.8914, 0.8876, 0.9338, 0.8592, 0.9110, 0.8602, 0.9126, 0.8957,\n",
      "        0.8644, 0.9632, 0.8984, 0.8686, 0.9456, 0.9124, 0.8891, 0.8787, 0.8484,\n",
      "        0.8967, 0.9004, 0.8997, 0.9228, 0.8950, 0.8711, 0.9120, 0.9054, 0.8928,\n",
      "        0.9133, 0.8336, 0.9238, 0.9033, 0.9115, 0.9243, 0.9398, 0.8959, 0.9478,\n",
      "        0.9230, 0.9206, 0.8960, 0.8733, 0.8717, 0.9129, 0.9338, 0.9034, 0.8852,\n",
      "        0.9319, 0.8701, 0.9205, 0.8776, 0.9065, 0.9195, 0.9312, 0.8910, 0.9169,\n",
      "        0.8725, 0.8933, 0.9239])\n",
      "encoder.5.1.block.0.bias \t tensor([-0.0007, -0.0246, -0.0409,  0.0090, -0.0361,  0.0551, -0.0173,  0.0387,\n",
      "        -0.0111,  0.0249, -0.0319,  0.0333, -0.0365, -0.0024,  0.0163, -0.0037,\n",
      "        -0.0197, -0.0077,  0.0500,  0.0438, -0.0317,  0.0276, -0.0085,  0.0043,\n",
      "        -0.0276,  0.0143, -0.0283,  0.0176,  0.0331,  0.0201, -0.0181,  0.0264,\n",
      "         0.0234, -0.0026,  0.0011,  0.0444, -0.0159, -0.0566, -0.0203, -0.0121,\n",
      "        -0.0200, -0.0132,  0.0164,  0.0321, -0.0249,  0.0073,  0.0169,  0.0046,\n",
      "         0.0359,  0.0331, -0.0349,  0.0156,  0.0324, -0.0090,  0.0065,  0.0046,\n",
      "         0.0180, -0.0412, -0.0098, -0.0033, -0.0171, -0.0016, -0.0725,  0.0182,\n",
      "        -0.0171,  0.0414, -0.0313, -0.0148, -0.0498, -0.0244, -0.0504, -0.0284,\n",
      "        -0.0131, -0.0131,  0.0206,  0.0276, -0.0212,  0.0004, -0.0201,  0.0135,\n",
      "        -0.0166, -0.0255,  0.0097,  0.0461, -0.0426, -0.0126, -0.0004, -0.0171,\n",
      "         0.0165, -0.0215, -0.0412,  0.0017, -0.0521,  0.0256, -0.0181,  0.0156,\n",
      "         0.0146, -0.0246, -0.0294, -0.0254, -0.0054, -0.0056, -0.0305, -0.0023,\n",
      "         0.0293,  0.0537,  0.0406,  0.0021,  0.0082, -0.0182,  0.0180, -0.0236,\n",
      "        -0.0015, -0.0462, -0.0003, -0.0170,  0.0337,  0.0386, -0.0290,  0.0458,\n",
      "         0.0080,  0.0138, -0.0399,  0.0207, -0.0432, -0.0384,  0.0279, -0.0095,\n",
      "        -0.0323, -0.0412,  0.0125, -0.0600, -0.0145, -0.0169,  0.0379, -0.0247,\n",
      "         0.0212,  0.0252, -0.0274,  0.0081, -0.0254, -0.0169, -0.0076,  0.0498,\n",
      "        -0.0179,  0.0005,  0.0179,  0.0270, -0.0202, -0.0221, -0.0142,  0.0260,\n",
      "        -0.0124,  0.0103, -0.0088,  0.0264, -0.0157, -0.0226,  0.0199, -0.0444,\n",
      "         0.0043,  0.0267, -0.0099, -0.0347, -0.0300, -0.0029,  0.0006, -0.0304,\n",
      "        -0.0180,  0.0052, -0.0171,  0.0075, -0.0289, -0.0082, -0.0144,  0.0094,\n",
      "        -0.0122, -0.0154,  0.0288,  0.0190, -0.0190, -0.0053, -0.0158,  0.0271,\n",
      "         0.0177, -0.0563, -0.0251, -0.0088,  0.0239,  0.0202, -0.0010, -0.0292])\n",
      "encoder.5.1.block.1.0.scale \t tensor(1.)\n",
      "encoder.5.1.block.1.0.zero_point \t tensor(0)\n",
      "encoder.5.1.block.1.0._packed_params.dtype \t torch.qint8\n",
      "encoder.5.1.block.1.0._packed_params._packed_params \t (tensor([[ 0.0521,  0.0118,  0.0370,  ...,  0.0303, -0.0437, -0.0437],\n",
      "        [-0.0219, -0.0471, -0.0084,  ..., -0.0421, -0.0252, -0.0656],\n",
      "        [-0.0471,  0.0454,  0.0454,  ...,  0.0168, -0.0017,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0505,  0.0101,  0.0622,  ..., -0.0219,  0.0101,  0.0202],\n",
      "        [ 0.0067, -0.0084,  0.0168,  ...,  0.0320,  0.0067,  0.0421],\n",
      "        [ 0.0454, -0.0034, -0.0404,  ...,  0.0421,  0.0404,  0.0067]],\n",
      "       size=(768, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0016822362085804343,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-7.5273e-02,  2.1969e-03,  1.5681e-02, -1.2614e-02, -9.9094e-02,\n",
      "        -9.5546e-02, -9.2263e-02, -7.6591e-03, -2.5358e-02, -5.8678e-02,\n",
      "        -9.2994e-02,  1.9307e-02, -6.7101e-02, -4.5706e-02,  2.1965e-03,\n",
      "        -5.8222e-02,  3.2780e-02, -5.7394e-03, -6.5517e-02,  1.9012e-02,\n",
      "        -4.1441e-02, -1.1813e-02, -4.8770e-02, -6.6007e-02,  2.2149e-02,\n",
      "        -7.9240e-02, -7.1180e-03,  1.1682e-02, -6.9622e-02,  1.7131e-02,\n",
      "         2.5554e-02,  2.2141e-02, -8.9963e-02, -8.8228e-02,  2.2565e-02,\n",
      "        -8.4027e-02, -2.0846e-02,  5.8467e-03,  2.0494e-02, -7.1039e-02,\n",
      "        -7.1811e-02, -7.5521e-02, -1.1504e-02,  7.9199e-03, -2.7444e-02,\n",
      "        -5.7015e-02, -3.2155e-02, -6.5690e-02, -6.8802e-02,  3.0033e-03,\n",
      "        -9.2280e-02, -3.8041e-02, -2.7101e-02,  2.2402e-02, -8.0670e-02,\n",
      "        -2.3697e-02, -6.8790e-02, -3.2319e-02, -2.4608e-02, -5.6097e-02,\n",
      "        -6.1554e-02,  1.1284e-02, -1.8062e-02, -4.7506e-02, -3.3227e-02,\n",
      "        -1.0689e-02, -6.0401e-04, -6.5150e-02, -3.9287e-02,  1.2919e-02,\n",
      "        -3.1267e-02, -3.0920e-02, -3.7415e-02, -5.6721e-02, -2.5102e-03,\n",
      "         3.6817e-02, -7.2550e-02, -5.5144e-02, -1.0125e-01, -2.4640e-02,\n",
      "         1.6587e-03, -7.3827e-02, -7.6223e-02, -8.0052e-02, -4.4292e-02,\n",
      "         2.0247e-02, -9.9124e-02,  5.9252e-03, -7.7978e-02, -5.6407e-02,\n",
      "         2.9535e-04,  5.6253e-03, -3.6935e-02, -8.8731e-03,  1.0523e-03,\n",
      "        -5.5635e-02, -1.7634e-02, -6.6490e-02, -5.5912e-02, -6.4341e-02,\n",
      "        -1.7246e-02, -1.0494e-01,  1.8709e-02, -9.4136e-02, -6.9043e-02,\n",
      "        -7.2224e-02,  1.6002e-02,  2.1472e-02, -6.1365e-03, -9.4545e-03,\n",
      "         5.9753e-03, -1.9067e-02,  2.3332e-02, -9.4311e-02, -9.0934e-02,\n",
      "        -2.0579e-02, -5.0694e-02, -6.5790e-02,  4.7152e-03, -8.5416e-02,\n",
      "        -9.6145e-02, -1.9999e-02, -6.0582e-02, -3.5314e-02,  3.3473e-02,\n",
      "        -4.4727e-02, -8.5445e-02, -3.5321e-02, -2.9837e-02, -5.5029e-02,\n",
      "        -3.2742e-03, -6.1126e-02,  1.5140e-02, -1.5697e-02, -7.3077e-03,\n",
      "         2.7220e-05, -5.4408e-03, -3.1655e-02, -4.4516e-03,  1.6767e-02,\n",
      "         1.6826e-02,  8.8546e-03,  1.8173e-02, -2.9608e-02, -6.3672e-02,\n",
      "         6.9730e-03,  3.7846e-02, -3.8881e-02, -5.9800e-02, -7.6337e-02,\n",
      "         1.6895e-02,  1.8361e-02, -6.2117e-02, -7.4096e-02, -9.0446e-02,\n",
      "        -7.2824e-02, -9.1070e-02, -7.7741e-02, -5.8224e-03, -3.7469e-02,\n",
      "        -8.3841e-02, -3.2957e-02, -7.7321e-02, -5.5499e-02, -6.8794e-02,\n",
      "        -3.8349e-02,  5.8929e-03, -2.9099e-02, -7.3224e-02, -9.9300e-02,\n",
      "        -6.2219e-02, -3.7641e-02,  1.3508e-02, -2.2693e-02, -6.7089e-02,\n",
      "         3.0177e-02, -3.2381e-02, -6.2014e-02, -9.4993e-02, -7.9255e-02,\n",
      "        -6.5416e-02, -2.2369e-02, -2.7315e-02, -7.0921e-02, -9.8040e-02,\n",
      "         3.9590e-02, -1.0123e-01, -1.9491e-02, -3.9264e-02, -6.3226e-02,\n",
      "        -7.3552e-02, -5.1780e-02, -7.3337e-02, -5.4616e-02,  2.6112e-03,\n",
      "        -5.7504e-02,  1.5847e-02, -7.8714e-02, -3.2474e-02,  5.6569e-03,\n",
      "        -6.2572e-02, -6.3553e-03, -3.7644e-02, -2.3186e-02, -2.5969e-02,\n",
      "        -2.3465e-02, -9.3191e-02, -7.1575e-02,  3.1439e-02, -5.1204e-02,\n",
      "         3.4706e-02, -8.8529e-02,  1.4649e-03, -4.6232e-02, -8.6847e-02,\n",
      "        -1.0338e-02, -6.9047e-02, -3.1157e-02, -1.1405e-01, -6.8622e-02,\n",
      "         5.0000e-03, -3.8358e-02, -8.1269e-02, -3.7526e-02, -6.6731e-03,\n",
      "        -2.2232e-02, -1.6795e-02, -4.6108e-02, -3.7118e-02,  8.8166e-03,\n",
      "        -1.0577e-02,  2.6856e-03,  7.5551e-03, -4.3342e-02, -7.4211e-02,\n",
      "        -4.6577e-02, -1.0042e-04, -6.9901e-02,  1.6724e-02, -1.8708e-02,\n",
      "        -2.2142e-02, -3.6241e-02, -5.8823e-02, -4.2788e-02,  8.6207e-03,\n",
      "        -7.2043e-02, -7.1012e-02, -7.9333e-02,  9.6325e-03, -2.5775e-02,\n",
      "        -3.3288e-02, -9.4188e-02, -8.3921e-02,  1.4128e-02, -4.5382e-03,\n",
      "        -8.4759e-02, -5.9739e-02, -5.1221e-03, -2.1276e-02, -3.1634e-02,\n",
      "        -7.6898e-02, -8.5085e-02, -2.6775e-02, -5.9442e-03, -7.2686e-02,\n",
      "        -7.3951e-02, -7.9569e-02, -1.0357e-01, -4.1421e-02,  5.7900e-03,\n",
      "        -3.5938e-02, -5.9600e-02,  2.8221e-02, -3.7110e-02, -1.6874e-02,\n",
      "        -2.3449e-02, -1.7805e-02, -8.8585e-02, -6.2456e-02,  2.0561e-02,\n",
      "        -2.1674e-02, -4.2422e-02, -5.9915e-02, -7.3668e-02,  4.0330e-02,\n",
      "        -4.5122e-02, -8.4833e-02,  2.2222e-03, -7.7877e-02, -1.9417e-02,\n",
      "        -4.9675e-02, -3.7234e-02, -1.9261e-02, -7.4424e-02, -5.7251e-02,\n",
      "        -3.2098e-02, -5.9382e-02, -5.6509e-02, -1.5408e-03, -3.8495e-02,\n",
      "        -7.4348e-02,  1.2062e-02, -8.5157e-02, -8.1691e-02, -1.1351e-02,\n",
      "        -3.3255e-02,  3.4215e-03, -1.0359e-01, -2.3898e-02, -3.5385e-02,\n",
      "        -5.6594e-02,  1.9431e-02,  1.7448e-02, -9.0452e-02, -8.3701e-02,\n",
      "        -7.6364e-02, -7.9378e-02, -8.9005e-02, -6.4629e-03,  2.1270e-02,\n",
      "         2.2129e-02, -1.1681e-02, -4.8220e-02, -7.6342e-02,  2.1056e-02,\n",
      "        -1.2928e-02, -4.7298e-02, -9.3712e-02, -1.2980e-02, -6.8749e-02,\n",
      "        -7.4581e-02,  9.5454e-03, -8.9351e-02, -2.6625e-02, -1.8846e-02,\n",
      "        -8.5876e-02, -3.7065e-03, -1.4385e-02, -1.5910e-02,  8.0301e-03,\n",
      "         1.1286e-02, -4.1226e-02,  1.7232e-02, -6.6699e-02, -8.8666e-02,\n",
      "         9.9352e-03, -5.2179e-02, -4.2816e-02,  3.6219e-03, -1.0627e-02,\n",
      "        -3.5488e-02, -6.8316e-02, -7.9599e-02, -5.8384e-02, -7.7101e-02,\n",
      "         1.0207e-02, -3.9919e-02, -9.5023e-02, -3.7564e-02, -8.2562e-02,\n",
      "         3.0016e-02, -1.4009e-02, -7.1959e-02, -9.6069e-02, -5.5702e-02,\n",
      "        -1.9287e-02,  1.5178e-02, -9.1620e-02, -8.4949e-02, -3.7897e-03,\n",
      "         1.9847e-02,  2.8177e-02, -7.0446e-02, -5.8803e-02, -8.7543e-02,\n",
      "         2.8733e-02, -3.1947e-02, -9.3838e-02, -8.3912e-02,  1.8046e-02,\n",
      "        -3.4819e-02,  1.2558e-02, -3.3425e-02, -2.7239e-02, -2.0824e-02,\n",
      "        -3.2259e-03, -3.6515e-02, -6.6467e-02, -7.7862e-02, -3.7190e-02,\n",
      "        -4.6287e-02, -5.5136e-02, -1.6311e-02, -6.5654e-02, -7.5602e-02,\n",
      "        -1.4266e-02, -7.8497e-02,  3.1333e-02, -3.4734e-02, -4.6981e-02,\n",
      "         1.1504e-02, -8.4304e-02, -7.4396e-02, -1.2768e-02, -6.2659e-02,\n",
      "        -1.5614e-02, -2.0581e-02,  1.7643e-02,  1.3549e-02, -3.7552e-02,\n",
      "         6.1400e-03, -7.9569e-02,  1.3783e-02, -5.1187e-02, -1.8408e-02,\n",
      "         1.5868e-02,  5.1591e-03, -9.0166e-02,  8.6872e-03, -5.9512e-02,\n",
      "         9.8207e-03, -8.4295e-02, -2.8748e-02, -2.5840e-02, -1.0440e-01,\n",
      "         1.8464e-02, -1.9211e-03, -2.1118e-02, -2.7304e-02, -4.3135e-02,\n",
      "        -6.6529e-02, -1.7666e-02, -4.2037e-02, -9.2070e-02, -3.7492e-02,\n",
      "        -6.7336e-02, -5.2622e-02, -9.8299e-02,  1.9010e-02, -5.9456e-04,\n",
      "        -9.5171e-02, -1.1318e-02, -3.2391e-02, -4.4198e-02, -1.8884e-03,\n",
      "        -1.8613e-02, -1.8301e-02,  4.5882e-03,  1.7651e-02, -5.0049e-03,\n",
      "        -5.6720e-02, -6.8346e-02, -4.2550e-02, -3.8968e-02,  2.1325e-02,\n",
      "        -9.4584e-02, -7.6213e-02, -1.9948e-02, -4.7644e-02, -4.8774e-02,\n",
      "        -5.1957e-02, -4.8286e-02, -3.7886e-02, -4.0973e-02, -5.9429e-02,\n",
      "        -6.6302e-02, -5.8979e-02, -9.1107e-04, -6.9891e-03, -7.0226e-02,\n",
      "        -5.0242e-02, -3.3157e-02, -8.3389e-02,  1.8273e-03, -5.9765e-02,\n",
      "        -8.0799e-02, -5.9266e-02, -8.8152e-02,  1.9844e-02, -1.0592e-01,\n",
      "        -5.7567e-02, -3.2359e-02, -8.0581e-02, -4.0993e-02,  2.4659e-02,\n",
      "        -5.3814e-02, -2.5083e-02, -2.8987e-02, -5.7027e-02, -9.1973e-02,\n",
      "         7.4972e-04, -4.1167e-02,  1.4088e-02,  1.5311e-02,  1.3104e-02,\n",
      "        -6.2993e-03,  4.4924e-03, -4.6321e-02, -7.3178e-02, -7.4230e-02,\n",
      "         9.5389e-03, -3.5206e-02,  1.4144e-04,  2.4928e-02, -4.7250e-02,\n",
      "        -3.4209e-02,  4.7381e-03,  3.3613e-02,  2.5784e-02, -9.4365e-02,\n",
      "        -8.2862e-02, -5.6197e-02, -5.5195e-02, -2.1514e-03, -7.7762e-02,\n",
      "         1.3884e-02, -7.6272e-02, -1.1327e-02, -2.7909e-02,  2.7420e-02,\n",
      "        -9.0081e-04, -3.5084e-02,  2.5113e-03, -6.3317e-02, -3.2889e-02,\n",
      "        -7.9498e-02, -6.3344e-02, -1.9533e-02, -3.2041e-02, -6.2791e-02,\n",
      "        -4.2669e-02, -9.8151e-03, -8.6917e-02, -1.6984e-02, -1.0023e-01,\n",
      "        -8.8397e-03, -1.1450e-02, -7.8016e-02,  1.7159e-02, -4.4351e-02,\n",
      "        -1.4912e-02,  1.5307e-02, -8.7432e-02, -7.6006e-02, -9.6575e-02,\n",
      "        -8.5194e-02, -1.2253e-02, -2.8893e-02, -8.0717e-02, -8.5983e-02,\n",
      "        -4.6671e-02, -1.5415e-02, -6.5742e-02, -7.7458e-03,  2.9534e-02,\n",
      "        -6.8638e-02, -8.8075e-03, -6.5765e-02,  3.8432e-02, -4.2238e-02,\n",
      "        -8.4214e-02, -6.8152e-02, -5.5611e-02, -7.7042e-02, -1.0205e-01,\n",
      "        -6.9490e-02, -4.5934e-02, -6.3971e-02, -1.7483e-02, -6.7521e-02,\n",
      "        -4.2439e-02, -8.8778e-02, -7.5379e-02, -4.1557e-03,  1.1931e-03,\n",
      "        -9.2802e-02,  2.9330e-02, -8.8776e-02, -1.0348e-01, -1.2519e-04,\n",
      "        -3.0262e-02, -1.3656e-02, -1.4370e-02,  1.1003e-02, -2.8521e-02,\n",
      "        -6.9348e-02,  2.1687e-03, -4.5562e-02, -4.0739e-02,  1.8276e-02,\n",
      "         3.5118e-03, -2.7221e-02, -8.0159e-02, -1.5795e-02, -9.0414e-02,\n",
      "        -4.1760e-02,  9.7560e-03, -6.1976e-02, -6.4137e-03, -4.1516e-02,\n",
      "        -9.9951e-02,  1.4806e-02, -5.5339e-02, -2.9150e-02, -9.8112e-03,\n",
      "        -2.2249e-02, -1.0328e-01, -7.6895e-02, -4.6523e-02, -9.2082e-02,\n",
      "         2.2444e-03, -5.4456e-02, -6.7668e-02, -3.2903e-03, -9.1050e-02,\n",
      "        -1.7218e-03, -6.1574e-02, -7.6300e-02, -6.1130e-02, -9.8488e-02,\n",
      "        -6.2076e-02,  4.5362e-03,  1.3000e-02, -1.1495e-02, -8.0207e-02,\n",
      "        -7.9763e-02, -6.8670e-02, -3.6732e-02, -3.3032e-02, -1.1530e-02,\n",
      "        -4.0174e-02,  1.3369e-02,  1.7667e-03, -2.6496e-02,  1.2834e-02,\n",
      "        -3.2600e-02,  1.2203e-03, -1.1478e-02, -5.4078e-02, -2.2758e-02,\n",
      "        -3.5040e-02, -9.4418e-02, -6.4705e-02, -4.8470e-02, -6.1518e-02,\n",
      "        -1.0691e-03,  4.2141e-03,  1.6444e-02, -3.0545e-02, -4.7345e-02,\n",
      "        -6.4976e-02, -5.6365e-02,  3.8226e-02,  5.1514e-04, -9.9817e-02,\n",
      "        -1.1002e-02, -9.8065e-02, -8.4865e-02, -2.1874e-02, -9.5491e-02,\n",
      "        -1.1358e-02, -8.1132e-02, -4.7264e-02, -3.4026e-02, -1.1410e-02,\n",
      "         3.4815e-02, -7.7793e-02, -3.1056e-02, -4.1297e-02,  1.1271e-02,\n",
      "         3.5576e-03, -3.4168e-02, -4.4711e-02, -7.7059e-02, -8.7050e-02,\n",
      "        -6.9947e-03, -1.2279e-03, -4.0308e-02, -3.5092e-02, -9.7306e-03,\n",
      "        -4.9754e-02, -6.9644e-02,  1.0922e-02,  2.0461e-02, -8.2730e-02,\n",
      "        -2.4026e-02,  3.6188e-03,  7.4406e-03, -5.3545e-03, -4.6133e-02,\n",
      "        -2.4448e-02, -4.6616e-02,  1.3700e-02,  1.4132e-04, -9.0126e-02,\n",
      "        -3.6408e-02,  6.2613e-03, -5.7360e-02, -4.8201e-02, -1.0167e-02,\n",
      "        -4.6631e-02,  8.9657e-03, -3.3836e-02,  1.2043e-02, -7.5535e-02,\n",
      "         1.0523e-03,  2.9882e-03, -6.8553e-02, -5.3306e-03, -6.2597e-02,\n",
      "         2.9147e-03, -7.7687e-02,  8.4316e-03, -8.7837e-02, -3.6052e-02,\n",
      "        -6.1484e-02, -3.9371e-02, -5.6772e-02,  1.6694e-02, -7.3766e-02,\n",
      "        -6.8216e-02, -1.7216e-02, -9.1906e-02, -1.4677e-02, -5.8619e-02,\n",
      "        -2.5046e-02, -7.2140e-02, -2.1852e-02,  1.9906e-02, -2.6993e-02,\n",
      "         7.3274e-04, -1.0010e-01, -4.3407e-02, -5.1010e-02, -1.5104e-02,\n",
      "        -4.4110e-02,  2.2351e-02, -5.4132e-02,  1.6115e-02, -8.0469e-02,\n",
      "        -8.7338e-02, -8.1021e-02, -4.3518e-02, -9.5842e-02, -6.7150e-02,\n",
      "         2.0641e-02, -4.5420e-02, -5.5775e-02, -8.8555e-02, -8.9499e-02,\n",
      "        -7.2043e-02, -2.4797e-02,  3.2445e-03, -2.8026e-02, -6.9087e-02,\n",
      "        -1.0108e-02,  1.9592e-02, -8.0607e-02, -6.1777e-02, -9.4543e-02,\n",
      "        -1.9409e-02,  2.3438e-02, -1.0797e-01,  2.0545e-02, -8.8642e-02,\n",
      "        -4.1047e-02,  3.7210e-03, -7.4536e-02], requires_grad=True))\n",
      "encoder.5.1.block.1.2.scale \t tensor(1.)\n",
      "encoder.5.1.block.1.2.zero_point \t tensor(0)\n",
      "encoder.5.1.block.1.2._packed_params.dtype \t torch.qint8\n",
      "encoder.5.1.block.1.2._packed_params._packed_params \t (tensor([[-0.0050, -0.0288,  0.0070,  ...,  0.0298, -0.0159,  0.0577],\n",
      "        [ 0.0040, -0.0109, -0.0209,  ..., -0.0219,  0.0249, -0.0298],\n",
      "        [ 0.0010, -0.0139, -0.0239,  ...,  0.0010,  0.0348,  0.0199],\n",
      "        ...,\n",
      "        [ 0.0537,  0.0318,  0.0139,  ..., -0.0586, -0.0189,  0.0328],\n",
      "        [-0.0408,  0.0119, -0.0060,  ..., -0.0159, -0.0308,  0.0000],\n",
      "        [-0.0040,  0.0070, -0.0199,  ..., -0.0239, -0.0219,  0.0467]],\n",
      "       size=(192, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0009940473828464746,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-0.0194,  0.0254, -0.0203, -0.0199, -0.0243, -0.0069, -0.0136, -0.0582,\n",
      "        -0.0227, -0.0327,  0.0329, -0.0464, -0.0392, -0.0214, -0.0446,  0.0046,\n",
      "         0.0055, -0.0065, -0.0337, -0.0248, -0.0126, -0.0394,  0.0045,  0.0403,\n",
      "        -0.0639, -0.0018, -0.0183, -0.0061, -0.0215,  0.0451,  0.0399,  0.0129,\n",
      "        -0.0182, -0.0239,  0.0075, -0.0229, -0.0035,  0.0238, -0.0531,  0.0512,\n",
      "        -0.0174,  0.0413,  0.0022, -0.0223,  0.0430,  0.0171, -0.0421,  0.0237,\n",
      "        -0.0243,  0.0053,  0.0091, -0.0041,  0.0167,  0.0076,  0.0212,  0.0257,\n",
      "        -0.0699, -0.0058,  0.0272,  0.0222,  0.0428,  0.0187,  0.0016, -0.0123,\n",
      "        -0.0100,  0.0262, -0.0195, -0.0395, -0.0008, -0.0043,  0.0463,  0.0122,\n",
      "        -0.0086, -0.0116, -0.0118,  0.0236, -0.0223,  0.0123,  0.0279, -0.0174,\n",
      "         0.0072,  0.0207, -0.0615, -0.0288,  0.0168, -0.0034, -0.0621, -0.0405,\n",
      "         0.0206, -0.0458, -0.0027, -0.0388, -0.0273, -0.0041, -0.0092, -0.0042,\n",
      "        -0.0038,  0.0133,  0.0277,  0.0044,  0.0326,  0.0070,  0.0408, -0.0145,\n",
      "        -0.0492, -0.0221,  0.0071, -0.0129, -0.0378, -0.0335, -0.0141,  0.0194,\n",
      "        -0.0021, -0.0069,  0.0173,  0.0424, -0.0395,  0.0204,  0.0165,  0.0217,\n",
      "         0.0277,  0.0295,  0.0303,  0.0202, -0.0424, -0.0124, -0.0359, -0.0174,\n",
      "        -0.0499, -0.0027, -0.0631,  0.0212, -0.0103, -0.0139,  0.0021, -0.0471,\n",
      "         0.0441,  0.0053,  0.0120,  0.0124, -0.0102,  0.0196,  0.0304,  0.0376,\n",
      "         0.0525, -0.0325, -0.0170, -0.0150, -0.0470, -0.0126,  0.0125, -0.0052,\n",
      "        -0.0143, -0.0201, -0.0107,  0.0307, -0.0320,  0.0295,  0.0058,  0.0091,\n",
      "        -0.0508,  0.0039, -0.0049,  0.0529,  0.0545,  0.0322,  0.0277, -0.0391,\n",
      "        -0.0028, -0.0307, -0.0272, -0.0154,  0.0271,  0.0086, -0.0351, -0.0430,\n",
      "        -0.0418,  0.0200, -0.0594,  0.0014,  0.0111, -0.0702,  0.0179,  0.0076,\n",
      "         0.0196,  0.0031, -0.0122, -0.0044, -0.0413, -0.0239, -0.0300,  0.0178],\n",
      "       requires_grad=True))\n",
      "classifier.1.scale \t tensor(1.)\n",
      "classifier.1.zero_point \t tensor(0)\n",
      "classifier.1._packed_params.dtype \t torch.qint8\n",
      "classifier.1._packed_params._packed_params \t (tensor([[-0.0519, -0.0047, -0.0031,  ...,  0.0818, -0.0189,  0.0094],\n",
      "        [ 0.0126,  0.0488,  0.0016,  ..., -0.0189, -0.0740,  0.0692],\n",
      "        [ 0.0220,  0.0724,  0.0771,  ..., -0.0708,  0.0567, -0.0283],\n",
      "        ...,\n",
      "        [ 0.0330, -0.0582, -0.0708,  ..., -0.0063, -0.0866,  0.0346],\n",
      "        [ 0.0629,  0.0708, -0.0567,  ...,  0.0944, -0.0299,  0.0629],\n",
      "        [-0.0079,  0.0362, -0.0629,  ..., -0.0787, -0.0488, -0.0142]],\n",
      "       size=(192, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0015736657660454512,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([-6.6232e-02,  4.7674e-02,  8.0487e-03, -5.2742e-02,  3.7137e-02,\n",
      "        -7.1006e-02,  4.7896e-02, -6.5654e-02, -6.1890e-02,  5.0832e-02,\n",
      "         1.6490e-03, -1.7313e-02,  7.1148e-02,  6.3303e-02, -1.0062e-02,\n",
      "         2.6146e-02, -5.7747e-02, -2.0599e-03, -3.7149e-02,  1.3092e-02,\n",
      "        -5.7639e-02,  7.3848e-02,  5.8929e-02, -5.9187e-02,  5.5090e-02,\n",
      "         6.7093e-03,  2.8539e-03,  4.4694e-03,  4.8093e-02,  6.7532e-02,\n",
      "         3.7768e-02,  5.5335e-02, -3.2916e-02, -1.5962e-02, -6.2855e-02,\n",
      "         5.2796e-02, -3.7437e-02, -1.5723e-02, -7.9997e-02,  7.4246e-02,\n",
      "         2.1174e-02, -4.4730e-02,  2.8419e-02,  4.3670e-02,  2.5071e-02,\n",
      "        -1.2603e-02, -3.4341e-02, -4.9038e-02, -5.2805e-02,  3.4661e-02,\n",
      "         2.5995e-02, -4.5936e-02,  6.0379e-02,  9.2535e-03, -1.0034e-02,\n",
      "         3.3461e-02, -7.1028e-03, -1.9339e-02,  1.1325e-01,  6.5036e-02,\n",
      "        -3.2130e-02,  5.4676e-02, -3.4531e-02, -3.6790e-03, -2.3024e-02,\n",
      "         3.6277e-02, -2.7980e-02,  5.3887e-02, -7.1066e-02,  4.9569e-02,\n",
      "        -3.5677e-02, -5.9209e-02, -7.3083e-02, -1.2568e-02, -5.0509e-02,\n",
      "        -6.0469e-02,  1.8622e-02,  6.1848e-02,  5.7495e-02,  1.6473e-02,\n",
      "        -6.6148e-02, -4.6984e-02,  1.8514e-02,  8.8724e-04, -7.1147e-02,\n",
      "         3.7304e-02, -1.1016e-02,  5.4164e-02, -5.4961e-03,  4.9584e-02,\n",
      "         3.2435e-02,  1.7126e-02,  3.6756e-03,  3.9992e-02, -4.5986e-02,\n",
      "         5.3193e-03,  5.9277e-02, -4.8579e-02, -5.2104e-02, -7.3354e-05,\n",
      "        -4.2325e-02,  1.6680e-02,  4.7784e-02, -8.5512e-02, -3.5777e-03,\n",
      "        -9.7562e-03, -7.4634e-02,  1.9937e-03, -6.2315e-02, -1.0537e-02,\n",
      "         2.1422e-02, -1.6925e-02, -6.7007e-02,  3.0307e-02, -2.4355e-03,\n",
      "         9.8077e-03,  4.5417e-02,  1.7266e-02, -3.5576e-02, -5.0942e-02,\n",
      "         4.3087e-02,  3.5423e-02,  6.7056e-03,  4.0251e-02, -8.9146e-02,\n",
      "        -9.9871e-03,  2.8595e-02,  6.2791e-03,  8.7054e-02,  5.7885e-02,\n",
      "         1.7846e-02,  2.1150e-03,  5.7663e-02,  2.5301e-02, -2.8113e-02,\n",
      "        -7.4322e-02, -8.1143e-02,  6.7017e-02, -4.4418e-02,  2.9720e-02,\n",
      "        -8.4747e-03, -6.1226e-02,  3.5240e-02,  6.0893e-02, -7.2417e-02,\n",
      "         1.0475e-02, -8.6237e-02,  2.3457e-02,  3.1731e-02,  5.1344e-02,\n",
      "        -4.1749e-02,  3.4333e-02, -4.2654e-02,  1.1977e-03, -3.0860e-02,\n",
      "        -6.4396e-02, -2.5480e-02,  2.6245e-02, -1.6239e-02,  1.7572e-02,\n",
      "        -3.8087e-02,  2.6935e-02,  6.5730e-02,  5.8837e-02,  5.5664e-02,\n",
      "        -7.5334e-03,  5.1864e-02, -7.1958e-02,  4.8928e-02,  1.0968e-02,\n",
      "         4.2143e-02, -5.6790e-02, -4.3635e-02,  1.0009e-02,  3.9158e-02,\n",
      "        -2.1027e-02,  1.5341e-02,  1.2530e-02, -1.3934e-02, -5.8229e-02,\n",
      "        -5.1306e-03,  4.5691e-02, -1.8482e-02, -4.0648e-02,  4.7596e-02,\n",
      "        -9.5196e-03,  5.9378e-02,  6.7778e-02,  3.7973e-02,  1.9938e-02,\n",
      "         5.5112e-03,  3.8456e-02], requires_grad=True))\n",
      "classifier.2.weight \t tensor([0.9869, 1.0017, 1.0445, 1.0618, 1.0261, 0.9845, 1.0502, 1.0739, 1.0209,\n",
      "        1.0552, 1.1130, 0.9620, 1.0001, 1.0461, 1.0402, 1.0090, 0.9988, 1.0399,\n",
      "        1.1220, 1.0196, 1.0275, 1.0092, 1.0113, 1.0440, 1.0050, 1.0488, 1.0299,\n",
      "        1.0490, 1.0821, 1.0643, 1.0137, 1.0469, 0.9998, 1.0563, 1.0224, 1.0165,\n",
      "        1.0169, 1.0310, 1.1327, 0.9927, 1.0409, 1.0055, 1.0865, 0.9970, 0.9912,\n",
      "        1.0918, 1.0617, 1.0404, 1.0043, 0.9907, 1.0316, 1.0464, 1.0421, 1.0968,\n",
      "        1.0833, 1.0448, 1.1120, 1.0754, 1.0049, 0.9991, 1.0836, 1.0625, 1.0337,\n",
      "        1.0181, 1.0757, 1.0204, 1.0076, 1.0276, 1.0606, 1.0179, 1.0378, 1.0282,\n",
      "        1.0505, 0.9807, 0.9817, 0.9748, 1.0157, 1.0649, 1.0531, 1.0021, 1.0816,\n",
      "        1.0403, 0.9454, 1.0773, 0.9735, 1.0431, 1.0451, 1.0531, 1.0148, 1.0311,\n",
      "        1.0865, 1.0806, 0.9906, 0.9319, 1.0350, 1.0353, 1.0686, 0.9949, 1.0253,\n",
      "        0.9975, 1.1136, 0.9987, 0.9923, 0.9983, 1.0667, 1.0589, 1.0014, 0.9779,\n",
      "        0.9920, 1.1052, 1.0173, 1.0051, 1.0214, 1.0351, 1.0635, 1.0168, 1.0959,\n",
      "        1.0390, 1.0227, 0.9165, 1.0484, 1.0200, 1.0341, 1.0542, 1.0412, 1.0196,\n",
      "        0.9724, 1.0765, 1.1242, 1.0140, 1.1116, 1.0503, 1.0084, 1.0289, 0.9866,\n",
      "        1.0158, 1.0473, 0.9861, 1.1162, 1.0454, 0.9890, 1.0933, 1.0417, 1.1444,\n",
      "        1.0366, 1.0735, 0.9711, 1.0191, 0.9735, 1.0057, 1.0309, 1.0659, 0.9955,\n",
      "        1.0462, 1.1000, 1.1156, 1.1235, 1.0316, 1.0128, 1.0709, 1.0543, 1.0088,\n",
      "        1.0747, 1.0815, 1.0378, 1.0953, 0.9911, 1.0275, 1.0463, 1.0469, 1.0702,\n",
      "        1.0340, 1.0036, 1.0018, 1.0244, 1.0388, 1.0135, 0.9978, 1.1183, 1.0231,\n",
      "        1.0744, 1.0719, 1.0091, 1.0237, 0.9744, 1.0028, 1.0637, 1.0180, 1.0187,\n",
      "        1.0806, 1.0167, 1.0144])\n",
      "classifier.2.bias \t tensor([ 0.0211,  0.0312, -0.0157, -0.0103,  0.0441, -0.0115, -0.0159, -0.0180,\n",
      "        -0.0482, -0.0108, -0.0136, -0.0374, -0.0385,  0.0342, -0.0187,  0.0440,\n",
      "        -0.0430,  0.0021, -0.0053, -0.0638, -0.0624, -0.0447, -0.0449,  0.0447,\n",
      "        -0.0587,  0.0352, -0.0064, -0.0103,  0.0438, -0.0091, -0.0622,  0.0040,\n",
      "         0.0062,  0.0429,  0.0551,  0.0272,  0.0192, -0.0076,  0.0357, -0.0351,\n",
      "         0.0168,  0.0575,  0.0036, -0.0578,  0.0576,  0.0122, -0.0006, -0.0013,\n",
      "        -0.0114,  0.0337,  0.0471,  0.0285,  0.0209,  0.0424,  0.0227, -0.0083,\n",
      "        -0.0042,  0.0059, -0.0159, -0.0361, -0.0340,  0.0399,  0.0155, -0.0009,\n",
      "         0.0218,  0.0360,  0.0483,  0.0377,  0.0258, -0.0445,  0.0384,  0.0406,\n",
      "         0.0117, -0.0412,  0.0286,  0.0122,  0.0236,  0.0218, -0.0215,  0.0458,\n",
      "         0.0521,  0.0001, -0.0437, -0.0203,  0.0565, -0.0298,  0.0134,  0.0128,\n",
      "        -0.0195, -0.0484,  0.0038, -0.0576,  0.0058,  0.0328, -0.0467,  0.0381,\n",
      "         0.0008,  0.0343, -0.0522, -0.0519,  0.0159, -0.0380,  0.0024,  0.0163,\n",
      "         0.0327, -0.0396,  0.0672,  0.0468,  0.0260,  0.0043, -0.0124, -0.0494,\n",
      "         0.0283, -0.0452,  0.0357,  0.0512,  0.0237, -0.0273, -0.0009,  0.0287,\n",
      "        -0.0073, -0.0030, -0.0380,  0.0149,  0.0339,  0.0278, -0.0016,  0.0346,\n",
      "        -0.0139,  0.0476,  0.0088, -0.0215, -0.0337, -0.0259,  0.0553,  0.0131,\n",
      "         0.0370,  0.0470, -0.0239, -0.0632, -0.0160, -0.0039, -0.0066, -0.0047,\n",
      "         0.0346,  0.0317,  0.0725, -0.0442, -0.0428,  0.0668,  0.0157, -0.0510,\n",
      "         0.0038,  0.0262, -0.0455,  0.0337, -0.0022, -0.0106,  0.0528,  0.0465,\n",
      "        -0.0130, -0.0285, -0.0552,  0.0253,  0.0047,  0.0412, -0.0422,  0.0125,\n",
      "         0.0216,  0.0275, -0.0158, -0.0504,  0.0145, -0.0485, -0.0143, -0.0037,\n",
      "        -0.0127, -0.0068,  0.0054, -0.0416,  0.0032,  0.0027,  0.0342, -0.0289,\n",
      "        -0.0696, -0.0165,  0.0171, -0.0081,  0.0089,  0.0357,  0.0612, -0.0186])\n",
      "classifier.3.scale \t tensor(1.)\n",
      "classifier.3.zero_point \t tensor(0)\n",
      "classifier.3._packed_params.dtype \t torch.qint8\n",
      "classifier.3._packed_params._packed_params \t (tensor([[ 0.0401,  0.0225, -0.0827,  ..., -0.0451, -0.0050,  0.0276],\n",
      "        [ 0.0251,  0.0676, -0.0777,  ...,  0.0326,  0.0877,  0.0251],\n",
      "        [ 0.0952, -0.0225, -0.0251,  ...,  0.0501,  0.0877, -0.1002],\n",
      "        [ 0.0050,  0.0426, -0.0050,  ..., -0.0651,  0.1002,  0.0727],\n",
      "        [ 0.0150, -0.0551,  0.0977,  ..., -0.2581, -0.0150,  0.0752],\n",
      "        [-0.0326,  0.0276, -0.0551,  ...,  0.0476, -0.0200, -0.0025]],\n",
      "       size=(6, 192), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0025054076686501503,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.0030,  0.0612,  0.0568, -0.0453, -0.0455, -0.0555],\n",
      "       requires_grad=True))\n",
      "embedding.cls_token \t tensor([[ 2.0106e+00, -1.0699e+00, -1.8253e+00, -7.8830e-01, -5.2101e-01,\n",
      "          2.1935e+00, -1.4414e+00,  1.3472e-01,  1.3371e+00, -1.8771e+00,\n",
      "         -6.4035e-01,  1.6987e+00,  1.7369e+00, -1.1074e+00,  4.2047e-02,\n",
      "          2.0465e-01,  7.2741e-01, -9.2241e-02,  1.0328e+00, -1.2076e-03,\n",
      "          1.6877e-01,  7.3198e-01, -2.3556e-01,  1.0377e+00,  9.6358e-01,\n",
      "          4.7341e-01, -2.9005e-01, -5.3149e-01,  5.7666e-01, -1.3720e-01,\n",
      "          1.4143e+00, -2.3384e-01,  1.3291e+00, -8.4292e-01,  4.0485e-01,\n",
      "          2.2890e+00,  1.0640e+00, -1.0492e+00, -9.4586e-01, -5.6292e-01,\n",
      "         -9.3332e-01,  6.6650e-02,  8.2287e-02,  9.9471e-01, -8.5265e-01,\n",
      "          1.7183e-01, -9.8757e-02,  1.6658e+00, -1.8893e+00, -1.7021e+00,\n",
      "          1.5398e+00, -1.3970e+00,  7.2036e-01,  6.9653e-01, -8.8098e-01,\n",
      "         -2.7534e-01, -2.2726e+00, -1.4202e+00, -1.0818e-01, -8.9872e-01,\n",
      "          2.0553e-01,  2.4847e-01, -1.4792e-01, -1.8536e-02, -9.1619e-02,\n",
      "         -7.7899e-01,  4.9590e-01, -1.0187e+00,  1.6931e-01,  7.2268e-01,\n",
      "         -6.7333e-01,  3.8499e-01, -1.7614e-01, -1.6451e-01, -3.5300e-01,\n",
      "         -6.3716e-01, -8.7161e-01,  8.6106e-01, -9.7389e-01, -8.3638e-01,\n",
      "          1.2089e-01,  1.8568e+00,  1.0119e+00, -1.1259e+00, -5.9346e-02,\n",
      "          4.9754e-01,  8.0902e-01,  7.3223e-01,  6.1771e-02, -1.9669e-01,\n",
      "         -5.7744e-01, -1.1526e-01, -2.3506e+00,  1.3354e-01,  6.6784e-01,\n",
      "          1.2981e+00, -9.1771e-01, -1.4974e+00,  1.1102e-01,  3.9912e-01,\n",
      "          2.5406e-01, -8.0418e-01, -1.7687e+00,  3.0916e-01, -1.0002e+00,\n",
      "         -1.8928e+00, -2.2125e+00,  5.5094e-01, -6.5253e-01,  9.4449e-01,\n",
      "          3.0353e-01,  8.2959e-01, -6.7539e-01,  6.5817e-01,  1.4936e+00,\n",
      "          2.2701e-01, -1.2311e+00, -1.8437e+00,  5.3194e-01,  4.1587e-01,\n",
      "          3.6991e-01, -1.1577e+00, -1.7253e-01, -1.3441e+00, -3.3803e-01,\n",
      "          5.3091e-01, -2.3032e-02,  9.0111e-01,  1.3162e+00, -2.8543e+00,\n",
      "         -3.1930e+00, -1.1025e-01,  8.1693e-03,  3.0688e-01,  1.5674e+00,\n",
      "         -6.3540e-01, -8.8699e-01, -9.7669e-01, -4.0867e-02, -1.1098e+00,\n",
      "         -7.7981e-01,  1.0809e+00, -9.6500e-01,  2.9775e+00,  1.1593e+00,\n",
      "          2.0744e-01,  2.5426e-01,  1.7376e+00,  2.2874e-01,  3.7374e-01,\n",
      "          8.1094e-01,  1.5457e+00, -7.0854e-01,  2.9528e+00, -2.1245e+00,\n",
      "          6.9321e-01, -9.8095e-01, -2.7427e+00,  1.2351e+00,  1.6874e-01,\n",
      "          1.3705e+00, -6.5401e-01, -1.0135e+00, -3.2540e-01,  8.1939e-01,\n",
      "          3.0842e-01,  2.6935e+00, -9.6694e-01,  6.2653e-01,  2.4580e-01,\n",
      "          6.8415e-01,  4.8263e-01,  9.0408e-01, -1.5090e+00, -1.7403e-01,\n",
      "         -8.5543e-01,  7.8173e-01,  9.6964e-01,  1.4504e+00,  1.7031e+00,\n",
      "          5.5484e-01,  7.4447e-02,  1.6451e+00,  7.2010e-01, -1.7024e+00,\n",
      "         -1.8969e+00,  7.7159e-01,  2.9893e-01, -1.7432e+00,  8.8721e-01,\n",
      "          1.5685e-01, -2.2769e+00]])\n",
      "embedding.0.scale \t tensor(1.)\n",
      "embedding.0.zero_point \t tensor(0)\n",
      "embedding.0._packed_params.dtype \t torch.qint8\n",
      "embedding.0._packed_params._packed_params \t (tensor([[ 0.0373],\n",
      "        [ 0.5219],\n",
      "        [-0.2703],\n",
      "        [-0.0932],\n",
      "        [-0.1864],\n",
      "        [-0.9599],\n",
      "        [ 0.5871],\n",
      "        [ 0.3355],\n",
      "        [ 0.1118],\n",
      "        [ 0.8574],\n",
      "        [-0.9040],\n",
      "        [-0.3448],\n",
      "        [ 0.1305],\n",
      "        [ 0.4473],\n",
      "        [-0.1771],\n",
      "        [ 0.6244],\n",
      "        [-0.1864],\n",
      "        [ 0.6431],\n",
      "        [ 0.6803],\n",
      "        [ 1.0158],\n",
      "        [-0.4846],\n",
      "        [ 0.8108],\n",
      "        [ 1.1743],\n",
      "        [-0.7642],\n",
      "        [ 0.5219],\n",
      "        [-0.2609],\n",
      "        [-0.8854],\n",
      "        [ 0.8481],\n",
      "        [ 0.5778],\n",
      "        [-0.2144],\n",
      "        [-0.2237],\n",
      "        [ 1.0718],\n",
      "        [ 1.0345],\n",
      "        [-0.6617],\n",
      "        [ 0.6431],\n",
      "        [ 0.9133],\n",
      "        [ 0.3728],\n",
      "        [-0.2982],\n",
      "        [-0.8015],\n",
      "        [-0.5405],\n",
      "        [-0.0466],\n",
      "        [ 0.6151],\n",
      "        [ 0.5592],\n",
      "        [ 0.9786],\n",
      "        [ 0.7083],\n",
      "        [ 0.6710],\n",
      "        [-0.5778],\n",
      "        [-0.5871],\n",
      "        [-0.9599],\n",
      "        [ 0.9599],\n",
      "        [ 0.0000],\n",
      "        [-0.8481],\n",
      "        [ 0.8294],\n",
      "        [ 0.0280],\n",
      "        [-0.6337],\n",
      "        [ 0.1771],\n",
      "        [ 0.4473],\n",
      "        [-0.6337],\n",
      "        [-0.4473],\n",
      "        [ 0.2703],\n",
      "        [ 0.1864],\n",
      "        [ 0.4846],\n",
      "        [-0.9506],\n",
      "        [-1.0811],\n",
      "        [-0.3355],\n",
      "        [ 0.2050],\n",
      "        [ 0.2889],\n",
      "        [ 0.7083],\n",
      "        [-0.0373],\n",
      "        [-0.3169],\n",
      "        [-0.7828],\n",
      "        [ 0.2050],\n",
      "        [ 0.4007],\n",
      "        [-0.4567],\n",
      "        [ 0.0559],\n",
      "        [ 0.8108],\n",
      "        [-1.0904],\n",
      "        [ 0.1771],\n",
      "        [ 1.1277],\n",
      "        [ 0.5219],\n",
      "        [ 1.0065],\n",
      "        [ 0.1678],\n",
      "        [-0.4939],\n",
      "        [ 0.2237],\n",
      "        [ 0.2423],\n",
      "        [-0.6803],\n",
      "        [-1.0904],\n",
      "        [-0.2796],\n",
      "        [-0.9226],\n",
      "        [-0.7083],\n",
      "        [ 0.3448],\n",
      "        [ 0.3448],\n",
      "        [-0.3728],\n",
      "        [ 0.8667],\n",
      "        [ 0.1864],\n",
      "        [-0.7549],\n",
      "        [ 0.3262],\n",
      "        [ 0.0839],\n",
      "        [ 0.0652],\n",
      "        [-0.6151],\n",
      "        [-0.6524],\n",
      "        [ 0.2889],\n",
      "        [ 0.6803],\n",
      "        [-0.1491],\n",
      "        [-0.3541],\n",
      "        [ 0.4473],\n",
      "        [ 0.9226],\n",
      "        [ 0.7083],\n",
      "        [ 0.9133],\n",
      "        [-0.0652],\n",
      "        [-0.0186],\n",
      "        [ 1.0345],\n",
      "        [ 0.6897],\n",
      "        [-0.0746],\n",
      "        [ 0.6803],\n",
      "        [-0.0652],\n",
      "        [ 0.3821],\n",
      "        [ 0.0466],\n",
      "        [-0.8574],\n",
      "        [-0.6524],\n",
      "        [-1.1929],\n",
      "        [ 0.6617],\n",
      "        [ 0.6337],\n",
      "        [ 0.2516],\n",
      "        [-1.1184],\n",
      "        [-0.4660],\n",
      "        [-0.7456],\n",
      "        [-0.7922],\n",
      "        [-0.9972],\n",
      "        [ 1.0531],\n",
      "        [-0.5312],\n",
      "        [-0.3169],\n",
      "        [-0.5405],\n",
      "        [-0.0746],\n",
      "        [ 0.6151],\n",
      "        [-0.9972],\n",
      "        [ 0.2982],\n",
      "        [-0.0746],\n",
      "        [-0.4473],\n",
      "        [ 0.6151],\n",
      "        [ 0.0839],\n",
      "        [ 0.0839],\n",
      "        [-0.8201],\n",
      "        [ 1.1277],\n",
      "        [ 0.5871],\n",
      "        [-1.0158],\n",
      "        [-1.1370],\n",
      "        [-0.9133],\n",
      "        [-0.0280],\n",
      "        [ 0.7922],\n",
      "        [ 0.0839],\n",
      "        [ 0.2609],\n",
      "        [ 0.2516],\n",
      "        [ 0.3914],\n",
      "        [-0.5219],\n",
      "        [-0.2330],\n",
      "        [ 0.2609],\n",
      "        [-0.6431],\n",
      "        [ 0.5312],\n",
      "        [ 0.9506],\n",
      "        [-0.7642],\n",
      "        [ 0.2889],\n",
      "        [ 0.4380],\n",
      "        [ 0.9226],\n",
      "        [ 0.7642],\n",
      "        [ 0.5405],\n",
      "        [ 0.1398],\n",
      "        [-0.7549],\n",
      "        [-0.2237],\n",
      "        [-0.7269],\n",
      "        [ 0.2982],\n",
      "        [-0.6431],\n",
      "        [ 0.3448],\n",
      "        [ 0.2516],\n",
      "        [-0.9413],\n",
      "        [-0.4194],\n",
      "        [-0.1398],\n",
      "        [ 0.7269],\n",
      "        [-0.4939],\n",
      "        [ 0.4473],\n",
      "        [ 0.3169],\n",
      "        [-0.6897],\n",
      "        [-0.7828],\n",
      "        [-0.9879],\n",
      "        [-0.0466],\n",
      "        [-0.3262],\n",
      "        [-0.1957],\n",
      "        [ 0.4846],\n",
      "        [ 1.0252],\n",
      "        [ 0.8667],\n",
      "        [ 0.9972],\n",
      "        [-0.1771]], size=(192, 1), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.00931959692388773,\n",
      "       zero_point=0), Parameter containing:\n",
      "tensor([ 0.5157, -0.4157,  0.1059, -0.1993, -0.2733, -0.0507, -0.4022,  0.7830,\n",
      "        -0.7480,  0.5301, -0.2230,  0.7724, -0.7556, -0.1024,  0.9087, -0.4279,\n",
      "        -0.1622, -0.7535,  0.4609,  0.4203, -0.5164,  0.2495, -0.5575, -0.6687,\n",
      "         0.8049, -0.6355, -0.3579, -0.0517, -0.0380, -0.3042, -0.6721,  0.1879,\n",
      "         0.4620, -0.1379, -0.6577,  0.7754, -0.7931,  0.0524,  0.1953, -0.6052,\n",
      "         0.4203, -0.3695, -0.4985,  0.5435, -0.1670,  0.2063,  0.5799,  0.6130,\n",
      "         0.5743,  0.1352,  0.5085, -0.6985,  0.5345, -0.3661, -0.7002,  0.5156,\n",
      "         0.7824,  0.5385,  0.1346, -0.7095, -0.3472, -0.1990,  0.3898,  0.6926,\n",
      "        -0.2622, -0.4160, -0.8261,  0.6653, -0.7143, -0.7089, -0.7925, -0.4369,\n",
      "        -0.6869, -0.4773,  0.6371, -0.5748,  0.5788,  0.7707, -0.5661, -0.3024,\n",
      "        -0.7627, -0.2950,  0.8774,  0.7776, -0.3480, -0.4833,  0.8334,  0.3130,\n",
      "        -0.3393,  0.7983, -0.7128, -0.1359,  0.6291, -0.1718, -0.2956,  0.2687,\n",
      "         0.7946, -0.4943, -0.7029,  0.2607, -0.5841, -0.4423, -0.2577, -0.4865,\n",
      "         0.4759,  0.2266,  0.3662,  0.5550,  0.4066, -0.0358,  0.4459,  0.0506,\n",
      "        -0.1777, -0.3225, -0.7439,  0.0798,  0.6307, -0.1381, -0.2132,  0.1352,\n",
      "         0.4390, -0.4644, -0.5964,  0.3286,  0.5983, -0.6488,  0.7214,  0.4765,\n",
      "         0.8370, -0.8828,  0.9281,  0.2169, -0.6953, -0.5550, -0.6141,  0.4269,\n",
      "        -0.2672, -0.7277, -0.2311,  0.1824,  0.1948, -0.0344,  0.0109,  0.1360,\n",
      "         0.1929,  1.0145,  0.2862,  0.4388, -0.3200,  0.5581,  0.0755, -0.4930,\n",
      "         0.2165, -0.7465,  0.4604, -0.0131,  0.5485, -0.3387, -0.7040, -0.6420,\n",
      "         0.7665, -0.4658, -0.5849, -0.5633, -0.4500, -0.6069, -0.3657,  0.7578,\n",
      "        -0.7927,  0.8805,  0.0766,  0.3725, -0.6927, -0.4523,  0.9125,  0.6106,\n",
      "         0.6328, -0.0684, -0.6845,  0.3573, -0.0024,  0.8368, -0.5532,  0.4479,\n",
      "        -0.6699, -0.4860, -0.5165, -0.2264,  0.7559,  0.5558,  0.3257,  0.8727],\n",
      "       requires_grad=True))\n",
      "embedding.1.weight \t tensor([0.9763, 1.1794, 1.0540, 0.9597, 1.0171, 1.0861, 1.0550, 0.9394, 0.9926,\n",
      "        1.0066, 1.0856, 1.0469, 1.0351, 1.0689, 1.0363, 1.0783, 0.9876, 1.0140,\n",
      "        0.9533, 1.0099, 1.0908, 1.0059, 1.3189, 1.0094, 0.9708, 1.0146, 0.9981,\n",
      "        1.0783, 1.0453, 1.0456, 1.0470, 1.0382, 0.9861, 0.9522, 1.0071, 0.9453,\n",
      "        1.0351, 1.0156, 1.0955, 1.0577, 0.9811, 1.0590, 1.1276, 1.0049, 1.0481,\n",
      "        1.0446, 1.1219, 1.0959, 1.1301, 1.0836, 1.0246, 1.0260, 1.0008, 0.9655,\n",
      "        1.0157, 0.9914, 0.9598, 1.1086, 1.0758, 0.9941, 0.9535, 1.0444, 1.1592,\n",
      "        1.1205, 1.0030, 0.9549, 1.0745, 0.9694, 1.0330, 1.0165, 1.0218, 0.9385,\n",
      "        1.0227, 1.0286, 0.9911, 1.2549, 1.1891, 0.9531, 1.2683, 1.0619, 1.1554,\n",
      "        0.9922, 1.0853, 0.9166, 0.9572, 1.0368, 1.1463, 1.0610, 1.0578, 1.1370,\n",
      "        1.0092, 1.0657, 1.0425, 1.1585, 1.0037, 1.1291, 0.9353, 0.9857, 1.0155,\n",
      "        1.0933, 1.0011, 0.9182, 1.1013, 0.9631, 1.0593, 1.0023, 1.0260, 0.9553,\n",
      "        1.0156, 0.9996, 0.9892, 1.1065, 1.0792, 0.9759, 1.0127, 0.9863, 0.9858,\n",
      "        0.9700, 1.0409, 1.0783, 1.2104, 1.2052, 1.0515, 0.9951, 1.1813, 0.9933,\n",
      "        1.1504, 1.1209, 1.1482, 1.2555, 1.0844, 1.0605, 1.0313, 1.0203, 0.9316,\n",
      "        1.1468, 1.0221, 1.0201, 1.0245, 1.0113, 1.0008, 1.0367, 1.0388, 1.1096,\n",
      "        1.0418, 1.1412, 1.2091, 1.1846, 1.0231, 0.9411, 1.0144, 1.0066, 0.9903,\n",
      "        0.9935, 1.1429, 0.9555, 0.9565, 0.9944, 0.9893, 1.2564, 1.1078, 0.9146,\n",
      "        1.0088, 1.2993, 1.2671, 1.0480, 0.9761, 1.1506, 1.0120, 1.1055, 1.0566,\n",
      "        1.1127, 1.0153, 1.0457, 1.1703, 1.0813, 0.9947, 1.0804, 0.9989, 1.0018,\n",
      "        1.0076, 1.1038, 0.9962, 1.1952, 1.0173, 1.0891, 0.9991, 1.0901, 0.9804,\n",
      "        0.9743, 1.0594, 1.0090])\n",
      "embedding.1.bias \t tensor([-0.0265,  0.0628,  0.0573,  0.0398, -0.0323,  0.0412,  0.1057, -0.0695,\n",
      "         0.0037, -0.0364, -0.0133,  0.0017, -0.0350, -0.0004, -0.0065,  0.0929,\n",
      "         0.0097,  0.0426, -0.0618, -0.0552, -0.0736, -0.0351,  0.0725,  0.0048,\n",
      "        -0.0417, -0.0157,  0.0192,  0.0248,  0.0133, -0.0402, -0.0453, -0.0204,\n",
      "        -0.0670, -0.0302,  0.0609, -0.0912, -0.0416,  0.0208,  0.0159, -0.0375,\n",
      "        -0.0290,  0.1406, -0.0038, -0.0366,  0.0170, -0.0198,  0.0465,  0.0251,\n",
      "         0.0560,  0.0046,  0.0174, -0.0021, -0.0390,  0.0264, -0.0049, -0.0157,\n",
      "        -0.0510,  0.0661,  0.0324,  0.0055,  0.0813,  0.0197,  0.0602,  0.0469,\n",
      "        -0.0316, -0.0023, -0.0734, -0.0577, -0.0409, -0.0119, -0.0172,  0.0871,\n",
      "        -0.0263, -0.0280, -0.0237,  0.1014,  0.0980, -0.0521,  0.1165,  0.0728,\n",
      "         0.0746,  0.0383, -0.0102, -0.0889,  0.0240, -0.0354,  0.0722,  0.0395,\n",
      "        -0.0383,  0.0895, -0.0138, -0.0048, -0.0021,  0.0696,  0.0072,  0.0687,\n",
      "        -0.0797,  0.0032, -0.0227,  0.0569,  0.0048,  0.1153,  0.0615,  0.0319,\n",
      "         0.0181, -0.0179, -0.0347, -0.0777, -0.0379,  0.0058, -0.0203,  0.0123,\n",
      "         0.0081,  0.0226,  0.0611,  0.0054, -0.0205,  0.0108, -0.0388,  0.0058,\n",
      "         0.0711,  0.0434,  0.0114, -0.0059,  0.0662,  0.0088,  0.0771,  0.0368,\n",
      "         0.0515,  0.0322, -0.0170,  0.0227, -0.0295, -0.0236,  0.1625,  0.0609,\n",
      "         0.0874, -0.0199, -0.0271, -0.0243,  0.0010,  0.0287,  0.1083,  0.0132,\n",
      "        -0.0009,  0.0486,  0.0744,  0.0794, -0.0256, -0.0949,  0.0185,  0.0177,\n",
      "        -0.0237, -0.0034,  0.0499,  0.0010, -0.0432,  0.0147, -0.0061,  0.1122,\n",
      "         0.0223,  0.1098,  0.0567,  0.1595,  0.1422,  0.0010,  0.0363,  0.0531,\n",
      "        -0.0168,  0.0290,  0.0170,  0.0418, -0.0140, -0.0437,  0.0741,  0.0371,\n",
      "        -0.0249,  0.0049,  0.0037, -0.0140, -0.0101,  0.0235,  0.0189,  0.0498,\n",
      "        -0.0249, -0.0783,  0.0025,  0.0435, -0.0671, -0.0562, -0.0115, -0.0060])\n"
     ]
    }
   ],
   "execution_count": 287
  },
  {
   "cell_type": "markdown",
   "id": "18593e383e1153c3",
   "metadata": {},
   "source": "## Parameter Extraction"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:58.440981Z",
     "start_time": "2024-05-04T16:33:58.307888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    'scale',\n",
    "    'zero_point',\n",
    "    '_packed_params.dtype'\n",
    "]\n",
    "\n",
    "with open('model_layers_params.txt', 'w') as f:\n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if not any(ignore_key in layer_name for ignore_key in ignore_keys):\n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                # Dynamically get the corresponding layer\n",
    "                layer_parts = layer_name.split('.')\n",
    "                layer = model_quantized\n",
    "                for part in layer_parts[:-1]:\n",
    "                    layer = getattr(layer, part)\n",
    "                \n",
    "                packed_params = getattr(layer, '_packed_params')\n",
    "                \n",
    "                # Unpack the quantized weights and biases\n",
    "                int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "                int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "                int8_bias_nd = int8_bias.detach().numpy()\n",
    "                \n",
    "                f.write(f\"Layer: {layer_name}\\n\")\n",
    "                f.write(f\"Quantized Weights:\\n{int8_weights_nd}\\n\")\n",
    "                f.write(f\"Quantization Scale: {int8_weights.q_scale()}\\n\")\n",
    "                f.write(f\"Quantized Bias:\\n{int8_bias_nd}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                # Convert param_tensor to ndarray\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                \n",
    "                f.write(f\"Layer: {layer_name}\\n\") \n",
    "                f.write(f\"Parameters:\\n{param_ndarray}\\n\")\n",
    "                f.write(\"\\n\")\n"
   ],
   "id": "dcb42f8ab80da6cf",
   "outputs": [],
   "execution_count": 288
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "測試提取參數是否正確",
   "id": "8daa3a13d9d6d95b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:33:58.470614Z",
     "start_time": "2024-05-04T16:33:58.441991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight_tensor_after = model_quantized.state_dict()['encoder.2.1.block.1.2._packed_params._packed_params']\n",
    "\n",
    "weight_tensor_after[0]"
   ],
   "id": "dfbfb7f21aac1f7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0196,  0.0141,  0.0685,  ...,  0.0087,  0.0435,  0.0120],\n",
       "        [-0.0457, -0.0598,  0.0130,  ...,  0.0272,  0.0478, -0.0402],\n",
       "        [ 0.0152, -0.0098, -0.0380,  ...,  0.0261,  0.0239,  0.0022],\n",
       "        ...,\n",
       "        [ 0.0174,  0.0152, -0.0076,  ..., -0.0033, -0.0544,  0.0163],\n",
       "        [ 0.0163,  0.0380,  0.0315,  ..., -0.0272, -0.0370, -0.0370],\n",
       "        [ 0.0239,  0.0043, -0.0109,  ...,  0.0098,  0.0228, -0.0359]],\n",
       "       size=(192, 768), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.001087119453586638,\n",
       "       zero_point=0)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 289
  },
  {
   "cell_type": "markdown",
   "id": "1b8d3a51dac9641d",
   "metadata": {},
   "source": "### 提取六層 Encoder, Classifier, Embedding 參數"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 32-bit 浮點數",
   "id": "20e7df3fed31e5a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:34:01.000705Z",
     "start_time": "2024-05-04T16:33:58.472725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    '.scale',\n",
    "    '.zero_point',\n",
    "    '._packed_params.dtype'\n",
    "]\n",
    "\n",
    "def extract_packed_params(layer_name, param_tensor, folder_path):\n",
    "    if '_packed_params._packed_params' in layer_name:\n",
    "        layer_parts = layer_name.split('.')\n",
    "        layer = model_quantized\n",
    "        for part in layer_parts[:-1]:\n",
    "            layer = getattr(layer, part)\n",
    "        \n",
    "        packed_params = getattr(layer, '_packed_params')\n",
    "        \n",
    "        int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "        int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "        int8_bias_nd = int8_bias.detach().numpy()\n",
    "        \n",
    "        # Save quantized weights\n",
    "        weights_file = os.path.join(folder_path, 'quantized_weights.txt')\n",
    "        with open(weights_file, 'w') as f:\n",
    "            for weight in int8_weights_nd.flatten():\n",
    "                f.write(f\"{weight}\\n\")\n",
    "        \n",
    "        # Save quantized biases\n",
    "        bias_file = os.path.join(folder_path, 'quantized_bias.txt')\n",
    "        with open(bias_file, 'w') as f:\n",
    "            np.savetxt(f, int8_bias_nd, fmt='%.8f')\n",
    "        \n",
    "        # Save quantization scale\n",
    "        scale_file = os.path.join(folder_path, 'quantization_scale.txt')\n",
    "        with open(scale_file, 'w') as f:\n",
    "            f.write(str(int8_weights.q_scale()))\n",
    "    else:\n",
    "        param_ndarray = param_tensor.detach().numpy()\n",
    "        \n",
    "        # Save regular parameters\n",
    "        param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "        with open(param_file, 'w') as f:\n",
    "            np.savetxt(f, param_ndarray.flatten(), fmt='%.8f')\n",
    "\n",
    "# Create the \"32float\" folder in the current directory\n",
    "float32_folder = '32float'\n",
    "os.makedirs(float32_folder, exist_ok=True)\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(6):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}') and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                extract_packed_params(layer_name, param_tensor, layer_folder)\n",
    "            else:\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                param_file = os.path.join(layer_folder, 'parameters.txt')\n",
    "                with open(param_file, 'w') as f:\n",
    "                    np.savetxt(f, param_ndarray.flatten(), fmt='%.8f')\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer) and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_packed_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ee1229d64b4c3fa1",
   "outputs": [],
   "execution_count": 290
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16-bit IEEE 754 binary format\n",
    "\n",
    "- 整數轉成8位元的二進制\n",
    "- 浮點數轉成16位元的IEEE 754二進制"
   ],
   "id": "d72a43aa9b0c05c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:34:08.803833Z",
     "start_time": "2024-05-04T16:34:01.001732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import struct\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    '.scale',\n",
    "    '.zero_point',\n",
    "    '._packed_params.dtype'\n",
    "]\n",
    "\n",
    "def float_to_bin_16bit(num):\n",
    "    # Convert float to 16-bit IEEE 754 binary format\n",
    "    binary = struct.pack('>e', num)\n",
    "    return ''.join('{:08b}'.format(b) for b in binary)\n",
    "\n",
    "def int_to_bin_8bit(num):\n",
    "    # Convert integer to 8-bit signed binary format\n",
    "    return '{:08b}'.format(num & 0xff)\n",
    "\n",
    "def extract_packed_params(layer_name, param_tensor, folder_path):\n",
    "    if '_packed_params._packed_params' in layer_name:\n",
    "        layer_parts = layer_name.split('.')\n",
    "        layer = model_quantized\n",
    "        for part in layer_parts[:-1]:\n",
    "            layer = getattr(layer, part)\n",
    "        \n",
    "        packed_params = getattr(layer, '_packed_params')\n",
    "        \n",
    "        int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "        int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "        int8_bias_nd = int8_bias.detach().numpy()\n",
    "        \n",
    "        # Save quantized weights as binary in text format\n",
    "        weights_file = os.path.join(folder_path, 'quantized_weights.txt')\n",
    "        with open(weights_file, 'w') as f:\n",
    "            for weight in int8_weights_nd.flatten():\n",
    "                f.write(int_to_bin_8bit(weight) + '\\n')\n",
    "        \n",
    "        # Save quantized biases as binary in text format\n",
    "        bias_file = os.path.join(folder_path, 'quantized_bias.txt')\n",
    "        with open(bias_file, 'w') as f:\n",
    "            for bias in int8_bias_nd.flatten():\n",
    "                f.write(float_to_bin_16bit(bias) + '\\n')\n",
    "        \n",
    "        # Save quantization scale as binary in text format\n",
    "        scale_file = os.path.join(folder_path, 'quantization_scale.txt')\n",
    "        with open(scale_file, 'w') as f:\n",
    "            f.write(float_to_bin_16bit(int8_weights.q_scale()))\n",
    "    else:\n",
    "        param_ndarray = param_tensor.detach().numpy()\n",
    "        \n",
    "        # Save regular parameters as binary in text format\n",
    "        param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "        with open(param_file, 'w') as f:\n",
    "            for param in param_ndarray.flatten():\n",
    "                if isinstance(param, np.integer):\n",
    "                    f.write(int_to_bin_8bit(param) + '\\n')\n",
    "                else:\n",
    "                    f.write(float_to_bin_16bit(param) + '\\n')\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(6):\n",
    "    folder_name = f'binary/encoder_{i}_params'\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}') and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_name, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                extract_packed_params(layer_name, param_tensor, layer_folder)\n",
    "            else:\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                param_file = os.path.join(layer_folder, 'parameters.txt')\n",
    "                with open(param_file, 'w') as f:\n",
    "                    for param in param_ndarray.flatten():\n",
    "                        if isinstance(param, np.integer):\n",
    "                            f.write(int_to_bin_8bit(param) + '\\n')\n",
    "                        else:\n",
    "                            f.write(float_to_bin_16bit(param) + '\\n')\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'binary/{layer}_params'\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer) and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_name, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_packed_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ad9f3ef3fd02811a",
   "outputs": [],
   "execution_count": 291
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
