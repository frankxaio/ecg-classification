{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "9f24395e3322cf24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-05-10T16:57:40.600070Z",
     "start_time": "2024-05-10T16:57:36.080221Z"
    }
   },
   "source": [
    "import itertools # 是 Python 的內建模組，提供了一組用於處理迭代器的函數和工具。\n",
    "                 # 它包含了各種用於高效處理迭代器的函數，可以幫助我們編寫更簡潔、高效的代碼。\n",
    "import sys # 是 Python 的內建模組，提供了與 Python 解釋器和運行環境相關的功能。\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# sys.path 是一個列表，包含了 Python 解釋器在導入模組時會搜尋的路徑。\n",
    "# 當你使用 import 語句導入模組時 Python 會依次在 sys.path 中的路徑下尋找對應的模組文件。\n",
    "sys.path.append(\"../ecg-classification/\")\n",
    "# sys.path.append(\"C:\\\\Users\\\\Chen_Lab01\\\\Documents\\\\GitHub/ecg-classification\")\n",
    "# from IPython.display import Video\n",
    "# import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(\"ggplot\") #  是 Matplotlib 庫中用於設置繪圖樣式的函數。它使用了一種名為 \"ggplot\" 的預定義樣式\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "                        #  該樣式模仿了 R 語言的 ggplot2 繪圖包的外觀。\n",
    "# print(sys.path)\n",
    "import torch\n",
    "from ecg_tools.config import EcgConfig, Mode\n",
    "from ecg_tools.data_loader import DatasetConfig, get_data_loaders\n",
    "from ecg_tools.model import ECGformer\n",
    "from ecg_tools.train import ECGClassifierTrainer\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "adbb91b9dbfa94e3",
   "metadata": {},
   "source": "## Load model"
  },
  {
   "cell_type": "code",
   "id": "29bb12472624e7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:59:37.405577Z",
     "start_time": "2024-05-10T16:59:37.373970Z"
    }
   },
   "source": [
    "import torch\n",
    "config = EcgConfig()    \n",
    "model_quantized = torch.load(\"..\\\\..\\\\model_save\\\\model_quantized_148.pth\")\n",
    "model = torch.load(\"..\\\\..\\\\model_save\\\\model_epoch_148.pth\")\n",
    "model_quantized.eval()\n",
    "model_quantized.to('cpu')\n",
    "model.eval()\n",
    "model.to('cpu')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xaio\\anaconda3\\envs\\pytorch-ecg\\lib\\site-packages\\torch\\_utils.py:382: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ECGformer(\n",
       "  (encoder): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (0): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): MultiHeadAttention(\n",
       "            (queries_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (values_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (keys_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (final_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): MLP(\n",
       "            (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (0): Reduce('b n e -> b e', 'mean')\n",
       "    (1): Linear(in_features=16, out_features=6, bias=True)\n",
       "  )\n",
       "  (embedding): LinearEmbedding(\n",
       "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "fdc775d31a626cf5",
   "metadata": {},
   "source": "## 量化模型"
  },
  {
   "cell_type": "code",
   "id": "fe171c96df432930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:59:08.264585Z",
     "start_time": "2024-05-10T16:59:08.232538Z"
    }
   },
   "source": [
    "# import torch.quantization\n",
    "# \n",
    "# # 使用 Eager Mode Quantization\n",
    "# # 將 torch.nn.Linear 的參數映射到 -127~127 之間\n",
    "# \n",
    "# model_quantized = torch.quantization.quantize_dynamic(\n",
    "#     model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "# )\n",
    "# \n",
    "# torch.save(model_quantized, \"..\\\\..\\\\model_save\\\\model_quantized_148.pth\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 準確度測試",
   "id": "a041d1c91993f194"
  },
  {
   "cell_type": "code",
   "id": "780d3550eb5bc030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T17:00:22.110416Z",
     "start_time": "2024-05-10T17:00:21.970744Z"
    }
   },
   "source": [
    "import einops\n",
    "loader = get_data_loaders(DatasetConfig())\n",
    "accuracy = 0\n",
    "for signal, label in loader[Mode.eval]:\n",
    "    signal.to('cpu')\n",
    "    label.to('cpu')\n",
    "    signal = einops.rearrange(signal, \"b c e -> b e c\")\n",
    "    # print(signal)\n",
    "    p = model_quantized(signal)\n",
    "    # print(p)\n",
    "    print(label)\n",
    "    # print(signal.shape, label.shape)\n",
    "    # print(p.argmax(1) == label)\n",
    "    accuracy += torch.sum(p.argmax(1) == label)\n",
    "    print(f\"accuracy: {accuracy / config.dataset.batch_size}\")\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 0, 5, 0, 0,\n",
      "        2, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 5, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4])\n",
      "accuracy: 0.890625\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## View Prameter",
   "id": "a8b7f2cf319ea7bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model layer",
   "id": "a64d994b3dd451d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T17:00:25.727849Z",
     "start_time": "2024-05-10T17:00:25.712337Z"
    }
   },
   "cell_type": "code",
   "source": "print(model_quantized)",
   "id": "c5ba9693145ab0eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGformer(\n",
      "  (encoder): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (0): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): MultiHeadAttention(\n",
      "            (queries_projection): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (values_projection): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (keys_projection): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (final_projection): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): MLP(\n",
      "            (0): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (1): ReLU()\n",
      "            (2): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (0): Reduce('b n e -> b e', 'mean')\n",
      "    (1): DynamicQuantizedLinear(in_features=16, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (embedding): LinearEmbedding(\n",
      "    (0): DynamicQuantizedLinear(in_features=1, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "字典形式的量化模型參數",
   "id": "a724a3db99f7bd99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 量化\n",
    "for param_name, param_tensor in model_quantized.state_dict().items():\n",
    "    print(f\"{param_name}\")"
   ],
   "id": "f3513f473313dc22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "字典形式的非量化模型參數",
   "id": "d308ce0f9be7ec25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 未量化\n",
    "for param_name, param_tensor in model.state_dict().items():\n",
    "    print(f\"{param_name}\")"
   ],
   "id": "f4dd095dd96a7ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packed_params",
   "id": "4ab7fe9f2a642014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T17:00:37.419444Z",
     "start_time": "2024-05-10T17:00:37.387886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "weight_tensor_after = model_quantized.state_dict()['encoder.0.0.block.0.final_projection._packed_params._packed_params']\n",
    "\n",
    "# packed_params = model_quantized.encoder[0][0].block[0].queries_projection._packed_params._packed_params\n",
    "\n",
    "# Unpack the quantized weights and biases\n",
    "int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "int8_bias_nd = int8_bias.detach().numpy()\n",
    "# # Dequantize the weights and biases\n",
    "# weights = int8_weights.dequantize()\n",
    "# bias = int8_bias.dequantize()\n",
    "\n",
    "print(int8_weights_nd)#　將量化後的權重轉換為整數表示並轉化為numpy# array\n",
    "print(int8_weights.q_scale()) #　獲取量化的scale\n",
    "print(int8_bias_nd)"
   ],
   "id": "d95afa5238bac0f7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'packed_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 7\u001B[0m\n\u001B[0;32m      2\u001B[0m weight_tensor_after \u001B[38;5;241m=\u001B[39m model_quantized\u001B[38;5;241m.\u001B[39mstate_dict()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoder.0.0.block.0.final_projection._packed_params._packed_params\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# packed_params = model_quantized.encoder[0][0].block[0].queries_projection._packed_params._packed_params\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Unpack the quantized weights and biases\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m int8_weights, int8_bias \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mquantized\u001B[38;5;241m.\u001B[39mlinear_unpack(\u001B[43mpacked_params\u001B[49m)\n\u001B[0;32m      8\u001B[0m int8_weights_nd \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(int8_weights\u001B[38;5;241m.\u001B[39mint_repr())\n\u001B[0;32m      9\u001B[0m int8_bias_nd \u001B[38;5;241m=\u001B[39m int8_bias\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'packed_params' is not defined"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for param_name, param_tensor in model_quantized.state_dict().items():\n",
    "    if isinstance(param_tensor, torch.Tensor):\n",
    "        if not any(special_param in param_name for special_param in ['scale', 'zero_point', 'dtype', '_packed_params']):\n",
    "            print(f\"{param_name}\\t{param_tensor.size()}\")"
   ],
   "id": "93d323b07c1341c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "印出模型中所有的weights和bias",
   "id": "9ec3ae45807205c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, param in model_quantized.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Weight: {param.data}\")\n",
    "    elif 'bias' in name:\n",
    "        print(f\"Layer: {name}\") \n",
    "        print(f\"Bias: {param.data}\")\n"
   ],
   "id": "42b88a0cb1bf2fa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for var_name in model_quantized.state_dict():\n",
    "    print(var_name, \"\\t\", model_quantized.state_dict()[var_name])"
   ],
   "id": "306fd264326966ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18593e383e1153c3",
   "metadata": {},
   "source": "## Parameter Extraction"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    'scale',\n",
    "    'zero_point',\n",
    "    '_packed_params.dtype'\n",
    "]\n",
    "\n",
    "with open('model_layers_params.txt', 'w') as f:\n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if not any(ignore_key in layer_name for ignore_key in ignore_keys):\n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                # Dynamically get the corresponding layer\n",
    "                layer_parts = layer_name.split('.')\n",
    "                layer = model_quantized\n",
    "                for part in layer_parts[:-1]:\n",
    "                    layer = getattr(layer, part)\n",
    "                \n",
    "                packed_params = getattr(layer, '_packed_params')\n",
    "                \n",
    "                # Unpack the quantized weights and biases\n",
    "                int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "                int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "                int8_bias_nd = int8_bias.detach().numpy()\n",
    "                \n",
    "                f.write(f\"Layer: {layer_name}\\n\")\n",
    "                f.write(f\"Quantized Weights:\\n{int8_weights_nd}\\n\")\n",
    "                f.write(f\"Quantization Scale: {int8_weights.q_scale()}\\n\")\n",
    "                f.write(f\"Quantized Bias:\\n{int8_bias_nd}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                # Convert param_tensor to ndarray\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                \n",
    "                f.write(f\"Layer: {layer_name}\\n\") \n",
    "                f.write(f\"Parameters:\\n{param_ndarray}\\n\")\n",
    "                f.write(\"\\n\")\n"
   ],
   "id": "dcb42f8ab80da6cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b8d3a51dac9641d",
   "metadata": {},
   "source": "### 提取六層 Encoder, Classifier, Embedding 參數"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 32-bit 浮點數(Unquantized model)",
   "id": "fe292a94fbec233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "def extract_params(layer_name, param_tensor, folder_path):\n",
    "    param_ndarray = param_tensor.detach().numpy()\n",
    "    \n",
    "    # Save parameters as 32-bit float representations\n",
    "    param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "    with open(param_file, 'w') as f:\n",
    "        if len(param_ndarray.shape) == 2:\n",
    "            for col in param_ndarray.T:\n",
    "                for value in col:\n",
    "                    f.write(f\"{value:.8e}\\n\")\n",
    "        else:\n",
    "            for value in param_ndarray.flatten():\n",
    "                f.write(f\"{value:.8e}\\n\")\n",
    "\n",
    "# Create the \"32bit_float\" folder in the current directory\n",
    "float_folder = '32bit_float'\n",
    "os.makedirs(float_folder, exist_ok=True)\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(1):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(float_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}'):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(float_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ce7d0fd7fa7900a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 16-bit IEEE 754 binary format(Unquantized model)",
   "id": "179952e3621e781b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import struct\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "def float_to_binary16(value):\n",
    "    # Convert float to IEEE 754 binary16 format\n",
    "    binary16 = struct.pack('>e', value)\n",
    "    return ''.join(f'{b:08b}' for b in binary16)\n",
    "\n",
    "def extract_params(layer_name, param_tensor, folder_path):\n",
    "    param_ndarray = param_tensor.detach().numpy()\n",
    "    \n",
    "    # Save parameters as binary16 representations\n",
    "    param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "    with open(param_file, 'w') as f:\n",
    "        if len(param_ndarray.shape) == 2:\n",
    "            for col in param_ndarray.T:\n",
    "                for value in col:\n",
    "                    binary16 = float_to_binary16(value)\n",
    "                    f.write(binary16 + '\\n')\n",
    "        else:\n",
    "            for value in param_ndarray.flatten():\n",
    "                binary16 = float_to_binary16(value)\n",
    "                f.write(binary16 + '\\n')\n",
    "\n",
    "# Create the \"16bit_binary\" folder in the current directory\n",
    "binary_folder = '16bit_binary'\n",
    "os.makedirs(binary_folder, exist_ok=True)\n",
    "\n",
    "# Extract encoder.0 to encoder.5\n",
    "for i in range(1):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(binary_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}'):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n",
    "\n",
    "# Extract remaining layers\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(binary_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "bbc0ca433c25d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 32-bit 浮點數(Quantized model)",
   "id": "20e7df3fed31e5a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T17:00:52.935005Z",
     "start_time": "2024-05-10T17:00:52.887648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    '.scale',\n",
    "    '.zero_point',\n",
    "    '._packed_params.dtype'\n",
    "]\n",
    "\n",
    "def extract_packed_params(layer_name, param_tensor, folder_path):\n",
    "    if '_packed_params._packed_params' in layer_name:\n",
    "        layer_parts = layer_name.split('.')\n",
    "        layer = model_quantized\n",
    "        for part in layer_parts[:-1]:\n",
    "            layer = getattr(layer, part)\n",
    "        \n",
    "        packed_params = getattr(layer, '_packed_params')\n",
    "        \n",
    "        int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "        int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "        int8_bias_nd = int8_bias.detach().numpy()\n",
    "        \n",
    "        weights_file = os.path.join(folder_path, 'quantized_weights.txt')\n",
    "        with open(weights_file, 'w') as f:\n",
    "            for weight in int8_weights_nd.flatten():\n",
    "                f.write(f\"{weight:>4}\\n\")\n",
    "        \n",
    "        bias_file = os.path.join(folder_path, 'quantized_bias.txt')\n",
    "        with open(bias_file, 'w') as f:\n",
    "            for bias in int8_bias_nd.flatten():  \n",
    "                f.write(f\"{bias:>10.8f}\\n\")\n",
    "        \n",
    "        scale_file = os.path.join(folder_path, 'quantization_scale.txt')\n",
    "        with open(scale_file, 'w') as f:\n",
    "            f.write(f\"{int8_weights.q_scale():>10.8f}\")\n",
    "        \n",
    "        zero_point_file = os.path.join(folder_path, 'quantization_zero_point.txt')\n",
    "        with open(zero_point_file, 'w') as f:  \n",
    "            f.write(f\"{int8_weights.q_zero_point():>4}\")\n",
    "    else:\n",
    "        param_ndarray = param_tensor.detach().numpy()\n",
    "        \n",
    "        param_file = os.path.join(folder_path, 'parameters.txt') \n",
    "        with open(param_file, 'w') as f:\n",
    "            for param in param_ndarray.flatten():\n",
    "                f.write(f\"{param:>10.8f}\\n\")\n",
    "\n",
    "float32_folder = '32float'\n",
    "os.makedirs(float32_folder, exist_ok=True)\n",
    "\n",
    "for i in range(1):\n",
    "    folder_name = f'encoder_{i}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name) \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}') and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                extract_packed_params(layer_name, param_tensor, layer_folder)\n",
    "            else:\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                param_file = os.path.join(layer_folder, 'parameters.txt')\n",
    "                with open(param_file, 'w') as f:\n",
    "                    for param in param_ndarray.flatten():\n",
    "                        f.write(f\"{param:>10.8f}\\n\")  \n",
    "\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding']\n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'{layer}_params'\n",
    "    folder_path = os.path.join(float32_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():  \n",
    "        if layer_name.startswith(layer) and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_path, layer_name.replace('.', '_')) \n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_packed_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ee1229d64b4c3fa1",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16-bit IEEE 754 binary format(Quantized model)\n",
    "\n",
    "- 整數轉成8位元的二進制\n",
    "- 浮點數轉成16位元的IEEE 754二進制"
   ],
   "id": "d72a43aa9b0c05c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T17:00:55.400241Z",
     "start_time": "2024-05-10T17:00:55.353421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import struct\n",
    "\n",
    "model_state_dict = model_quantized.state_dict()\n",
    "\n",
    "ignore_keys = [\n",
    "    '.scale',\n",
    "    '.zero_point',\n",
    "    '._packed_params.dtype'\n",
    "]\n",
    "\n",
    "def float_to_bin_16bit(num):\n",
    "    binary = struct.pack('>e', num)\n",
    "    return ''.join('{:08b}'.format(b) for b in binary)\n",
    "\n",
    "def int_to_bin_8bit(num):\n",
    "    return '{:08b}'.format(num & 0xff)\n",
    "\n",
    "def extract_packed_params(layer_name, param_tensor, folder_path):\n",
    "    if '_packed_params._packed_params' in layer_name:\n",
    "        layer_parts = layer_name.split('.')\n",
    "        layer = model_quantized\n",
    "        for part in layer_parts[:-1]:\n",
    "            layer = getattr(layer, part)\n",
    "        \n",
    "        packed_params = getattr(layer, '_packed_params')\n",
    "        \n",
    "        int8_weights, int8_bias = torch.ops.quantized.linear_unpack(packed_params)\n",
    "        int8_weights_nd = np.array(int8_weights.int_repr())\n",
    "        int8_bias_nd = int8_bias.detach().numpy()\n",
    "        \n",
    "        weights_file = os.path.join(folder_path, 'quantized_weights.txt')\n",
    "        with open(weights_file, 'w') as f:\n",
    "            for weight in int8_weights_nd.flatten():\n",
    "                f.write(f\"{int_to_bin_8bit(weight):>8}\\n\")\n",
    "        \n",
    "        bias_file = os.path.join(folder_path, 'quantized_bias.txt') \n",
    "        with open(bias_file, 'w') as f:\n",
    "            for bias in int8_bias_nd.flatten():\n",
    "                f.write(f\"{float_to_bin_16bit(bias):>16}\\n\")\n",
    "        \n",
    "        scale_file = os.path.join(folder_path, 'quantization_scale.txt')\n",
    "        with open(scale_file, 'w') as f:\n",
    "            f.write(f\"{float_to_bin_16bit(int8_weights.q_scale()):>16}\")\n",
    "    else:\n",
    "        param_ndarray = param_tensor.detach().numpy()\n",
    "        \n",
    "        param_file = os.path.join(folder_path, 'parameters.txt')\n",
    "        with open(param_file, 'w') as f:\n",
    "            for param in param_ndarray.flatten():\n",
    "                if isinstance(param, np.integer):\n",
    "                    f.write(f\"{int_to_bin_8bit(param):>8}\\n\") \n",
    "                else:\n",
    "                    f.write(f\"{float_to_bin_16bit(param):>16}\\n\")\n",
    "\n",
    "for i in range(1):\n",
    "    folder_name = f'binary/encoder_{i}_params'\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(f'encoder.{i}') and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_name, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            if '_packed_params._packed_params' in layer_name:\n",
    "                extract_packed_params(layer_name, param_tensor, layer_folder)\n",
    "            else:\n",
    "                param_ndarray = param_tensor.detach().numpy()\n",
    "                param_file = os.path.join(layer_folder, 'parameters.txt')\n",
    "                with open(param_file, 'w') as f:\n",
    "                    for param in param_ndarray.flatten():\n",
    "                        if isinstance(param, np.integer):\n",
    "                            f.write(f\"{int_to_bin_8bit(param):>8}\\n\")\n",
    "                        else:  \n",
    "                            f.write(f\"{float_to_bin_16bit(param):>16}\\n\")\n",
    "\n",
    "layers_to_extract = ['classifier', 'embedding', 'positional_encoding'] \n",
    "for layer in layers_to_extract:\n",
    "    folder_name = f'binary/{layer}_params'\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for layer_name, param_tensor in model_state_dict.items():\n",
    "        if layer_name.startswith(layer) and not any(layer_name.endswith(ignore_key) for ignore_key in ignore_keys):\n",
    "            layer_folder = os.path.join(folder_name, layer_name.replace('.', '_'))\n",
    "            os.makedirs(layer_folder, exist_ok=True)\n",
    "            \n",
    "            extract_packed_params(layer_name, param_tensor, layer_folder)\n"
   ],
   "id": "ad9f3ef3fd02811a",
   "outputs": [],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
