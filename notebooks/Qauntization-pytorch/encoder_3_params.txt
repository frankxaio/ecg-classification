Layer: encoder.3.0.block.0.weight
Parameters: tensor([1.0814, 1.0765, 1.0622, 1.0329, 1.1102, 1.0746, 1.0925, 1.0411, 1.1040,
        1.0981, 1.0950, 1.0356, 1.0815, 1.1120, 1.0693, 1.0973, 1.1254, 1.0787,
        1.0592, 1.0606, 1.0992, 1.0615, 0.9902, 1.0858, 1.0415, 1.1419, 1.0079,
        1.0934, 1.1100, 1.0668, 1.0658, 1.1220, 1.0822, 1.0712, 1.0245, 1.0677,
        1.0573, 1.0459, 1.0717, 1.0428, 1.0451, 1.0981, 1.0555, 1.0663, 1.0625,
        1.1187, 1.1033, 1.0720, 1.0314, 1.0786, 1.0697, 1.0485, 1.0711, 1.0635,
        1.0471, 1.0872, 1.0857, 1.0699, 1.0979, 1.0730, 1.0468, 1.0317, 1.0492,
        1.0516, 1.0897, 1.0804, 1.0888, 1.0284, 1.0744, 1.1144, 1.0221, 1.0429,
        1.0943, 1.0632, 1.0699, 1.0654, 1.0951, 0.9977, 1.0544, 1.0982, 1.0271,
        1.1078, 1.0240, 1.0728, 1.0496, 1.0858, 1.1356, 1.0749, 1.0588, 1.0832,
        1.0828, 1.0805, 1.1403, 1.0709, 1.1037, 1.0706, 1.0642, 1.0828, 1.0812,
        1.0569, 1.0600, 1.0737, 1.0908, 1.0724, 1.0601, 1.0410, 1.1175, 1.0310,
        1.0576, 1.0817, 1.0750, 1.0618, 1.0438, 1.0588, 1.0755, 1.0793, 1.0518,
        1.0599, 1.0509, 1.0689, 1.0683, 1.0697, 1.1012, 1.0485, 1.1046, 1.0697,
        1.0422, 1.1182, 1.0877, 1.0309, 1.0545, 1.0816, 1.1037, 1.0711, 1.1021,
        1.1056, 1.0422, 1.0668, 1.0743, 1.0884, 1.0687, 1.1000, 1.0329, 1.0914,
        1.0499, 1.0940, 1.1198, 1.0762, 1.0780, 1.0616, 1.0400, 1.0842, 1.0716,
        1.0918, 1.0975, 1.0989, 1.0515, 1.0576, 1.0828, 1.0621, 1.0996, 1.0448,
        1.0778, 1.0919, 1.0676, 1.0589, 1.0729, 1.0909, 1.0650, 1.0993, 1.0963,
        1.0632, 1.0514, 1.0683, 1.0946, 1.1234, 1.0735, 1.1094, 1.0514, 1.0855,
        1.0378, 1.0669, 1.0823, 1.0403, 1.0336, 1.0844, 1.0274, 1.0631, 1.0846,
        1.0818, 1.1047, 1.0734])

Layer: encoder.3.0.block.0.bias
Parameters: tensor([-1.0134e-02, -2.5528e-02, -4.4422e-05,  1.1421e-02, -4.3505e-03,
         8.3018e-03,  1.1776e-02, -2.4206e-02,  1.1184e-02, -4.4472e-03,
        -7.8210e-03, -6.7566e-03,  2.8519e-03, -6.2728e-03, -2.5072e-02,
         1.0841e-02, -1.6337e-02, -6.2578e-03,  9.7308e-04,  2.8110e-04,
         5.3742e-02, -1.5228e-02,  3.3288e-02, -1.7523e-02,  1.3413e-02,
         3.2621e-03,  1.9949e-02,  1.8062e-02, -2.8281e-03, -1.8056e-03,
         1.5385e-03, -9.0413e-03, -3.6256e-02, -1.2111e-02,  7.1971e-03,
         5.1344e-03,  1.4347e-02,  1.1417e-03,  1.1738e-02,  2.6959e-02,
         2.9889e-04,  1.0288e-02,  1.6434e-02, -7.6264e-03, -2.3411e-02,
         2.1314e-02,  2.0690e-03, -1.1223e-02, -1.8921e-02,  1.5514e-02,
         1.5659e-02,  2.0060e-02, -1.2657e-02,  8.0964e-03,  1.5373e-02,
         1.3688e-02,  1.7458e-03,  5.0623e-04, -1.4722e-02,  1.8194e-02,
         1.0136e-02,  2.2101e-02,  9.5553e-05, -8.6899e-04,  5.1772e-03,
         2.9227e-02, -9.9683e-03, -1.9101e-02, -2.7740e-03,  4.4398e-03,
        -1.5628e-04, -8.2828e-03,  3.4074e-03,  1.1169e-02,  1.3783e-02,
         5.8001e-03, -1.9167e-02, -3.6250e-02,  1.7682e-02, -6.9122e-03,
         1.6996e-03,  1.4231e-02, -3.5898e-02,  9.2764e-04,  1.4663e-03,
         1.3185e-02, -7.5674e-03,  1.1094e-02,  1.4695e-04, -7.0876e-03,
         2.0398e-03, -4.5756e-03,  8.2286e-03,  4.5048e-03,  2.4132e-02,
        -2.0049e-02, -3.9077e-03,  1.3582e-02,  1.2941e-02,  4.3835e-03,
        -1.1672e-03,  3.7391e-04,  1.6218e-02,  1.2284e-02, -7.6780e-03,
         4.5622e-03, -2.1223e-03, -1.3082e-02, -1.4154e-02,  1.3219e-03,
        -3.0446e-03,  9.1153e-03,  8.7442e-03,  1.2832e-02,  1.9866e-02,
        -2.8401e-03, -1.3944e-02, -6.4145e-03,  2.6456e-02, -1.0234e-02,
         6.4360e-04, -9.5210e-03, -1.2136e-02, -1.4196e-02, -6.3746e-03,
         9.8575e-03, -1.7399e-02,  7.2580e-03,  6.3795e-04, -1.0779e-02,
        -2.5968e-02, -1.7997e-02, -1.4348e-02,  1.5219e-02,  1.5313e-03,
         1.1217e-02, -9.1681e-03,  1.2280e-03,  1.5434e-03,  1.7528e-02,
        -7.6899e-03,  2.9323e-03,  5.3500e-03,  1.0023e-02,  4.5727e-03,
        -1.4378e-03, -6.9523e-03,  1.1435e-02,  1.8095e-02, -1.3672e-02,
        -7.9346e-03,  2.5498e-03,  6.5080e-04,  6.5759e-05, -4.0068e-03,
        -5.7062e-03, -2.0910e-02, -1.2364e-03,  7.8252e-04, -1.1298e-02,
        -1.5854e-02,  2.6461e-02,  1.6140e-02,  1.7532e-02,  4.5712e-03,
        -5.9016e-03, -1.5556e-02, -7.0299e-03,  1.3481e-02, -4.3510e-02,
         1.4426e-02,  1.4176e-02,  2.5755e-02,  2.5410e-02,  4.2517e-03,
        -1.6566e-02, -9.7021e-03,  1.8497e-02,  7.8582e-03,  2.0400e-02,
        -7.4422e-03, -2.8774e-02, -3.1756e-02, -3.3795e-02,  1.2743e-02,
         1.3058e-02,  3.4846e-02,  1.6342e-02,  1.8074e-02,  7.4222e-03,
        -1.2218e-03, -6.3475e-03])

Layer: encoder.3.0.block.1.queries_projection._packed_params._packed_params
Parameters: (tensor([[-0.0093,  0.0371,  0.0000,  ..., -0.0056,  0.0056, -0.0019],
        [ 0.0871, -0.0093,  0.0185,  ..., -0.0297, -0.0222,  0.0853],
        [ 0.0519,  0.0593, -0.0260,  ...,  0.0742, -0.0797,  0.0667],
        ...,
        [ 0.0371,  0.0482, -0.0260,  ..., -0.0983, -0.0074, -0.0853],
        [ 0.0538, -0.0705, -0.0111,  ..., -0.0742, -0.0074, -0.0946],
        [ 0.0667,  0.0556, -0.0241,  ...,  0.0612, -0.0111, -0.0111]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0018541408935561776,
       zero_point=0), Parameter containing:
tensor([-0.0359,  0.0520,  0.0130,  0.0192, -0.0129, -0.0217,  0.0028, -0.0354,
        -0.0802, -0.0535,  0.0499,  0.0394,  0.0455,  0.0489, -0.0126,  0.0351,
         0.0091,  0.0399,  0.0232,  0.0172,  0.0468, -0.0543, -0.1107,  0.0295,
        -0.0098,  0.0670, -0.0362, -0.0751,  0.0047, -0.0123, -0.0358, -0.0097,
        -0.0455, -0.0307, -0.0469,  0.0635, -0.0362, -0.0335,  0.0391, -0.0297,
        -0.0131, -0.0334, -0.0361, -0.0664,  0.0946, -0.0252,  0.0386, -0.0518,
         0.0619,  0.0290, -0.0554,  0.0513,  0.0332, -0.0322,  0.0507, -0.0346,
        -0.0090,  0.0039,  0.0088,  0.0144, -0.0029,  0.0094, -0.0505,  0.0166,
        -0.0147, -0.0186, -0.0159,  0.0748, -0.0377, -0.0708,  0.0619, -0.0660,
        -0.0425,  0.0549, -0.0499, -0.0389,  0.0112, -0.0459, -0.0415, -0.0770,
         0.0194,  0.0640, -0.0082, -0.0291,  0.0151, -0.0286,  0.0541, -0.0418,
         0.0872, -0.0305, -0.0008, -0.0407, -0.0527,  0.0168, -0.0494, -0.0404,
         0.0384, -0.0317,  0.0768, -0.0657,  0.0386,  0.0210,  0.0511, -0.0576,
         0.0239,  0.0447,  0.0085,  0.0167,  0.0412,  0.0634, -0.0174,  0.0035,
         0.0966, -0.0297, -0.0334, -0.0301, -0.0172,  0.0815, -0.0304,  0.0325,
         0.0709, -0.0821, -0.0337,  0.0528,  0.0402,  0.0118,  0.0467,  0.0142,
         0.0376,  0.0633,  0.0058, -0.0345,  0.0118, -0.0283, -0.0153,  0.0122,
        -0.0482, -0.0007,  0.0530,  0.0229,  0.0018,  0.0411,  0.0331, -0.0092,
        -0.0257, -0.0284, -0.0718, -0.0565, -0.0249,  0.0136, -0.0759,  0.0469,
         0.0046, -0.0175,  0.0146, -0.0361, -0.0462,  0.0095,  0.0864,  0.0366,
         0.0615, -0.0319,  0.0134,  0.0178,  0.0289, -0.0220,  0.0906,  0.0848,
         0.0577,  0.0491, -0.0766,  0.0261, -0.0275, -0.0189,  0.0234,  0.0464,
        -0.0194,  0.0826, -0.0442, -0.0603, -0.0508,  0.0328,  0.0643,  0.0694,
        -0.0228, -0.0822, -0.0300,  0.0811,  0.0212,  0.0644,  0.0325, -0.0328],
       requires_grad=True))

Layer: encoder.3.0.block.1.values_projection._packed_params._packed_params
Parameters: (tensor([[-0.0240, -0.0370, -0.0566,  ..., -0.0131, -0.0261,  0.1219],
        [ 0.0806, -0.0479,  0.0675,  ..., -0.0632,  0.0218, -0.0327],
        [-0.0065, -0.0566, -0.0806,  ...,  0.0501,  0.0523,  0.0871],
        ...,
        [-0.0022, -0.1002,  0.0871,  ..., -0.0348, -0.0871,  0.0065],
        [ 0.0109,  0.1503,  0.1002,  ...,  0.0305, -0.0283,  0.0065],
        [ 0.0087,  0.0196,  0.0261,  ...,  0.1372,  0.1002, -0.0109]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0021776140201836824,
       zero_point=0), Parameter containing:
tensor([-1.8627e-02, -4.3878e-02,  3.0386e-02, -9.0771e-03,  6.4997e-02,
         1.3909e-02,  4.2109e-02, -4.6108e-02, -6.3745e-02,  6.8599e-02,
        -1.9122e-02,  4.6904e-02, -1.7008e-02, -5.4764e-02, -4.5532e-02,
         7.5426e-02,  4.6319e-02, -5.5850e-02,  5.6144e-02,  6.4452e-02,
        -3.7236e-02, -3.3246e-02,  5.1906e-02, -1.6680e-02, -2.2416e-02,
         7.6488e-02,  6.1575e-03,  1.1236e-03, -4.2900e-02,  1.6354e-02,
         3.8671e-02,  2.8949e-02,  4.7820e-03,  3.6182e-02, -2.2576e-02,
        -7.4321e-02, -5.1352e-02, -4.4083e-02, -6.4162e-02, -2.6958e-02,
         4.4976e-02, -5.6647e-02,  6.3612e-02, -4.0450e-02, -1.5776e-05,
         9.9622e-03, -6.4098e-02,  1.9413e-02,  3.3225e-02,  4.1251e-02,
        -4.7372e-02, -1.1767e-02,  4.8844e-03, -5.6332e-02, -6.4923e-02,
         1.1726e-03, -5.9593e-02, -2.6627e-02, -5.7443e-02, -3.0700e-02,
        -1.5003e-02, -5.3839e-02,  4.4623e-02,  5.5929e-02, -5.4194e-02,
        -3.0195e-02, -4.3561e-03,  5.1839e-02,  3.9929e-02, -7.5012e-03,
        -5.6717e-02,  6.1861e-02,  3.7536e-02, -1.3286e-02, -1.4462e-02,
        -3.0207e-02,  7.5615e-02, -2.9839e-02,  3.7269e-02, -3.3005e-02,
         6.5594e-02, -4.8426e-02,  5.7481e-02, -4.3456e-02, -4.7630e-02,
         5.8360e-02, -4.5112e-02,  1.7742e-02,  3.0869e-02,  3.8056e-02,
        -4.6854e-02,  3.8081e-02,  7.1573e-03, -5.8796e-02, -9.0919e-03,
        -6.7218e-02,  3.3946e-02, -7.7046e-02,  4.9335e-02, -2.1596e-02,
        -3.3229e-02,  3.7816e-02,  3.3158e-02, -1.6959e-02, -2.8203e-02,
         3.5621e-02, -2.4840e-02, -1.2198e-02, -1.4581e-02, -3.0446e-02,
        -2.2278e-02, -3.4568e-02, -6.8032e-02,  4.4890e-02, -1.9235e-02,
         4.3933e-03,  7.1321e-02, -6.7825e-02, -6.6121e-02, -1.7426e-02,
         5.0659e-02, -3.7577e-02, -4.3897e-02,  5.0975e-02,  4.4334e-02,
        -4.3769e-03,  2.2281e-02, -6.0068e-02, -6.2605e-02,  8.9926e-03,
         1.0928e-03,  5.0441e-02,  2.3453e-03,  2.5639e-02, -2.4920e-02,
         5.7549e-02,  3.2589e-02, -5.5314e-02,  5.2479e-03, -4.9523e-02,
        -3.2717e-02, -2.5194e-02, -3.5227e-02,  6.7698e-03,  1.6051e-02,
         3.1840e-02, -5.6988e-02,  3.2806e-03, -3.2078e-02,  2.0541e-02,
         6.6848e-02, -3.5439e-02, -4.2255e-02, -6.1124e-02, -1.5311e-02,
        -4.3044e-02,  4.4349e-02,  4.1350e-02,  3.8279e-02, -2.2775e-02,
         3.4741e-02, -2.9598e-02,  5.0590e-02, -1.4463e-02, -2.4443e-02,
         2.1954e-02,  6.7381e-02, -3.3791e-02, -5.0199e-02,  6.9101e-02,
        -7.4168e-02, -1.6203e-02, -5.1621e-02, -2.1846e-02,  2.0785e-02,
         5.9565e-02, -7.1396e-02, -2.6188e-02, -5.7225e-02, -2.3865e-02,
        -4.1376e-02, -6.0427e-02,  1.2254e-02,  3.8355e-02, -5.5849e-02,
        -3.3683e-03, -3.6224e-02,  2.7925e-02, -3.9555e-02,  5.4546e-02,
         2.7005e-02, -1.2565e-02], requires_grad=True))

Layer: encoder.3.0.block.1.keys_projection._packed_params._packed_params
Parameters: (tensor([[ 0.0601,  0.0237, -0.0437,  ..., -0.1312,  0.0109, -0.0128],
        [-0.0437,  0.0492, -0.0310,  ...,  0.0747, -0.0620, -0.0219],
        [ 0.0292,  0.0510,  0.0128,  ...,  0.0073,  0.0583,  0.0036],
        ...,
        [ 0.0328, -0.0073,  0.0182,  ...,  0.0583, -0.0529, -0.0219],
        [ 0.0255,  0.0820, -0.0437,  ..., -0.0547, -0.1002, -0.0109],
        [-0.0292,  0.1130, -0.0766,  ..., -0.0911, -0.1185, -0.0036]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0018226621905341744,
       zero_point=0), Parameter containing:
tensor([ 0.0168, -0.0558,  0.0213,  0.0414, -0.0583,  0.0139,  0.0314, -0.0470,
         0.0552, -0.0622,  0.0093,  0.0062, -0.0072,  0.0152, -0.0001,  0.0234,
        -0.0667, -0.0265, -0.0547, -0.0699, -0.0379, -0.0578,  0.0260,  0.0734,
         0.0189,  0.0241, -0.0370,  0.0735,  0.0407,  0.0032, -0.0133,  0.0431,
        -0.0107,  0.0042,  0.0185, -0.0412, -0.0114, -0.0594,  0.0718, -0.0340,
         0.0107,  0.0163, -0.0102, -0.0072,  0.0441,  0.0628,  0.0483, -0.0207,
        -0.0254, -0.0603,  0.0025, -0.0076, -0.0461,  0.0279, -0.0520, -0.0339,
        -0.0384, -0.0415, -0.0133,  0.0611, -0.0623, -0.0077,  0.0157,  0.0383,
        -0.0068, -0.0006,  0.0014, -0.0054, -0.0156, -0.0469, -0.0197,  0.0352,
        -0.0692,  0.0423, -0.0515, -0.0297,  0.0169, -0.0503, -0.0486,  0.0192,
        -0.0201,  0.0002, -0.0257, -0.0357,  0.0346,  0.0196, -0.0195,  0.0691,
        -0.0270,  0.0508,  0.0278, -0.0698,  0.0266, -0.0061,  0.0170,  0.0100,
         0.0046,  0.0311, -0.0166, -0.0495, -0.0763,  0.0646,  0.0171,  0.0228,
        -0.0524, -0.0485, -0.0271, -0.0375,  0.0691,  0.0131, -0.0012,  0.0076,
         0.0172, -0.0607,  0.0397,  0.0153,  0.0088, -0.0357, -0.0306, -0.0449,
         0.0663,  0.0119,  0.0696,  0.0344, -0.0591,  0.0156,  0.0198,  0.0723,
         0.0380,  0.0437, -0.0254, -0.0478, -0.0587, -0.0112,  0.0051, -0.0472,
         0.0520,  0.0189, -0.0246, -0.0427,  0.0139,  0.0367,  0.0469,  0.0610,
         0.0336, -0.0686,  0.0961, -0.0320,  0.0174,  0.0419,  0.0609, -0.0354,
         0.0156, -0.0651,  0.0194,  0.0004, -0.0354,  0.0555, -0.0491,  0.0262,
         0.0116,  0.0006, -0.0023,  0.0572, -0.0793, -0.0079, -0.0343, -0.0732,
         0.0070, -0.0495, -0.0511,  0.0067, -0.0445, -0.0367,  0.0648, -0.0599,
         0.0617,  0.0240,  0.0326, -0.0410,  0.0059, -0.0547, -0.0079,  0.0153,
         0.0417,  0.0592,  0.0723, -0.0002, -0.0341,  0.0435, -0.0296, -0.0106],
       requires_grad=True))

Layer: encoder.3.0.block.1.final_projection._packed_params._packed_params
Parameters: (tensor([[ 0.0071,  0.2029, -0.0354,  ..., -0.0047,  0.0896,  0.1062],
        [ 0.0236,  0.1014,  0.0377,  ...,  0.1085,  0.0802, -0.0236],
        [ 0.0613, -0.1085, -0.0094,  ..., -0.0307,  0.0401, -0.0401],
        ...,
        [ 0.0849, -0.0212,  0.1109,  ..., -0.0330, -0.0637, -0.0613],
        [ 0.0967, -0.0708,  0.0425,  ..., -0.0873,  0.1510,  0.0519],
        [ 0.0967,  0.0661, -0.0047,  ...,  0.0425, -0.1062, -0.0142]],
       size=(192, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.002359129721298814,
       zero_point=0), Parameter containing:
tensor([-0.0725, -0.0306,  0.0382, -0.0450, -0.0248,  0.0554,  0.0271,  0.0127,
         0.0490, -0.0521,  0.0512, -0.0082, -0.0057,  0.0657, -0.0411, -0.0007,
        -0.0245, -0.0377,  0.0111,  0.0190,  0.0687, -0.0731, -0.0201,  0.0064,
        -0.0029, -0.0624,  0.0508, -0.0592,  0.0223,  0.0777,  0.0588, -0.0685,
        -0.0548, -0.0096, -0.0550,  0.0440, -0.0405, -0.0547,  0.0494,  0.0116,
        -0.0223,  0.0511, -0.0050, -0.0491, -0.0296,  0.0342, -0.0598, -0.0008,
        -0.0165, -0.0543,  0.0546,  0.0563, -0.0376, -0.0461, -0.0027,  0.0548,
        -0.0879, -0.0259, -0.0028,  0.0713,  0.0370,  0.0055, -0.0355, -0.0775,
        -0.0498,  0.0321, -0.0626,  0.0301,  0.0382,  0.0303, -0.0630, -0.0092,
        -0.0445,  0.0410, -0.0400,  0.0551, -0.0267, -0.0818, -0.0038,  0.0616,
         0.0634, -0.0630,  0.0311, -0.0835,  0.0552,  0.0238, -0.0953, -0.0481,
        -0.0294, -0.0777, -0.0438, -0.0194, -0.0773,  0.0366, -0.0149, -0.0552,
        -0.0363, -0.0669, -0.0205,  0.0311,  0.0250,  0.0818,  0.0847, -0.0125,
         0.0349, -0.0081,  0.0387, -0.0281,  0.0227,  0.0646,  0.0292,  0.0387,
        -0.0059,  0.0772, -0.0122,  0.0383,  0.0394, -0.0438, -0.0282, -0.0397,
        -0.0041, -0.0680,  0.0183, -0.0552, -0.0111, -0.0258, -0.0046,  0.0357,
        -0.0750, -0.0630, -0.0629, -0.0218, -0.0425,  0.0422, -0.0437,  0.0251,
         0.0008,  0.0128, -0.0250,  0.0117, -0.0781, -0.0688,  0.0210, -0.0149,
         0.0543,  0.0209, -0.0037, -0.0060,  0.0476,  0.0156,  0.0639,  0.0396,
         0.0224, -0.0409, -0.0427,  0.0212,  0.0073, -0.0624, -0.0373, -0.0449,
        -0.0735, -0.0462,  0.0033,  0.0199, -0.0322, -0.0355, -0.0203, -0.0158,
        -0.0401,  0.0173, -0.0711, -0.0577,  0.0353,  0.0215, -0.0341, -0.0500,
         0.0227,  0.0057, -0.0758,  0.0668,  0.0465,  0.0014,  0.0652, -0.0614,
         0.0710,  0.0663,  0.0487,  0.0682, -0.0090,  0.0548, -0.0081, -0.0786],
       requires_grad=True))

Layer: encoder.3.1.block.0.weight
Parameters: tensor([0.9066, 0.9549, 0.9005, 0.9677, 0.9374, 0.9001, 0.9792, 0.9835, 0.9143,
        0.9321, 0.9239, 0.9890, 0.9120, 0.9731, 0.9166, 0.9120, 0.8989, 0.8742,
        0.9751, 0.9445, 0.9493, 0.9389, 0.9456, 0.9089, 0.9042, 0.9618, 0.9344,
        0.9328, 0.9151, 0.9459, 0.9610, 0.9111, 0.9331, 0.8996, 0.9495, 0.9342,
        0.9607, 0.9440, 0.9047, 0.9750, 0.9387, 0.9338, 0.8973, 0.9585, 0.8789,
        0.9052, 0.9494, 0.9259, 0.9597, 0.9287, 0.9070, 0.9320, 0.9194, 0.9477,
        0.8842, 0.9239, 0.9677, 0.8407, 0.8824, 0.9557, 0.9609, 0.9392, 0.9502,
        0.9706, 0.9700, 0.9410, 0.9798, 0.9730, 0.9317, 0.9516, 0.9685, 0.9847,
        0.9344, 0.9720, 0.9624, 0.9134, 0.9182, 0.8976, 0.9797, 0.9265, 0.8619,
        0.9799, 0.9834, 0.9351, 0.9317, 0.9333, 0.8785, 0.9528, 0.8928, 0.9045,
        0.9446, 0.9226, 0.9423, 0.9003, 0.9174, 0.9145, 0.8956, 0.9545, 0.9564,
        0.9211, 0.9645, 0.9022, 0.9441, 0.9229, 0.9321, 0.9309, 0.8879, 1.0082,
        0.9600, 0.9380, 0.9158, 0.9012, 0.9356, 0.9247, 0.9203, 0.9054, 0.9848,
        0.9380, 1.0223, 0.9361, 0.8965, 0.9450, 0.9288, 0.9287, 0.8653, 0.9260,
        0.9288, 0.9780, 0.9203, 0.9156, 0.9730, 0.9319, 0.9143, 0.9719, 0.9519,
        0.9156, 0.9135, 0.8683, 0.9648, 0.8951, 0.8977, 0.9285, 0.9385, 0.8888,
        0.8578, 0.9370, 0.9535, 0.9214, 0.9613, 0.9586, 0.9445, 0.9689, 0.9685,
        0.9414, 0.9345, 0.8977, 0.9753, 0.9153, 0.9055, 0.9173, 0.9642, 0.9099,
        0.9438, 0.9518, 0.9411, 0.9281, 0.9639, 0.8878, 0.8680, 0.9565, 0.9291,
        0.9047, 0.9400, 0.8986, 0.9066, 0.9123, 0.9656, 0.8870, 0.9437, 0.9578,
        0.9383, 0.9238, 0.9340, 0.8809, 0.9424, 0.9483, 0.9663, 0.9065, 0.9823,
        0.9635, 0.9082, 0.8686])

Layer: encoder.3.1.block.0.bias
Parameters: tensor([ 0.0640, -0.0412, -0.0450, -0.0043, -0.0797,  0.0126, -0.0350,  0.0474,
        -0.0172,  0.0257, -0.0365,  0.0557,  0.0105, -0.0296,  0.0206, -0.0070,
        -0.0298, -0.0201,  0.0545,  0.0808,  0.0003,  0.0237, -0.0207, -0.0193,
         0.0203, -0.0337, -0.0438, -0.0133,  0.0446, -0.0189, -0.0452,  0.0227,
         0.0084,  0.0260, -0.0393,  0.0334, -0.0027, -0.0431,  0.0121, -0.0265,
         0.0178, -0.0301, -0.0138,  0.0182, -0.0073,  0.0010,  0.0332,  0.0228,
         0.0280, -0.0229,  0.0249,  0.0012,  0.0495, -0.0369, -0.0593,  0.0225,
         0.0312, -0.0625,  0.0130, -0.0029, -0.0475, -0.0556, -0.0030,  0.0392,
        -0.0017, -0.0257, -0.0300,  0.0437, -0.0042, -0.0493, -0.0644, -0.0108,
        -0.0267, -0.0232,  0.0668, -0.0216,  0.0085,  0.0396, -0.0143,  0.0099,
        -0.0199, -0.0405,  0.0533,  0.0488, -0.0428, -0.0322, -0.0129,  0.0172,
        -0.0054, -0.0062, -0.0377,  0.0025,  0.0131, -0.0118, -0.0477, -0.0016,
         0.0145, -0.0300, -0.0413, -0.0050, -0.0441, -0.0002, -0.0232, -0.0096,
         0.0375,  0.0242,  0.0130,  0.0081,  0.0194,  0.0199,  0.0289,  0.0190,
        -0.0513, -0.0197, -0.0375, -0.0689,  0.0549, -0.0085, -0.0463,  0.0283,
        -0.0262, -0.0158, -0.0162,  0.0567, -0.0191, -0.0368,  0.0478,  0.0231,
         0.0231, -0.0540,  0.0366,  0.0094, -0.0408, -0.0675, -0.0067,  0.0089,
        -0.0450, -0.0199, -0.0600,  0.0037, -0.0192,  0.0005, -0.0569, -0.0032,
        -0.0049,  0.0150,  0.0424,  0.0389, -0.0417,  0.0022, -0.0324, -0.0135,
        -0.0304, -0.0436,  0.0334,  0.0204,  0.0506, -0.0208, -0.0265, -0.0456,
         0.0291, -0.0158, -0.0707, -0.0562, -0.0822, -0.0022, -0.0199,  0.0187,
        -0.0248,  0.0216, -0.0093, -0.0033, -0.0198,  0.0036,  0.0293,  0.0505,
         0.0038, -0.0133, -0.0625,  0.0333, -0.0020,  0.0366,  0.0278,  0.0276,
        -0.0294, -0.0704, -0.0076, -0.0242,  0.0201,  0.0321,  0.0307,  0.0227])

Layer: encoder.3.1.block.1.0._packed_params._packed_params
Parameters: (tensor([[-0.0146, -0.0088, -0.0540,  ..., -0.0467, -0.0847, -0.0380],
        [ 0.0146,  0.0453, -0.0190,  ..., -0.0292, -0.0482, -0.0088],
        [ 0.0394, -0.0175,  0.0248,  ..., -0.0818, -0.0453, -0.0102],
        ...,
        [-0.0380, -0.0336,  0.0891,  ...,  0.0263,  0.0073, -0.0599],
        [-0.0336,  0.0482,  0.0175,  ...,  0.0424, -0.0482,  0.0424],
        [ 0.0716,  0.0482, -0.0234,  ..., -0.0336, -0.0175,  0.0190]],
       size=(768, 192), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0014605092583224177,
       zero_point=0), Parameter containing:
tensor([ 3.3441e-02,  3.8430e-02, -7.1661e-02,  1.8153e-02,  1.7913e-02,
         1.6649e-02,  2.7896e-02, -8.7044e-02, -1.1498e-02, -3.3865e-02,
         1.5553e-03, -1.0537e-02, -4.1885e-02, -5.0961e-02, -7.7284e-02,
        -8.8529e-02,  1.8900e-02, -2.5519e-02, -6.8470e-02,  3.9677e-02,
         2.8829e-02, -9.1577e-02, -5.1707e-02, -7.6878e-02, -4.3976e-03,
         2.9819e-02, -8.1328e-02, -4.5817e-02,  3.2524e-02, -7.1132e-02,
        -3.1062e-02, -9.3084e-02, -5.5924e-02,  4.5697e-02,  2.7919e-02,
         3.1967e-02, -8.6873e-02, -9.6734e-02, -1.0127e-01, -1.4408e-02,
         1.0889e-02,  4.1143e-02, -4.8754e-02, -9.8918e-03, -1.3009e-02,
         5.0141e-02,  3.5069e-03, -4.4662e-02, -3.1905e-02,  2.1381e-02,
        -3.2227e-02,  2.1005e-02, -7.4122e-02, -6.8380e-02, -4.6641e-02,
        -9.0629e-02, -9.0009e-02,  1.4431e-02, -8.8289e-02, -3.5326e-02,
        -7.2763e-02,  1.6765e-02, -6.8302e-02, -7.0011e-02, -2.1894e-02,
         2.5839e-02, -1.8000e-02, -5.1103e-02, -3.3540e-02, -6.7795e-02,
        -1.9931e-02,  5.5793e-03,  2.0302e-02, -1.6584e-02, -7.4950e-02,
        -8.5374e-02, -9.0080e-02, -5.0019e-02, -1.5877e-02, -9.6485e-02,
        -6.6753e-04,  3.3127e-02, -8.2108e-03, -6.8637e-02,  4.4627e-03,
         8.7925e-03,  2.1210e-02, -8.2811e-02, -7.0205e-02, -1.9657e-02,
         9.8905e-03, -3.4725e-02, -9.5684e-02, -3.2369e-02, -4.4377e-03,
        -2.9430e-02,  3.2112e-02, -3.1664e-02,  9.8534e-03, -1.1850e-02,
        -3.0268e-02, -8.8512e-02, -7.3054e-02, -4.8161e-02, -1.6475e-03,
        -8.4980e-02,  5.3006e-04,  1.4045e-02, -2.3867e-02, -8.3876e-02,
        -9.3897e-02, -2.3258e-02, -4.4264e-02, -3.0870e-02, -3.0806e-02,
        -2.6533e-02, -8.6030e-02, -7.0670e-02, -5.8013e-02,  9.9902e-03,
        -7.9273e-02,  7.6808e-03,  3.6190e-02, -3.4774e-03, -8.8645e-02,
        -9.8625e-02,  3.9855e-02, -6.3530e-02,  3.1181e-03, -6.0887e-02,
        -1.1161e-01, -9.9400e-02, -2.2841e-02, -4.7200e-02,  1.7902e-02,
        -8.1330e-02, -9.6801e-02, -3.0196e-02, -6.4598e-02, -9.1760e-02,
        -4.9230e-02, -2.2816e-02,  2.3309e-02, -7.8388e-02, -7.8573e-02,
        -8.8531e-02, -2.5693e-02, -6.3832e-02, -4.8289e-02,  1.7903e-02,
        -2.6012e-02, -3.2069e-03, -9.4093e-02,  8.5007e-03, -2.2988e-02,
        -2.3428e-02,  5.9329e-04, -7.7983e-02, -6.6312e-02,  3.2075e-03,
         2.5644e-02, -8.0212e-02, -8.8107e-02, -8.7515e-03,  4.0728e-02,
        -5.0416e-02,  4.7111e-02, -8.4791e-02, -5.4048e-02, -8.8282e-02,
        -9.5644e-02, -8.8619e-02,  7.3811e-03,  2.7402e-02,  3.6557e-02,
        -3.7604e-02, -7.4199e-02,  2.7139e-02,  1.6120e-02,  9.6329e-03,
        -6.8838e-02, -1.2372e-02,  7.2094e-03, -3.7336e-02,  3.4935e-02,
         3.0687e-02,  9.1444e-04, -9.3370e-03, -3.3904e-03,  2.5988e-02,
        -8.1621e-02, -2.0207e-02, -8.1310e-02, -5.6275e-02,  2.2272e-02,
        -8.5633e-02,  9.2056e-03, -2.7302e-02, -5.9395e-02,  3.2839e-02,
        -5.4196e-02, -3.1749e-02, -1.0364e-01,  1.9338e-02,  2.2273e-02,
         1.2328e-02,  1.6391e-03, -5.4641e-03, -7.7859e-02, -5.4960e-02,
         1.7740e-02,  4.4753e-02, -6.7995e-02,  1.3576e-02, -8.2310e-02,
         1.3889e-02, -2.0589e-02, -5.3974e-02,  9.8417e-03, -8.7330e-02,
        -9.9583e-02, -3.7876e-02, -5.2796e-03,  1.9224e-02,  1.4179e-02,
         1.1080e-02, -5.2723e-02,  2.1289e-02, -2.4432e-02, -8.9743e-02,
        -7.6236e-02, -2.5705e-03, -6.5596e-02,  1.8978e-02, -4.9269e-02,
         2.7551e-02,  2.5665e-03, -6.3687e-02, -2.3700e-02, -9.8900e-02,
        -1.2857e-04, -9.3810e-02,  1.5482e-02, -7.1694e-03,  3.3307e-02,
        -9.0199e-02, -8.0881e-02, -7.3295e-02, -3.1349e-02, -4.9192e-02,
         2.2334e-02, -1.6943e-02,  1.3723e-02, -7.0845e-02, -9.9997e-02,
         2.8440e-02,  1.6336e-02, -4.0416e-02, -2.8165e-03, -7.7386e-02,
        -6.5318e-02, -8.4303e-02, -4.9964e-03, -9.1975e-02,  4.5618e-03,
        -9.5014e-02, -4.8431e-03,  4.0951e-02,  4.0655e-02,  9.4432e-03,
        -6.2935e-02,  2.5214e-02, -6.4543e-03, -4.2306e-02, -4.5869e-02,
        -7.5739e-02, -8.3143e-02, -4.5556e-02, -5.6682e-02, -7.3457e-02,
        -6.1315e-02, -1.0809e-02, -1.7806e-02,  1.2333e-03,  5.1923e-03,
        -2.8557e-02, -4.9020e-02, -3.9339e-02,  4.8971e-02, -3.0262e-02,
        -2.2337e-02, -1.0196e-01, -3.7748e-02, -9.3831e-02, -6.7889e-02,
        -1.0619e-01, -7.1129e-02, -5.7592e-02, -9.5269e-02, -1.0721e-02,
        -1.0152e-01, -1.7403e-02, -1.4064e-02,  6.4221e-03, -4.2807e-02,
        -9.7154e-02, -5.4263e-02,  2.1896e-02, -1.1734e-02, -5.4799e-02,
        -6.9422e-02,  5.3353e-03, -3.8527e-02, -5.9866e-02,  1.8368e-02,
        -8.8982e-02,  2.6855e-02, -9.2068e-03, -2.2966e-02, -6.5606e-02,
        -4.1881e-03, -3.6888e-02, -5.0803e-02, -5.7827e-02, -1.0067e-01,
        -3.5818e-02, -5.1365e-02, -8.9642e-02, -5.3733e-02, -1.0399e-01,
        -1.0929e-02,  9.0069e-03, -1.0012e-01,  2.3657e-02,  3.8638e-03,
        -9.6186e-02, -1.1113e-02,  2.6285e-02,  1.4663e-02, -6.9535e-02,
        -6.0718e-03,  6.3939e-03, -2.5461e-02, -2.7860e-02, -1.8090e-02,
        -4.3109e-02, -1.5994e-02,  1.4864e-02, -2.9525e-02, -2.4714e-02,
        -1.0707e-02, -4.2378e-02, -6.6208e-02, -2.2795e-02, -8.1938e-02,
        -1.2115e-02, -5.1927e-02,  2.2454e-02, -7.8778e-02, -8.3510e-02,
        -3.3089e-02,  1.2228e-02, -1.5605e-02, -3.9323e-02, -4.6221e-02,
        -4.3302e-03,  2.3649e-02, -3.3788e-02, -1.9352e-02,  1.4964e-02,
        -3.2664e-02,  3.2163e-02,  1.5573e-02, -6.0458e-02,  2.4822e-03,
        -3.1644e-02, -3.6982e-02, -6.5191e-02,  8.5853e-03, -3.1106e-02,
        -3.2412e-02, -1.5203e-02, -5.9668e-02, -2.7540e-02, -7.8562e-02,
        -2.3305e-02,  2.1029e-02, -3.8998e-02, -1.4977e-02, -1.0522e-01,
        -3.0929e-02, -1.4339e-02, -3.5542e-02, -3.6579e-02, -4.1272e-02,
         2.1958e-02,  1.6541e-03, -3.2110e-02, -3.9092e-02, -5.9712e-02,
        -1.0188e-01, -3.6707e-03, -1.0450e-03, -3.5000e-02, -8.1435e-02,
        -4.8767e-02, -8.7150e-02, -8.0051e-02, -1.9912e-03,  2.4357e-02,
        -8.5525e-02, -3.9574e-02, -8.7585e-02,  4.1714e-02, -3.4004e-02,
         4.0280e-03, -7.3623e-02, -9.5764e-02, -7.6754e-02, -3.9082e-02,
         1.9052e-03, -9.8114e-02, -5.6225e-02, -4.6219e-02,  3.9182e-02,
        -9.6728e-02,  9.4538e-03, -6.9746e-02, -7.0175e-02, -6.4498e-02,
        -6.9286e-02, -6.5739e-02, -6.2058e-02, -7.7012e-03, -1.0953e-01,
        -6.3375e-02,  6.6056e-03, -2.3941e-02,  4.2272e-02, -1.2117e-02,
        -6.1747e-02,  2.6530e-02, -4.8446e-02, -6.0966e-02,  2.2526e-02,
        -6.8358e-02,  2.7224e-02,  6.7504e-03, -2.9032e-02, -2.6012e-02,
        -2.3698e-02, -3.8934e-02, -9.4467e-02, -1.0349e-02, -3.1300e-02,
        -7.7978e-02,  3.7067e-04, -6.5569e-02,  1.7830e-02, -7.5138e-02,
         5.9299e-03, -1.9459e-03, -4.6472e-02, -7.0932e-02,  2.0597e-02,
         2.3119e-02, -2.7020e-02, -3.3173e-02, -5.9831e-02, -7.2248e-02,
        -5.1415e-02, -3.2829e-02, -4.5160e-02,  2.3913e-02, -5.6768e-02,
        -6.8424e-02, -1.1249e-02, -2.1859e-02, -3.0401e-02, -1.1159e-02,
        -7.0607e-02, -3.4970e-02,  2.8226e-03,  4.2129e-03,  8.0755e-03,
         2.6925e-02, -1.1704e-01, -1.1700e-02, -9.7348e-02, -4.3125e-02,
        -1.8860e-02, -2.1843e-02,  2.8692e-02,  1.1608e-02, -3.9565e-04,
        -3.9528e-04, -5.2813e-02,  7.9253e-03,  3.2017e-02, -7.9881e-02,
         2.5331e-02, -6.8579e-02, -2.0219e-02,  1.3410e-02,  2.5089e-02,
        -2.5439e-02,  2.3273e-02, -4.7141e-02, -4.9935e-02, -8.0539e-02,
        -7.5962e-02, -6.5641e-03, -5.9874e-02,  1.9584e-02, -7.0711e-02,
         7.8002e-03, -3.1648e-02,  2.8595e-02,  5.1081e-02,  1.3754e-02,
        -1.1641e-02,  5.1147e-02,  2.9803e-02, -9.5134e-02, -1.0099e-01,
        -4.3209e-03,  2.4611e-02,  7.5309e-03, -2.4803e-02, -4.0680e-02,
        -9.3661e-02,  3.8314e-02, -8.3644e-02, -4.8021e-02,  1.9035e-02,
         3.0829e-02, -3.7275e-04, -7.4077e-02,  2.5226e-02,  2.8870e-02,
         1.4766e-02,  2.2358e-02,  2.4847e-03, -8.5020e-02, -7.2200e-02,
        -4.8243e-03, -5.8959e-02, -5.7697e-02, -9.3426e-02, -4.7220e-02,
        -1.0181e-01, -4.2730e-02,  2.0634e-03, -2.2809e-02, -7.9139e-02,
        -6.7606e-03, -3.2039e-02, -2.7061e-02, -6.1321e-02, -2.4493e-02,
         2.0769e-02, -7.0282e-02,  3.6356e-02, -6.0010e-02, -3.2055e-02,
         7.5805e-04, -8.6813e-02, -7.0397e-02, -6.5407e-02, -3.9945e-02,
        -5.6572e-02, -6.7270e-02, -8.6002e-02, -4.1048e-02, -3.3401e-02,
        -5.7577e-02, -3.8586e-02,  1.9643e-03, -5.9244e-02,  1.3922e-02,
         2.0540e-02,  2.0141e-02, -5.3601e-02, -7.3935e-02,  2.8122e-02,
        -3.1121e-02, -3.0290e-03, -1.0093e-01,  3.0509e-02,  3.9773e-02,
        -2.6442e-02, -1.9202e-02,  3.4863e-02, -3.9349e-02, -2.9100e-02,
        -8.8096e-02, -8.1039e-02, -9.1917e-02, -3.0813e-02, -9.9715e-02,
        -9.3793e-03, -3.3948e-02, -1.8608e-02, -6.2622e-02,  1.9663e-02,
         3.5373e-02, -1.4685e-02, -9.7672e-02, -6.0088e-02, -6.2356e-02,
        -4.4013e-03, -8.4475e-02, -7.7166e-03, -7.5485e-02, -8.3846e-02,
        -3.6766e-02, -7.2459e-02,  1.0944e-02,  1.9061e-02, -7.1653e-02,
        -6.9866e-02, -2.1593e-02, -6.5706e-02, -7.1916e-03,  7.2179e-04,
        -8.5615e-02, -3.6451e-02, -1.9299e-02, -4.8186e-02,  1.5110e-02,
        -2.7953e-02, -5.5852e-02,  1.1687e-02, -3.2649e-02,  3.7290e-02,
        -3.0685e-02, -9.8530e-02, -5.1073e-02,  1.8729e-02,  1.8008e-02,
        -3.5330e-02, -5.1840e-02, -5.7237e-02, -1.0756e-05, -8.1662e-02,
        -1.7157e-02,  6.3652e-03, -1.5074e-02, -1.2757e-02, -3.6156e-02,
        -3.9825e-02,  8.7369e-03, -7.0702e-03, -4.1305e-02,  1.8006e-02,
        -9.3407e-02, -4.5959e-04, -6.8399e-02, -7.0077e-02,  1.9552e-02,
        -3.1802e-02, -1.1662e-02,  1.1397e-02,  2.1372e-02,  3.5546e-02,
        -3.1894e-02, -4.9944e-02, -5.3372e-02,  2.5399e-02, -2.8569e-04,
        -7.8378e-02,  1.0062e-03, -4.9141e-02, -3.6139e-03, -6.2803e-02,
        -1.1202e-02,  3.8185e-03, -7.8598e-02, -9.1733e-02, -7.7255e-02,
         3.3476e-02, -6.8725e-02, -3.5863e-02, -4.3473e-02,  2.7754e-02,
         1.8694e-02, -4.2100e-02, -5.3840e-02, -5.4418e-02, -1.4064e-02,
        -2.5241e-02, -2.7827e-02,  1.1537e-02,  2.0792e-02, -2.2233e-02,
         2.1526e-02,  2.0989e-02,  1.8733e-02, -8.9651e-02,  4.9983e-03,
        -4.2422e-02, -5.1920e-02, -5.2299e-03, -5.5330e-02, -1.6249e-03,
         3.4618e-02, -2.1098e-03, -6.5688e-02, -9.0384e-02, -3.0208e-02,
        -5.5144e-02, -3.1086e-02, -4.3077e-03,  1.3133e-02, -2.1231e-02,
        -2.3042e-02, -8.2960e-02, -4.5219e-02, -1.7343e-02, -7.1771e-02,
        -1.8579e-02, -5.4593e-03, -1.0340e-02, -4.5149e-02,  1.7131e-02,
        -3.3945e-02,  1.7480e-02, -7.1708e-02, -5.8631e-02, -6.7172e-03,
         3.5955e-02, -3.6470e-02, -5.1661e-02, -4.4575e-02,  1.6170e-02,
         1.2500e-02, -2.4210e-02, -9.7514e-02, -5.7835e-02,  1.5479e-02,
        -1.0234e-02,  1.1082e-02, -2.5356e-02,  1.1103e-02, -2.0862e-02,
         2.5243e-02, -4.0036e-02,  1.9773e-02, -6.5612e-03,  7.6747e-03,
        -8.5329e-02,  3.0811e-02, -9.7028e-03, -4.5356e-02, -2.3893e-02,
        -4.8819e-02, -1.7807e-02, -3.1872e-02, -7.4933e-02,  2.4569e-02,
        -2.7530e-02, -3.4258e-02, -1.0015e-01, -7.1280e-02,  3.0927e-02,
        -3.3897e-02, -2.9214e-02,  1.1343e-02], requires_grad=True))

Layer: encoder.3.1.block.1.2._packed_params._packed_params
Parameters: (tensor([[-0.0489,  0.0359, -0.0272,  ..., -0.0098,  0.0457, -0.0196],
        [ 0.0120,  0.0000, -0.0272,  ...,  0.0250,  0.0065, -0.0033],
        [ 0.0207,  0.0141,  0.0740,  ..., -0.0348, -0.0044,  0.0131],
        ...,
        [ 0.0370,  0.0152,  0.0033,  ..., -0.0174, -0.0457, -0.0163],
        [ 0.0402,  0.0000,  0.0076,  ..., -0.0272, -0.0283, -0.0305],
        [ 0.0054, -0.0185,  0.0076,  ...,  0.0087,  0.0435, -0.0370]],
       size=(192, 768), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0010876706801354885,
       zero_point=0), Parameter containing:
tensor([-1.0037e-03, -8.5418e-03,  3.2087e-02,  2.8338e-02, -1.6778e-02,
         2.5375e-02,  2.8864e-02, -1.1825e-02, -2.3556e-02,  2.6449e-02,
         3.4684e-02,  3.4674e-03, -2.7540e-02,  1.9697e-02, -6.3831e-02,
         2.0652e-02, -3.0529e-02,  1.1182e-02, -3.9847e-02,  3.1403e-02,
        -8.8374e-03,  2.0049e-03, -2.3910e-02,  5.4574e-02, -2.7673e-02,
         3.6172e-03,  1.3900e-02,  3.3865e-02, -3.0893e-02, -1.2857e-02,
         3.3860e-02,  4.0392e-03,  1.8213e-02,  2.3025e-03,  1.2871e-02,
        -1.1859e-02, -2.7720e-02, -4.2813e-02, -4.1133e-02,  6.6520e-03,
         1.5337e-02, -2.8147e-03,  7.5340e-03, -1.1510e-02,  3.9856e-02,
         1.0310e-02, -5.7408e-02,  9.6478e-04, -3.2182e-02,  3.4547e-02,
        -4.0738e-03, -1.0236e-02, -1.3705e-02,  1.5009e-02, -3.5272e-02,
        -2.1837e-02, -1.4615e-02, -6.1347e-03, -4.3597e-02, -3.0252e-02,
        -9.3754e-03,  4.2047e-02, -1.2771e-02, -1.8173e-04, -4.1749e-02,
        -1.4659e-02, -2.1413e-02, -1.3414e-02,  1.4907e-02, -2.4521e-02,
         3.4788e-02,  1.6397e-02, -1.3721e-02, -2.2453e-02, -1.3973e-02,
         1.7136e-03,  6.4993e-03, -9.5267e-03, -2.2603e-02, -6.4888e-03,
         2.7936e-02,  9.8066e-03, -5.3320e-02, -2.7568e-02, -2.3378e-02,
        -6.2156e-04, -1.1233e-02, -2.2080e-02,  2.4353e-02, -5.8303e-02,
        -1.0803e-02, -3.8393e-02, -3.6945e-02,  6.2695e-03,  2.3310e-02,
        -9.4642e-03, -2.7811e-02, -6.6660e-03,  1.9368e-02, -5.1195e-02,
         1.7265e-02,  2.5570e-02,  3.9772e-02, -2.9609e-02, -3.8756e-02,
         1.2818e-02, -3.7244e-03, -2.6336e-02,  3.4319e-03,  2.3178e-02,
        -4.9157e-04, -3.7813e-02,  8.8967e-03,  3.2749e-03,  1.8660e-03,
         4.1052e-02, -1.6205e-02, -1.6217e-05, -2.0909e-02, -4.3287e-02,
        -8.3200e-03,  2.6677e-02, -2.4542e-03, -2.1259e-02, -1.4268e-02,
        -5.3820e-04, -4.0703e-02, -2.2650e-03,  1.1175e-02,  4.7301e-03,
        -6.3202e-02, -5.1364e-02,  4.1999e-02,  4.7264e-02, -9.3971e-03,
        -1.1662e-02,  4.8651e-02, -6.7894e-04,  7.3210e-03,  2.1874e-02,
        -4.1737e-02, -2.9020e-02, -8.0362e-03, -1.2904e-02, -5.3793e-04,
        -5.6242e-02, -3.6868e-02, -7.1430e-02, -2.9752e-02, -1.1623e-02,
        -1.3045e-02,  2.8129e-02, -1.8660e-02,  9.9071e-03, -2.6548e-02,
        -1.3560e-02, -4.2182e-02,  1.5348e-02, -3.5814e-03, -2.2825e-03,
        -1.5248e-02,  3.1960e-04, -1.7675e-02,  1.0479e-02, -7.9718e-03,
         4.0915e-02,  3.7634e-02, -5.9756e-02,  2.3788e-02, -2.2634e-02,
        -3.2624e-02, -2.8276e-02,  3.1671e-02, -3.3303e-02, -4.6724e-02,
        -3.8836e-02, -4.1448e-02,  1.3491e-02, -2.0844e-02,  1.0571e-02,
        -3.1206e-02, -5.6161e-02, -3.4224e-02,  2.2168e-04,  3.7798e-02,
        -1.3469e-02,  9.4888e-03,  1.7160e-02,  6.7917e-04, -5.0891e-03,
        -2.9269e-02, -4.6736e-02], requires_grad=True))

