{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:52.744083Z",
     "start_time": "2024-05-04T02:38:52.729138Z"
    }
   },
   "source": [
    "import itertools # 是 Python 的內建模組，提供了一組用於處理迭代器的函數和工具。\n",
    "                 # 它包含了各種用於高效處理迭代器的函數，可以幫助我們編寫更簡潔、高效的代碼。\n",
    "import sys # 是 Python 的內建模組，提供了與 Python 解釋器和運行環境相關的功能。\n",
    "# sys.path 是一個列表，包含了 Python 解釋器在導入模組時會搜尋的路徑。\n",
    "# 當你使用 import 語句導入模組時 Python 會依次在 sys.path 中的路徑下尋找對應的模組文件。\n",
    "sys.path.append(\"../ecg-classification/\")\n",
    "# sys.path.append(\"C:\\\\Users\\\\Chen_Lab01\\\\Documents\\\\GitHub/ecg-classification\")\n",
    "# from IPython.display import Video\n",
    "# import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(\"ggplot\") #  是 Matplotlib 庫中用於設置繪圖樣式的函數。它使用了一種名為 \"ggplot\" 的預定義樣式\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "                        #  該樣式模仿了 R 語言的 ggplot2 繪圖包的外觀。\n",
    "# print(sys.path)\n",
    "import torch\n",
    "from ecg_tools.config import EcgConfig, Mode\n",
    "from ecg_tools.data_loader import DatasetConfig, get_data_loaders\n",
    "from ecg_tools.model import ECGformer\n",
    "from ecg_tools.train import ECGClassifierTrainer\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize the model",
   "id": "47dab4244e5f9743"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load",
   "id": "3a881f2b807408c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the model",
   "id": "ab9000cbc621d3da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:53.035683Z",
     "start_time": "2024-05-04T02:38:52.821604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"..\\\\..\\\\model_save\\\\model_epoch_98.pth\")\n",
    "model.eval()\n",
    "model.to('cpu')"
   ],
   "id": "65a7293516b0cca3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECGformer(\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (0): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (queries_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (values_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (keys_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (final_projection): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (0): Reduce('b n e -> b e', 'mean')\n",
       "    (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=192, out_features=6, bias=True)\n",
       "  )\n",
       "  (embedding): LinearEmbedding(\n",
       "    (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "    (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load quantized model",
   "id": "a131bd3c8ecabac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:52.805669Z",
     "start_time": "2024-05-04T02:38:52.746180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = EcgConfig()    \n",
    "model_quantized = torch.load(\"..\\\\..\\\\model_save\\\\model_quantized_98_torch.pth\")\n",
    "model_quantized.eval()\n",
    "model_quantized.to('cpu')"
   ],
   "id": "97ef5e082b44725f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xaio\\anaconda3\\envs\\pytorch-ecg\\lib\\site-packages\\torch\\_utils.py:382: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ECGformer(\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (0): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (queries_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (values_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (keys_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (final_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (block): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MLP(\n",
       "            (0): DynamicQuantizedLinear(in_features=192, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): DynamicQuantizedLinear(in_features=768, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (0): Reduce('b n e -> b e', 'mean')\n",
       "    (1): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): DynamicQuantizedLinear(in_features=192, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       "  (embedding): LinearEmbedding(\n",
       "    (0): DynamicQuantizedLinear(in_features=1, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "print model",
   "id": "d21832b978b1967c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TorchInfo summary",
   "id": "8844fda5bfe38d64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:53.051076Z",
     "start_time": "2024-05-04T02:38:53.036760Z"
    }
   },
   "cell_type": "code",
   "source": "print(model_quantized)",
   "id": "c3ad1d2037868cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGformer(\n",
      "  (encoder): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (0): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): MultiHeadAttention(\n",
      "            (queries_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (values_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (keys_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (final_projection): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualAdd(\n",
      "        (block): Sequential(\n",
      "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): MLP(\n",
      "            (0): DynamicQuantizedLinear(in_features=192, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): DynamicQuantizedLinear(in_features=768, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (0): Reduce('b n e -> b e', 'mean')\n",
      "    (1): DynamicQuantizedLinear(in_features=192, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): DynamicQuantizedLinear(in_features=192, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (embedding): LinearEmbedding(\n",
      "    (0): DynamicQuantizedLinear(in_features=1, out_features=192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "model summary",
   "id": "bb2b5f7aa602013b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T05:29:12.314991Z",
     "start_time": "2024-05-04T05:29:12.187426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "# input = (batch_size, signal_length, input_channels)\n",
    "summary(model_quantized, input_size=(1, config.model.signal_length, 1),\n",
    "       device='cpu',\n",
    "       col_names=(\"input_size\", \"output_size\"),\n",
    "       depth=6) # depth = 3 表示只顯示到第三層"
   ],
   "id": "616e6e444a6c674d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape\n",
       "====================================================================================================\n",
       "ECGformer                                          [1, 187, 1]               [1, 6]\n",
       "├─LinearEmbedding: 1-1                             [1, 187, 1]               [1, 188, 192]\n",
       "│    └─Linear: 2-1                                 [1, 187, 1]               [1, 187, 192]\n",
       "│    └─LayerNorm: 2-2                              [1, 187, 192]             [1, 187, 192]\n",
       "│    └─GELU: 2-3                                   [1, 187, 192]             [1, 187, 192]\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─TransformerEncoderLayer: 2-4                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-1                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-1                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-1               [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MultiHeadAttention: 5-2      [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-1             [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-2             [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-3             [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-4             [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-3                 [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-2                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-2                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-4               [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MLP: 5-5                     [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-5             [1, 188, 192]             [1, 188, 768]\n",
       "│    │    │    │    │    └─GELU: 6-6               [1, 188, 768]             [1, 188, 768]\n",
       "│    │    │    │    │    └─Linear: 6-7             [1, 188, 768]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-6                 [1, 188, 192]             [1, 188, 192]\n",
       "│    └─TransformerEncoderLayer: 2-5                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-3                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-3                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-7               [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MultiHeadAttention: 5-8      [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-8             [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-9             [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-10            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-11            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-9                 [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-4                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-4                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-10              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MLP: 5-11                    [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-12            [1, 188, 192]             [1, 188, 768]\n",
       "│    │    │    │    │    └─GELU: 6-13              [1, 188, 768]             [1, 188, 768]\n",
       "│    │    │    │    │    └─Linear: 6-14            [1, 188, 768]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-12                [1, 188, 192]             [1, 188, 192]\n",
       "│    └─TransformerEncoderLayer: 2-6                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-5                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-5                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-13              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MultiHeadAttention: 5-14     [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-15            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-16            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-17            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-18            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-15                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-6                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-6                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-16              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MLP: 5-17                    [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-19            [1, 188, 192]             [1, 188, 768]\n",
       "│    │    │    │    │    └─GELU: 6-20              [1, 188, 768]             [1, 188, 768]\n",
       "│    │    │    │    │    └─Linear: 6-21            [1, 188, 768]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-18                [1, 188, 192]             [1, 188, 192]\n",
       "│    └─TransformerEncoderLayer: 2-7                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-7                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-7                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-19              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MultiHeadAttention: 5-20     [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-22            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-23            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-24            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-25            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-21                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-8                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-8                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-22              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MLP: 5-23                    [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-26            [1, 188, 192]             [1, 188, 768]\n",
       "│    │    │    │    │    └─GELU: 6-27              [1, 188, 768]             [1, 188, 768]\n",
       "│    │    │    │    │    └─Linear: 6-28            [1, 188, 768]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-24                [1, 188, 192]             [1, 188, 192]\n",
       "│    └─TransformerEncoderLayer: 2-8                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-9                       [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-9                   [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-25              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MultiHeadAttention: 5-26     [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-29            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-30            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-31            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-32            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-27                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-10                      [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-10                  [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-28              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MLP: 5-29                    [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-33            [1, 188, 192]             [1, 188, 768]\n",
       "│    │    │    │    │    └─GELU: 6-34              [1, 188, 768]             [1, 188, 768]\n",
       "│    │    │    │    │    └─Linear: 6-35            [1, 188, 768]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-30                [1, 188, 192]             [1, 188, 192]\n",
       "│    └─TransformerEncoderLayer: 2-9                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-11                      [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-11                  [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-31              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MultiHeadAttention: 5-32     [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-36            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-37            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-38            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-39            [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-33                [1, 188, 192]             [1, 188, 192]\n",
       "│    │    └─ResidualAdd: 3-12                      [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    └─Sequential: 4-12                  [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─LayerNorm: 5-34              [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    └─MLP: 5-35                    [1, 188, 192]             [1, 188, 192]\n",
       "│    │    │    │    │    └─Linear: 6-40            [1, 188, 192]             [1, 188, 768]\n",
       "│    │    │    │    │    └─GELU: 6-41              [1, 188, 768]             [1, 188, 768]\n",
       "│    │    │    │    │    └─Linear: 6-42            [1, 188, 768]             [1, 188, 192]\n",
       "│    │    │    │    └─Dropout: 5-36                [1, 188, 192]             [1, 188, 192]\n",
       "├─Classifier: 1-3                                  [1, 188, 192]             [1, 6]\n",
       "│    └─Reduce: 2-10                                [1, 188, 192]             [1, 192]\n",
       "│    └─Linear: 2-11                                [1, 192]                  [1, 192]\n",
       "│    └─LayerNorm: 2-12                             [1, 192]                  [1, 192]\n",
       "│    └─Linear: 2-13                                [1, 192]                  [1, 6]\n",
       "====================================================================================================\n",
       "Total params: 41,664\n",
       "Trainable params: 41,664\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 3.75\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 3.78\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neutron 可視化",
   "id": "7c8315ac48f63a4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ONNX 檔案\n",
    "Neutron 可視化最齊全的是 ONNX 格式，但是不支援 quantized model，這裡使用原始 model"
   ],
   "id": "de93834caff6d353"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:54.032236Z",
     "start_time": "2024-05-04T02:38:53.267642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_names = [\"ECG Classification Input\"]\n",
    "output_names = [\"Prediction\"]\n",
    "X = torch.randn(1, 187, 1)\n",
    "torch.onnx.export(model, X, \"model.onnx\", input_names=input_names, output_names=output_names)"
   ],
   "id": "82433c541eabc31f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TorchScript 檔案",
   "id": "c01e28d852e6189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:55.745965Z",
     "start_time": "2024-05-04T02:38:54.033302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.randn(1, 187, 1)\n",
    "traced_script_module = torch.jit.trace(model_quantized, inputs)\n",
    "traced_script_module.save(\"traced_resnet_model.pth\")\n",
    "\n",
    "import netron\n",
    "modelData = 'traced_resnet_model.pth'\n",
    "netron.start(modelData)"
   ],
   "id": "28936bdc1d18e69e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'traced_resnet_model.pth' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grapviz 可視化",
   "id": "b424843e36b94c23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:38:56.590436Z",
     "start_time": "2024-05-04T02:38:55.747530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchviz import make_dot\n",
    "vis_graph = make_dot(model_quantized(inputs), params=dict(model_quantized.named_parameters()), show_attrs=True, show_saved=True)\n",
    "vis_graph.view()  # 當前目錄保存為 pdf 文件"
   ],
   "id": "6541dab9a16979c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
