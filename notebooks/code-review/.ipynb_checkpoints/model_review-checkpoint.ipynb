{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "mount_file_id": "1kC5OaXAzH5gPIp_YH4RjewVYJl1Q6OlK",
   "authorship_tag": "ABX9TyPyR551RQXU/DgDzdhuOCRg"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Model Reveiew",
   "metadata": {
    "id": "7BHqO2iOEDn-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from math import sqrt\n",
    "\n",
    "import einops\n",
    "from einops.layers.torch import Reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtcvQe1JzOiC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712836267688,
     "user_tz": -480,
     "elapsed": 20916,
     "user": {
      "displayName": "Xaio",
      "userId": "03940136356471857780"
     }
    },
    "outputId": "14c7c985-9ea5-43a9-e52d-3cc116181bc0",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-30T14:38:26.368965Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic PyTorch"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Dropout \n",
    "Each forward pass will zero out some of the elements of the input tensor with probability p. And it will scale the remaining elements by $\\frac{1}{1-p}$."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:14:02.780522Z",
     "start_time": "2024-04-29T19:14:02.776010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = 0.5\n",
    "\n",
    "module = nn.Dropout(p=p)\n",
    "module.training\n",
    "inp = torch.ones(3,5)\n",
    "print(f'scale: {1/(1-p)}')\n",
    "print(f'before dropout:\\n{inp}')\n",
    "print(f'after droput:\\n{module(inp)}')\n",
    "# module(inp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: 2.0\n",
      "before dropout:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "after droput:\n",
      "tensor([[2., 2., 2., 0., 2.],\n",
      "        [0., 0., 0., 2., 0.],\n",
      "        [0., 2., 0., 2., 0.]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "使用 random 實作 dropout"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T11:24:23.257727Z",
     "start_time": "2024-04-26T11:24:23.253726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# 將 5 成的資料設成 0\n",
    "dropout_rate = 0.5\n",
    "# Example output containing 10 values\n",
    "example_output = [0.27, -1.03, 0.67, 0.99, 0.05, \n",
    "                  -0.37, -2.01, 1.13, -0.07, 0.73]\n",
    "\n",
    "# Repeat as long as necessary \n",
    "while True:\n",
    "    # Randomly choose index and set value to 0\n",
    "    index = random.randint(0, len(example_output) - 1)\n",
    "    example_output[index] = 0\n",
    "    \n",
    "    # Count values that are exactly 0\n",
    "    dropped_out = 0\n",
    "    for value in example_output:\n",
    "        if value == 0:\n",
    "            dropped_out += 1\n",
    "    \n",
    "    # If required number of outputs is zeroed - leave the loop        \n",
    "    if dropped_out / len(example_output) >= dropout_rate:\n",
    "        break\n",
    "\n",
    "print(example_output)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27, 0, 0.67, 0.99, 0.05, 0, 0, 0, -0.07, 0]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Linear\n",
    "轉換輸入與輸出之間的維度關係，x 是輸入，y是輸出\n",
    "$$\n",
    "y = xA^T + b\n",
    "$$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T11:24:25.656115Z",
     "start_time": "2024-04-26T11:24:25.645113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 將輸入 dimension 10 轉換成 output dimension 20\n",
    "module = nn.Linear(in_features=10, out_features=20)\n",
    "print(f'module:{module}') \n",
    "\n",
    "n_samples = 40\n",
    "# 最後面的 10 是輸入的維度，前面是指定的 batch size\n",
    "inp_2d = torch.randn(n_samples, 10) \n",
    "inp_3d = torch.randn(n_samples, 33, 10)\n",
    "inp_5d = torch.randn(n_samples, 2, 3, 4, 5, 10)\n",
    "# inp_5d_false = torch.randn(n_samples, 2, 3, 4, 5, 20)\n",
    "\n",
    "print(f'module(inp_2d).shape: {module(inp_2d).shape}')\n",
    "print(f'module(inp_3d).shape: {module(inp_3d).shape}')\n",
    "print(f'module(inp_5d).shape: {module(inp_5d).shape}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module:Linear(in_features=10, out_features=20, bias=True)\n",
      "module(inp_2d).shape: torch.Size([40, 20])\n",
      "module(inp_3d).shape: torch.Size([40, 33, 20])\n",
      "module(inp_5d).shape: torch.Size([40, 2, 3, 4, 5, 20])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T09:00:34.411717Z",
     "start_time": "2024-04-26T09:00:34.390962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 測試 nn.Linear(embed_size, embed_size) 的作用\n",
    "# y = xA^T + b\n",
    "embed_size = 64\n",
    "linear = nn.Linear(embed_size, embed_size)\n",
    "# 產生隨機的 x.shap==3 的資料，其中第一維是 batch size，第二維是序列長度，第三維是嵌入維度\n",
    "x = torch.randn(32, 10, embed_size)\n",
    "print(f\"x: {x[0][0][0]}\")\n",
    "print(f\"linear(x)[0][0][0]: {linear(x)[0][0][0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -0.856232762336731\n",
      "linear(x)[0][0][0]: -0.37569767236709595\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LayerNorm\n",
    "LayerNorm 會對輸入進行標準化，使得輸入的均值為 0，方差為 1。這有助於模型訓練，因為它可以使不同特徵的數值範圍保持一致，從而更容易學習權重。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:16:08.473833Z",
     "start_time": "2024-04-21T08:16:08.462354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 輸入特徵是一維資料\n",
    "inp_features = torch.tensor([0.5, 0.3, 0.7, 0.2, 0.6])\n",
    "\n",
    "# print the input and the mean and variance\n",
    "mean_inp = torch.mean(inp_features)\n",
    "variance_inp = torch.var(inp_features, unbiased=False)\n",
    "print(f'input: {inp_features}')\n",
    "print(f'before LayerNorm mean: {mean_inp}, variance: {variance_inp}')\n",
    "\n",
    "# LayerNorm\n",
    "layer_norm = nn.LayerNorm([5]) # Layer Normalization 填入特徵的維度\n",
    "output = layer_norm(inp_features)\n",
    "\n",
    "# print the output and the mean and variance\n",
    "mean_opt = torch.mean(output)\n",
    "variance_opt = torch.var(output, unbiased=False)\n",
    "print(output)\n",
    "print(f'after LayerNorm mean: {mean_opt}, variance: {variance_opt}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([0.5000, 0.3000, 0.7000, 0.2000, 0.6000])\n",
      "before LayerNorm mean: 0.46000003814697266, variance: 0.03439999744296074\n",
      "tensor([ 0.2156, -0.8625,  1.2938, -1.4016,  0.7547],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "after LayerNorm mean: 0.0, variance: 0.9997094869613647\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding + Input Embedding\n",
    "\n",
    "### Code 簡介\n",
    "`cls_token = nn.Parameter(torch.randn(1, output_channels))`\n",
    "\n",
    "這行程式碼的意思是創建一個可學習的類別標記(class token),並將其定義為模型的參數。讓我們分解這行程式碼:\n",
    "\n",
    "- `torch.randn(1, output_channels)` 創建了一個形狀為 (1, output_channels) 的隨機張量。\n",
    "    - `1` 表示這個張量只有一個元素,即類別標記。\n",
    "    - `output_channels` 表示類別標記的維度,與嵌入的維度相同。\n",
    "    - `torch.randn` 函數從標準常態分佈中隨機取樣值來初始化張量。\n",
    "- `nn.Parameter(...)` 將這個隨機初始化的張量封裝為一個可學習的參數。\n",
    "    - 通過將張量傳遞給 `nn.Parameter`,我們告訴 PyTorch 這個張量是模型的一部分,需要在訓練過程中進行優化和更新。\n",
    "    - 在這種情況下,類別標記 `cls_token` 將作為模型的一個參數,在反向傳播期間根據梯度進行更新。\n",
    "`cls_token = ...` 將這個可學習的參數賦值給 `cls_token` 變數,以便在模型的前向傳播中使用。\n",
    "\n",
    "舉個例子,假設 output_channels 的值為 20,那麼 cls_token 將是一個形狀為 (1, 20) 的張量,表示一個 20 維的類別標記。在訓練過程中,這個類別標記將與嵌入的序列一起傳遞給模型,並與序列的其他部分一起進行優化。\n",
    "\n",
    "\n",
    "`torch.cat([einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0]), embedded], dim=1)`\n",
    "\n",
    "這行程式碼的目的是將類別標記 (`cls_token`) 重複 `x.shape[0]` 次,並將其與嵌入的序列 (`embedded`) 在第二個維度 (dim=1) 上進行串聯。\n",
    "\n",
    "1. `einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0])`:\n",
    "    \n",
    "    - `self.cls_token` 是形狀為 `(1, output_channels)` 的類別標記張量。\n",
    "        \n",
    "    - `einops.repeat` 是一個函數,用於重塑和重複張量。它使用一種特殊的表示法來指定輸入和輸出的維度。\n",
    "        \n",
    "    - `\"n e -> b n e\"` 是重塑的表示法,其中:\n",
    "        \n",
    "        - `n` 表示類別標記的數量,這裡是 1。\n",
    "            \n",
    "        - `e` 表示類別標記的維度,即 `output_channels`。\n",
    "            \n",
    "        - `b` 表示批次大小,即 `x.shape[0]`。\n",
    "            \n",
    "    - `b=x.shape[0]` 指定了重複的次數,即批次大小。\n",
    "        \n",
    "    - 這行程式碼的作用是將類別標記 `cls_token` 重複 `x.shape[0]` 次,使其與批次中的每個樣本相對應。\n",
    "        \n",
    "2. `torch.cat([einops.repeat(...), embedded], dim=1)`:\n",
    "    \n",
    "    - `torch.cat` 是一個函數,用於在指定維度上串聯張量。\n",
    "        \n",
    "    - 這裡,我們將重複後的類別標記張量 `einops.repeat(...)` 和嵌入的序列張量 `embedded` 在第二個維度 (dim=1) 上進行串聯。\n",
    "        \n",
    "    - 串聯後的結果將是一個新的張量,形狀為 `(batch_size, sequence_length + 1, output_channels)`,其中第二維的長度增加了 1,**因為我們在序列的開頭添加了類別標記**。\n",
    "\n",
    "### Positional Encoding 的特色\n",
    "\n",
    "因為Transformer中沒有Conv跟Recurrent, 沒有東西可以表示token在序列中的\"相對位置或是絕對位置\"，也就是說Attention機制沒有考慮\"順序\" ，所以需要Positional Encoding。因此他在encoder和decoder最底層的input embedding加上positional encodings來表示位置\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:35:56.286339Z",
     "start_time": "2024-04-30T14:35:56.271299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearEmbedding(nn.Sequential):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.LayerNorm(output_channels),\n",
    "            nn.GELU()\n",
    "        ])\n",
    "        # 創建一個可學習的類別標記(class token)，因為輸入的 channel 是一維的，並將其定義為模型的參數(nn.Parameter 特有的功能)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, output_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 直接使用上面創建的 Linear -> LayerNorm -> GELU()\n",
    "        embedded = super().forward(x)\n",
    "        return torch.cat([einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0]), embedded], dim=1)\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LayerNorm\n",
    "\n",
    "$\\gamma,\\ \\beta$ 為可學習的參數，$\\epsilon$ 為避免梯度消失而添加\n",
    "\n",
    "$$\n",
    "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2,\\quad \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\n",
    "$$\n",
    "\n",
    "$\\epsilon =  1\\times10^-5$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 電路執行\n",
    "\n",
    "1. 計算輸入陣列的平均值\n",
    "2. 計算標準差\n",
    "3. 對每個值減去平均值，然後除標準差，不需要加上 1e-5"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:13:34.253699Z",
     "start_time": "2024-04-30T14:13:34.233774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "signal = np.array([1.00E+00,7.58E-01,1.12E-01,0.00E+00,8.06E-02,7.85E-02,6.61E-02,4.96E-02])\n",
    "\n",
    "# avg =  np.mean(signal)\n",
    "sum_avg = 0\n",
    "for i in range(len(signal)):\n",
    "    sum_avg += signal[i]\n",
    "avg = sum_avg / len(signal)\n",
    "\n",
    "# var = np.var(signal)\n",
    "sum_var = 0\n",
    "for i in range(len(signal)):\n",
    "    sum_var += (signal[i] - avg) ** 2\n",
    "var =  sum_var / len(signal)\n",
    "\n",
    "layer_norm = (signal - avg) / np.sqrt(var + 1e-5)\n",
    "print(layer_norm)\n",
    "\n",
    "signal_torch = torch.tensor(signal)\n",
    "layer_norm_torch = nn.LayerNorm(signal_torch.size())\n",
    "print(layer_norm_torch(signal_torch.float()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.03811871  1.36422237 -0.43469098 -0.74657689 -0.52213043 -0.52797829\n",
      " -0.56250851 -0.60845599]\n",
      "tensor([ 2.0381,  1.3642, -0.4347, -0.7466, -0.5221, -0.5280, -0.5625, -0.6085],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### cls_token 是甚麼?"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:36:59.044582Z",
     "start_time": "2024-04-30T14:36:59.033168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cls_token 是甚麼?\n",
    "import torch.nn as nn \n",
    "# 創建 1 個類別標記，維度為 10，也就是 [1, 10]\n",
    "# 創建 4 個類別標記，維度為 10，也就是 [4, 10]\n",
    "\n",
    "cls_token = nn.Parameter(torch.randn(1, 192))\n",
    "print(cls_token.size())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:36:05.400051Z",
     "start_time": "2024-04-30T14:36:05.337683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 測試 Linearembedding 的輸入與輸出\n",
    "input_channels = 1\n",
    "output_channels = 192\n",
    "batch_size = 1\n",
    "sequence_length = 5\n",
    "x = torch.randn(batch_size, sequence_length, input_channels)\n",
    "\n",
    "# 第一維 4 表示批次大小。\n",
    "# 第二維 6 表示串聯後的序列長度,其中包括 5 個原始序列元素和 1 個添加的類別標記。\n",
    "# 第三維 20 表示嵌入的維度。\n",
    "embedding = LinearEmbedding(input_channels, output_channels)\n",
    "output = embedding(x)\n",
    "print(f\"LinearEmbedding:{embedding}\")\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"output.shape: {output.shape}\")\n",
    "# 5->6 因為增加一個類別標記\n",
    "# 1->192，因為 Linear embedding 的 output channel 設定 192"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearEmbedding:LinearEmbedding(\n",
      "  (0): Linear(in_features=1, out_features=192, bias=True)\n",
      "  (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): GELU(approximate='none')\n",
      ")\n",
      "x.shape: torch.Size([1, 5, 1])\n",
      "output.shape: torch.Size([1, 6, 192])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Residual connection"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "class ResidualAdd(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`ResuidalAdd`"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 6, 9], dtype=torch.int8)\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "import torch\n",
    "\n",
    "# 定義一個簡單的神經網絡模塊，將輸入乘以2\n",
    "class DoubleBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# 使用ResidualAdd類來創建一個帶殘差連接的模塊\n",
    "residual_block = ResidualAdd(DoubleBlock())\n",
    "\n",
    "# 創建一個輸入張量\n",
    "input_tensor = torch.tensor([1, 2, 3], dtype=torch.int8)\n",
    "\n",
    "# 通過殘差模塊傳遞輸入\n",
    "output = residual_block(input_tensor)\n",
    "\n",
    "# 輸入 [1,2,3] 經過 DoubleBlock 得到 [2,4,6]，再加上輸入得到 [3,6,9]\n",
    "# 此例子充分展示 Residual add 的功能，通過 x + self.block(x) 進行殘差連接\n",
    "# x 是原本的輸入 tensor，self.block(x) 是經過模塊處理後的輸出\n",
    "print(output)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Head-Attention\n",
    "\n",
    "### 簡單說明 Multi-Head-Attention 用到的變數\n",
    "這段代碼定義了一個名為 MultiHeadAttention 的 PyTorch 模組類別,用於實現多頭注意力機制。讓我們逐步解釋這段代碼:\n",
    "\n",
    "1. `__init__` 方法初始化了模組的參數,包括三個線性投影層 (queries_projection, values_projection, keys_projection) 用於將輸入張量投影到 queries、keys 和 values 的空間中。另一個線性投影層 final_projection 用於最終的輸出。此外,還設置了 embed_size 和 num_heads 參數。\n",
    "2. `forward` 方法定義了模組的前向傳播行為。它首先檢查輸入張量 x 的維度是否為 3 (批次大小、序列長度、嵌入維度)。\n",
    "接下來,輸入張量 x 被投影到 keys、values 和 queries 的空間中。\n",
    "3. 使用 `einops` 庫,keys、values 和 queries 被重新排列為多頭表示,其中每個頭對應一個特定的子空間。\n",
    "4. 計算 `queries` 和 keys 之間的點積,得到 energy_term。\n",
    "5. `energy_term` 被縮放以防止極端的 softmax 值,然後經過 softmax 運算得到 mh_out。\n",
    "6. 使用 `torch.einsum` 計算加權和,將 mh_out 與 values 相乘並求和,得到每個頭的輸出。\n",
    "7. 所有頭的輸出被連接起來,然後通過 `final_projection` 層得到最終的輸出張量。\n",
    "\n",
    "總的來說,這個模組實現了標準的多頭注意力機制,將輸入序列映射到一組注意力加權的表示,捕獲了不同子空間中的重要信息。這種注意力機制被廣泛應用於諸如 Transformer 等自然語言處理模型中。\n",
    "\n",
    "### Attention 的運作原理\n",
    "\n",
    "#### Attention 簡介\n",
    "- 注意力function做的事情可被描述為mapping一個query和一群key-value的pairs到輸出 (Q, K, V, Output都是vector)\n",
    "- Output is computed as a weighted sum of the values，values的weight則是由query和其相對應的key所算出\n",
    "- Q,K,V 在文字翻譯上而言， Q是在找哪個字的key vector可能會貢獻我的語意最多 , K是這個字可以貢獻給哪個字最多語意, V是最後的輸出，也就是這個字的語意是什麼。\n",
    "- 換句話說就是\n",
    "    - Q : to match other\n",
    "    - K : to be matched\n",
    "    - V : information to be extracted\n",
    "    - Attention: 吃兩個向量，輸出一個分數來代表這兩個向量有多匹配、多相關\n",
    "- Self Attention: 拿每個q去對每個k做Attention (Scaled Dot-product attention)\n",
    "\n",
    "\n",
    "#### Transformer 的 Scaled Dot-Product Attention\n",
    "一樣是做 Q 和 K 的內積，內積以矩陣乘法表示的話就是 $QK^{T}$，但是 Q, K 都是 $d_k$ 維度的輸入，也就是 embedded size。內積完的結果除 $\\sqrt{d_k}$，再送入 softmax 就是 attention 的結果。**需要注意的是 Q的計算和這個 attention 的計算是同時進行的(硬體 Pipeline 優化的方向)** \n",
    "\n",
    "> 研究員懷疑 $d_k$ 很大的時候dot product的規模太大，導致softmax後的數值太小 (extremely small gradients), 才會加開根號緩解\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "### Multi-Head Attention : Transformer 最重要的機制\n",
    "![](https://imgur-backup.hackmd.io/4xMlLna.png)\n",
    "- 很多個 self-attention concat 後所得出的結果\n",
    "- Multi-head attention 用來計算相對全部的字而言當前這個字能給予多少的資訊量 (讓每個head都能學到不同feature的特徵)\n",
    "- Transformer 原文使用 8 個 heads，每個 head 的維度是 64，也就是 embedded size。但是每個 head 都有降維，實際上的計算不會和 single-head 差太多。\n",
    "\n",
    "> Self-attention layer in Encoder: 在這一層中 所有的key, values, queries都是從同一個地方來的, 他們都是從前一層的encoder的output來的\n",
    "\n",
    "### FFN (Feed Forward Network) MLP\n",
    "也就是在 `model.py` 中的 MLP，FFN 通常由兩個線性變換(Linear)和一個 ReLU(GeLU) 組成，層和層之間(不同的head之間)是用不同的參數丟入FFN之中。\n",
    "\n",
    "> FFN 可看成kernel size=1的2個 Conv\n",
    "\n",
    "\n",
    "### Residual Connection\n",
    "\n",
    "![](https://imgur-backup.hackmd.io/JBqsUsH.png)\n",
    "\n",
    "`TransformerEncoderLayer` 描述的就是 encoder 的灰色方框，由Multi-head self-attention 和 FC Feed Forward所組成\n",
    "，其中每兩個 sublayers 之間用Residual connection連接。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, attention_store=None):\n",
    "        super().__init__()\n",
    "        # 通過 nn.Linear 相當於做通過全連接層\n",
    "        self.queries_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.values_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.keys_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.final_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 3\n",
    "        keys = self.keys_projection(x)\n",
    "        values = self.values_projection(x)\n",
    "        queries = self.queries_projection(x)\n",
    "        keys = einops.rearrange(keys, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        queries = einops.rearrange(queries, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        values = einops.rearrange(values, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        # q, k 做 dot-product -> self-attention\n",
    "        energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)\n",
    "        divider = sqrt(self.embed_size)\n",
    "        mh_out = torch.softmax(energy_term, -1)\n",
    "        out = torch.einsum('bihv, bvhd -> bihd ', mh_out / divider, values)\n",
    "        out = einops.rearrange(out, \"b n h e -> b n (h e)\")\n",
    "        return self.final_projection(out)\n",
    "    \n",
    "\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_channels, expansion=4):\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, input_channels * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_channels * expansion, input_channels)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "class TransformerEncoderLayer(torch.nn.Sequential):\n",
    "    def __init__(self, embed_size=768, expansion=4, num_heads=8, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__(\n",
    "            *[\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MultiHeadAttention(embed_size, num_heads),\n",
    "                    nn.Dropout(dropout)\n",
    "                ])),\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MLP(embed_size, expansion),\n",
    "                    nn.Dropout(dropout)\n",
    "                ]))\n",
    "            ]\n",
    "        )"
   ],
   "metadata": {
    "id": "s-06i9LCFWPW",
    "ExecuteTime": {
     "end_time": "2024-04-26T08:48:03.772942Z",
     "start_time": "2024-04-26T08:48:03.756433Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`assert`\n",
    "\n",
    "如果 x 的維度數量不是 3，那麼 assert len(x.shape) == 3 會引發一個 AssertionError 異常，程式會在這裡停止執行。用於在程式執行過程中檢查某些條件是否滿足，如果不滿足則提前終止程式，以避免產生不可預期的錯誤。\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:50:56.275667Z",
     "start_time": "2024-04-26T14:50:55.766466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_discount(price, discount):\n",
    "    # 断言价格必须大于 0\n",
    "    assert price > 0, \"Price must be greater than 0\"\n",
    "    # 断言折扣必须在 0 到 1 之间\n",
    "    assert 0 <= discount <= 1, \"Discount must be between 0 and 1\"\n",
    "    return price * (1 - discount)\n",
    "\n",
    "# 正常情况，不会抛出异常\n",
    "print(apply_discount(100, 0.1))\n",
    "\n",
    "# 价格为负，会抛出异常\n",
    "print(apply_discount(-100, 0.1))\n",
    "\n",
    "# 折扣超过 1，会抛出异常\n",
    "print(apply_discount(100, 1.1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Price must be greater than 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(apply_discount(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m0.1\u001B[39m))\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 价格为负，会抛出异常\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mapply_discount\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# 折扣超过 1，会抛出异常\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(apply_discount(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m1.1\u001B[39m))\n",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m, in \u001B[0;36mapply_discount\u001B[1;34m(price, discount)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_discount\u001B[39m(price, discount):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;66;03m# 断言价格必须大于 0\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m price \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice must be greater than 0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# 断言折扣必须在 0 到 1 之间\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m discount \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscount must be between 0 and 1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Price must be greater than 0"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`einops.rearrange` \n",
    "\n",
    "- b 表示批次大小。\n",
    "- n 表示序列長度。\n",
    "- h 表示頭的數量，這裡是 self.num_heads。\n",
    "- e 表示每個頭的嵌入維度，這裡是 embed_size / num_heads。\n",
    "\n",
    "將輸入張量 keys 的形狀從 (batch_size, sequence_length, embed_size) 變為 (batch_size, sequence_length, num_heads, embed_size / num_heads)。這樣做的目的是將嵌入維度 embed_size 分割成 num_heads 個子空間，每個子空間的維度是 embed_size / num_heads。舉例來說，假設我們有一個形狀為 (32, 10, 64) 的張量，num_heads 為 8。那麼這行程式碼將會將這個張量的形狀變為 (32, 10, 8, 8)。這意味著我們將 **64 維的嵌入空間分割成了 8 個 8 維的子空間，每個子空間對應一個頭**。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:43:34.451499Z",
     "start_time": "2024-04-26T14:43:31.521654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "# 假設我們有一個形狀為 (2, 10, 64) 的張量\n",
    "x = torch.randn(2, 10, 64)\n",
    "\n",
    "# 我們希望將最後一個維度分割成 8 個子空間，每個子空間的維度為 8\n",
    "num_heads = 8\n",
    "\n",
    "# 使用 einops.rearrange 進行重塑，(he) = h * e\n",
    "x_rearranged = einops.rearrange(x, \"b n (h e) -> b n h e\", h=num_heads)\n",
    "\n",
    "print(x_rearranged.shape)  # 輸出：torch.Size([2, 10, 8, 8])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8, 8])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)`\n",
    "\n",
    "The line of code `energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)` is using the `torch.einsum` function, which stands for **Einstein summation**. This function provides a concise way to perform operations on multidimensional arrays.\n",
    "\n",
    "In this case, it's being used to calculate the dot product between the `queries` and `keys` tensors in the multi-head attention mechanism of a Transformer model. The string `\"bqhe, bkhe -> bqhk\"` is an equation that describes the operation to be performed.\n",
    "\n",
    "Here's what each symbol means:\n",
    "\n",
    "- `b` represents the batch size.\n",
    "- `q` and `k` represent the sequence length, but they are different dimensions because one is for queries and the other is for keys.\n",
    "- `h` represents the number of attention heads.\n",
    "- `e` represents the embedding size.\n",
    "\n",
    "**The equation `\"bqhe, bkhe -> bqhk\"` means that for each batch (`b`), for each position in the sequence of queries (`q`) and keys (`k`), for each attention head (`h`), calculate the dot product of the embeddings (`e`).** The result is a tensor that contains the compatibility score of each query with each key, for each attention head in each batch. This score measures how much attention should be paid to each key when processing each query.數學式表示成以下，q 的每一項與 k 的每一項做點積\n",
    "\n",
    "`output[i, j, k, l] = sum(queries[i, j, k, :] * keys[i, l, k, :])`\n",
    "\n",
    "The output tensor `energy_term` has the shape `(batch_size, sequence_length, sequence_length, num_heads)`, where each element is the compatibility score of a query with a key. This tensor is used in the subsequent softmax operation to obtain the attention weights.\n",
    "\n",
    "[更多 einusm 的用法和 C++ 作法](!https://zhuanlan.zhihu.com/p/361209187)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:01:10.436660Z",
     "start_time": "2024-04-26T15:01:10.427686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 假設我們有一個批次大小為 32，序列長度為 10，頭數為 8，嵌入維度為 64 的模型\n",
    "batch_size = 32\n",
    "seq_length = 10\n",
    "num_heads = 8\n",
    "embed_size = 64\n",
    "\n",
    "# 創建隨機張量 queries 和 keys\n",
    "queries = torch.randn(batch_size, seq_length, num_heads, embed_size)\n",
    "keys = torch.randn(batch_size, seq_length, num_heads, embed_size)\n",
    "\n",
    "# 使用 torch.einsum 計算 queries 和 keys 的點積\n",
    "energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)\n",
    "\n",
    "print(f\"queries.shape: {queries.shape}\")  # 輸出：torch.Size([32, 10, 8, 64]\n",
    "print(f\"keys.shape: {keys.shape}\")  # 輸出：torch.Size([32, 10, 8, 8])\n",
    "print(f\"energy_term.shape: {energy_term.shape}\")  # 輸出：torch.Size([32, 10, 10, 8])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape: torch.Size([32, 10, 8, 64])\n",
      "keys.shape: torch.Size([32, 10, 8, 64])\n",
      "energy_term.shape: torch.Size([32, 10, 8, 10])\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ]
}
