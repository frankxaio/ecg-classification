{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "mount_file_id": "1kC5OaXAzH5gPIp_YH4RjewVYJl1Q6OlK",
   "authorship_tag": "ABX9TyPyR551RQXU/DgDzdhuOCRg"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Model Reveiew",
   "metadata": {
    "id": "7BHqO2iOEDn-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from math import sqrt\n",
    "\n",
    "import einops\n",
    "from einops.layers.torch import Reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtcvQe1JzOiC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712836267688,
     "user_tz": -480,
     "elapsed": 20916,
     "user": {
      "displayName": "Xaio",
      "userId": "03940136356471857780"
     }
    },
    "outputId": "14c7c985-9ea5-43a9-e52d-3cc116181bc0",
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:44.663569Z",
     "start_time": "2024-05-01T12:52:44.653511Z"
    }
   },
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic PyTorch"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Dropout \n",
    "Each forward pass will zero out some of the elements of the input tensor with probability p. And it will scale the remaining elements by $\\frac{1}{1-p}$."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:44.803378Z",
     "start_time": "2024-05-01T12:52:44.794366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = 0.5\n",
    "\n",
    "module = nn.Dropout(p=p)\n",
    "module.training\n",
    "inp = torch.ones(3,5)\n",
    "print(f'scale: {1/(1-p)}')\n",
    "print(f'before dropout:\\n{inp}')\n",
    "print(f'after droput:\\n{module(inp)}')\n",
    "# module(inp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: 2.0\n",
      "before dropout:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "after droput:\n",
      "tensor([[0., 2., 0., 2., 0.],\n",
      "        [0., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 2., 0.]])\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "使用 random 實作 dropout"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:44.835547Z",
     "start_time": "2024-05-01T12:52:44.823413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# 將 5 成的資料設成 0\n",
    "dropout_rate = 0.5\n",
    "# Example output containing 10 values\n",
    "example_output = [0.27, -1.03, 0.67, 0.99, 0.05, \n",
    "                  -0.37, -2.01, 1.13, -0.07, 0.73]\n",
    "\n",
    "# Repeat as long as necessary \n",
    "while True:\n",
    "    # Randomly choose index and set value to 0\n",
    "    index = random.randint(0, len(example_output) - 1)\n",
    "    example_output[index] = 0\n",
    "    \n",
    "    # Count values that are exactly 0\n",
    "    dropped_out = 0\n",
    "    for value in example_output:\n",
    "        if value == 0:\n",
    "            dropped_out += 1\n",
    "    \n",
    "    # If required number of outputs is zeroed - leave the loop        \n",
    "    if dropped_out / len(example_output) >= dropout_rate:\n",
    "        break\n",
    "\n",
    "print(example_output)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -1.03, 0, 0.99, 0, -0.37, 0, 1.13, 0, 0.73]\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Linear\n",
    "轉換輸入與輸出之間的維度關係，x 是輸入，y是輸出\n",
    "$$\n",
    "y = xA^T + b\n",
    "$$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:44.912769Z",
     "start_time": "2024-05-01T12:52:44.874129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 將輸入 dimension 10 轉換成 output dimension 20\n",
    "module = nn.Linear(in_features=10, out_features=20)\n",
    "print(f'module:{module}') \n",
    "\n",
    "n_samples = 40\n",
    "# 最後面的 10 是輸入的維度，前面是指定的 batch size\n",
    "inp_2d = torch.randn(n_samples, 10) \n",
    "inp_3d = torch.randn(n_samples, 33, 10)\n",
    "inp_5d = torch.randn(n_samples, 2, 3, 4, 5, 10)\n",
    "# inp_5d_false = torch.randn(n_samples, 2, 3, 4, 5, 20)\n",
    "\n",
    "print(f'module(inp_2d).shape: {module(inp_2d).shape}')\n",
    "print(f'module(inp_3d).shape: {module(inp_3d).shape}')\n",
    "print(f'module(inp_5d).shape: {module(inp_5d).shape}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module:Linear(in_features=10, out_features=20, bias=True)\n",
      "module(inp_2d).shape: torch.Size([40, 20])\n",
      "module(inp_3d).shape: torch.Size([40, 33, 20])\n",
      "module(inp_5d).shape: torch.Size([40, 2, 3, 4, 5, 20])\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:44.943830Z",
     "start_time": "2024-05-01T12:52:44.918772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 測試 nn.Linear(embed_size, embed_size) 的作用\n",
    "# y = xA^T + b\n",
    "embed_size = 64\n",
    "linear = nn.Linear(embed_size, embed_size)\n",
    "# 產生隨機的 x.shap==3 的資料，其中第一維是 batch size，第二維是序列長度，第三維是嵌入維度\n",
    "x = torch.randn(32, 10, embed_size)\n",
    "print(f\"x: {x[0][0][0]}\")\n",
    "print(f\"linear(x)[0][0][0]: {linear(x)[0][0][0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 0.3019552528858185\n",
      "linear(x)[0][0][0]: -0.3937355875968933\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LayerNorm\n",
    "LayerNorm 會對輸入進行標準化，使得輸入的均值為 0，方差為 1。這有助於模型訓練，因為它可以使不同特徵的數值範圍保持一致，從而更容易學習權重。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:44.990951Z",
     "start_time": "2024-05-01T12:52:44.969410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 輸入特徵是一維資料\n",
    "inp_features = torch.tensor([0.5, 0.3, 0.7, 0.2, 0.6])\n",
    "\n",
    "# print the input and the mean and variance\n",
    "mean_inp = torch.mean(inp_features)\n",
    "variance_inp = torch.var(inp_features, unbiased=False)\n",
    "print(f'input: {inp_features}')\n",
    "print(f'before LayerNorm mean: {mean_inp}, variance: {variance_inp}')\n",
    "\n",
    "# LayerNorm\n",
    "layer_norm = nn.LayerNorm([5]) # Layer Normalization 填入特徵的維度\n",
    "output = layer_norm(inp_features)\n",
    "\n",
    "# print the output and the mean and variance\n",
    "mean_opt = torch.mean(output)\n",
    "variance_opt = torch.var(output, unbiased=False)\n",
    "print(output)\n",
    "print(f'after LayerNorm mean: {mean_opt}, variance: {variance_opt}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([0.5000, 0.3000, 0.7000, 0.2000, 0.6000])\n",
      "before LayerNorm mean: 0.46000003814697266, variance: 0.03439999744296074\n",
      "tensor([ 0.2156, -0.8625,  1.2938, -1.4016,  0.7547],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "after LayerNorm mean: 0.0, variance: 0.9997094869613647\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding + Input Embedding\n",
    "\n",
    "### Code 簡介\n",
    "`cls_token = nn.Parameter(torch.randn(1, output_channels))`\n",
    "\n",
    "這行程式碼的意思是創建一個可學習的類別標記(class token),並將其定義為模型的參數。讓我們分解這行程式碼:\n",
    "\n",
    "- `torch.randn(1, output_channels)` 創建了一個形狀為 (1, output_channels) 的隨機張量。\n",
    "    - `1` 表示這個張量只有一個元素,即類別標記。\n",
    "    - `output_channels` 表示類別標記的維度,與嵌入的維度相同。\n",
    "    - `torch.randn` 函數從標準常態分佈中隨機取樣值來初始化張量。\n",
    "- `nn.Parameter(...)` 將這個隨機初始化的張量封裝為一個可學習的參數。\n",
    "    - 通過將張量傳遞給 `nn.Parameter`,我們告訴 PyTorch 這個張量是模型的一部分,需要在訓練過程中進行優化和更新。\n",
    "    - 在這種情況下,類別標記 `cls_token` 將作為模型的一個參數,在反向傳播期間根據梯度進行更新。\n",
    "`cls_token = ...` 將這個可學習的參數賦值給 `cls_token` 變數,以便在模型的前向傳播中使用。\n",
    "\n",
    "舉個例子,假設 output_channels 的值為 20,那麼 cls_token 將是一個形狀為 (1, 20) 的張量,表示一個 20 維的類別標記。在訓練過程中,這個類別標記將與嵌入的序列一起傳遞給模型,並與序列的其他部分一起進行優化。\n",
    "\n",
    "\n",
    "`torch.cat([einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0]), embedded], dim=1)`\n",
    "\n",
    "這行程式碼的目的是將類別標記 (`cls_token`) 重複 `x.shape[0]` 次,並將其與嵌入的序列 (`embedded`) 在第二個維度 (dim=1) 上進行串聯。\n",
    "\n",
    "1. `einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0])`:\n",
    "    \n",
    "    - `self.cls_token` 是形狀為 `(1, output_channels)` 的類別標記張量。\n",
    "        \n",
    "    - `einops.repeat` 是一個函數,用於重塑和重複張量。它使用一種特殊的表示法來指定輸入和輸出的維度。\n",
    "        \n",
    "    - `\"n e -> b n e\"` 是重塑的表示法,其中:\n",
    "        \n",
    "        - `n` 表示類別標記的數量,這裡是 1。\n",
    "            \n",
    "        - `e` 表示類別標記的維度,即 `output_channels`。\n",
    "            \n",
    "        - `b` 表示批次大小,即 `x.shape[0]`。\n",
    "            \n",
    "    - `b=x.shape[0]` 指定了重複的次數,即批次大小。\n",
    "        \n",
    "    - 這行程式碼的作用是將類別標記 `cls_token` 重複 `x.shape[0]` 次,使其與批次中的每個樣本相對應。\n",
    "        \n",
    "2. `torch.cat([einops.repeat(...), embedded], dim=1)`:\n",
    "    \n",
    "    - `torch.cat` 是一個函數,用於在指定維度上串聯張量。\n",
    "        \n",
    "    - 這裡,我們將重複後的類別標記張量 `einops.repeat(...)` 和嵌入的序列張量 `embedded` 在第二個維度 (dim=1) 上進行串聯。\n",
    "        \n",
    "    - 串聯後的結果將是一個新的張量,形狀為 `(batch_size, sequence_length + 1, output_channels)`,其中第二維的長度增加了 1,**因為我們在序列的開頭添加了類別標記**。\n",
    "\n",
    "### Positional Encoding 的特色\n",
    "\n",
    "因為Transformer中沒有Conv跟Recurrent, 沒有東西可以表示token在序列中的\"相對位置或是絕對位置\"，也就是說Attention機制沒有考慮\"順序\" ，所以需要Positional Encoding。因此他在encoder和decoder最底層的input embedding加上positional encodings來表示位置"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.037115Z",
     "start_time": "2024-05-01T12:52:45.017082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearEmbedding(nn.Sequential):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.LayerNorm(output_channels),\n",
    "            nn.GELU()\n",
    "        ])\n",
    "        # 創建一個可學習的類別標記(class token)，因為輸入的 channel 是一維的，並將其定義為模型的參數(nn.Parameter 特有的功能)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, output_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 直接使用上面創建的 Linear -> LayerNorm -> GELU()\n",
    "        embedded = super().forward(x)\n",
    "        return torch.cat([einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0]), embedded], dim=1)\n"
   ],
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GeLU\n",
    "Gaussian Error Linear Unit (GELU) 是一種激活函數，它結合了線性函數和高斯誤差函數。GELU 的定義如下:\n",
    "$$\n",
    "\\text{GELU}\\left(x\\right) = x{P}\\left(X\\leq{x}\\right) = x\\Phi\\left(x\\right) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}(x/\\sqrt{2})\\right],\n",
    "$$\n",
    "\n",
    "可以近似成\n",
    "\n",
    "$$\n",
    "0.5x\\left(1+\\tanh\\left[\\sqrt{2/\\pi}\\left(x + 0.044715x^{3}\\right)\\right]\\right)\n",
    "$$\n",
    "\n",
    "#### 電路\n",
    "使用LUT進行激活函數"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.114905Z",
     "start_time": "2024-05-01T12:52:45.102365Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LayerNorm\n",
    "\n",
    "$\\gamma,\\ \\beta$ 為可學習的參數，$\\epsilon$ 為避免梯度消失而添加\n",
    "\n",
    "$$\n",
    "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2,\\quad \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\n",
    "$$\n",
    "\n",
    "$\\epsilon =  1\\times10^-5$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 電路執行\n",
    "\n",
    "1. 計算輸入陣列的平均值\n",
    "2. 計算標準差\n",
    "3. 對每個值減去平均值，然後除標準差，不需要加上 1e-5"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.301071Z",
     "start_time": "2024-05-01T12:52:45.286517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "signal = np.array([1.00E+00,7.58E-01,1.12E-01,0.00E+00,8.06E-02,7.85E-02,6.61E-02,4.96E-02])\n",
    "\n",
    "# avg =  np.mean(signal)\n",
    "sum_avg = 0\n",
    "for i in range(len(signal)):\n",
    "    sum_avg += signal[i]\n",
    "avg = sum_avg / len(signal)\n",
    "\n",
    "# var = np.var(signal)\n",
    "sum_var = 0\n",
    "for i in range(len(signal)):\n",
    "    sum_var += (signal[i] - avg) ** 2\n",
    "var =  sum_var / len(signal)\n",
    "\n",
    "layer_norm = (signal - avg) / np.sqrt(var + 1e-5)\n",
    "print(layer_norm)\n",
    "\n",
    "signal_torch = torch.tensor(signal)\n",
    "layer_norm_torch = nn.LayerNorm(signal_torch.size())\n",
    "print(layer_norm_torch(signal_torch.float()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.03811871  1.36422237 -0.43469098 -0.74657689 -0.52213043 -0.52797829\n",
      " -0.56250851 -0.60845599]\n",
      "tensor([ 2.0381,  1.3642, -0.4347, -0.7466, -0.5221, -0.5280, -0.5625, -0.6085],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### cls_token 是甚麼?\n",
    "\n",
    "輸入是 batch size = 1 且有 186 個點的訊號(`Size=[186]`)，透過線性層輸出 embed_size = 192 個 channel(`Size=[1, 186, 192]`)，再產生對應批次和通道的 `cls_token`(`Size=[1,1,192]`)，最後透過 `torch.cat` 串接起來(`Size=[1, 187, 192]`)。 "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.502811Z",
     "start_time": "2024-05-01T12:52:45.475218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class embed(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.LayerNorm(output_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# cls_token 是一個二維矩陣但是存了一筆一維陣列\n",
    "# 假設 embed_size = 8, input_channels = 1, batch_size = 1\n",
    "signal = torch.tensor([1.00E+00,7.58E-01,1.12E-01,0.00E+00,8.06E-02,7.85E-02,6.61E-02,4.96E-02])\n",
    "batch_signal_channel = signal.unsqueeze(0).unsqueeze(2) # 在第0位置增加維度，在第2位置增加維度\n",
    "\n",
    "embed = embed(1, 8)\n",
    "embed_signal = embed(batch_signal_channel)\n",
    "\n",
    "cls_token = nn.Parameter(torch.randn(1, 8))\n",
    "cls_token_repeat = einops.repeat(cls_token, \"n e -> b n e\", b=batch_signal_channel.shape[0])\n",
    "\n",
    "output = torch.cat(([cls_token_repeat, embed_signal]), dim=1)"
   ],
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.582286Z",
     "start_time": "2024-05-01T12:52:45.561715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Create a new PrettyTable instance\n",
    "table = PrettyTable()\n",
    "\n",
    "# Add the column headers\n",
    "table.field_names = [\"Variable\", \"Size\", \"Note\"]\n",
    "\n",
    "# 假設 embed_size = 8, batch_size = 1, signal length = 8\n",
    "table.add_row([\"signal\", signal.size(), \"input signal consists of 8 samples\"])\n",
    "table.add_row([\"batch_signal_channel\", batch_signal_channel.size(), \"Size=[batch, 187, 1], Corresponds to batch, signal length, channel.\"])\n",
    "table.add_row([\"embed_signal\", embed_signal.size(), \"Through the first layer, the output is sent to embed_size neurons.\"])\n",
    "table.add_row([\"cls_token\", cls_token.size(), \"The number of embed_size determines the length of the token needed.\"])\n",
    "table.add_row([\"cls_token_repeat\", cls_token_repeat.size(), \"Batch size = 1\"])\n",
    "table.add_row([\"output\", output.size(), \"concatenate the vector with dimension = 1.\"])\n",
    "\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------+---------------------------------------------------------------------+\n",
      "|       Variable       |          Size         |                                 Note                                |\n",
      "+----------------------+-----------------------+---------------------------------------------------------------------+\n",
      "|        signal        |    torch.Size([8])    |                  input signal consists of 8 samples                 |\n",
      "| batch_signal_channel | torch.Size([1, 8, 1]) | Size=[batch, 187, 1], Corresponds to batch, signal length, channel. |\n",
      "|     embed_signal     | torch.Size([1, 8, 8]) |  Through the first layer, the output is sent to embed_size neurons. |\n",
      "|      cls_token       |   torch.Size([1, 8])  | The number of embed_size determines the length of the token needed. |\n",
      "|   cls_token_repeat   | torch.Size([1, 1, 8]) |                            Batch size = 1                           |\n",
      "|        output        | torch.Size([1, 9, 8]) |              concatenate the vector with dimension = 1.             |\n",
      "+----------------------+-----------------------+---------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Linearembedding 的輸入與輸出"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.613401Z",
     "start_time": "2024-05-01T12:52:45.587793Z"
    }
   },
   "cell_type": "code",
   "source": [
    " \n",
    "input_channels = 1\n",
    "output_channels = 192\n",
    "batch_size = 1\n",
    "sequence_length = 5\n",
    "x = torch.randn(batch_size, sequence_length, input_channels)\n",
    "\n",
    "# 第一維 4 表示批次大小。\n",
    "# 第二維 6 表示串聯後的序列長度,其中包括 5 個原始序列元素和 1 個添加的類別標記。\n",
    "# 第三維 20 表示嵌入的維度。\n",
    "embedding = LinearEmbedding(input_channels, output_channels)\n",
    "output = embedding(x)\n",
    "print(f\"LinearEmbedding:{embedding}\")\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"output.shape: {output.shape}\")\n",
    "# 5->6 因為增加一個類別標記\n",
    "# 1->192，因為 Linear embedding 的 output channel 設定 192"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearEmbedding:LinearEmbedding(\n",
      "  (0): Linear(in_features=1, out_features=192, bias=True)\n",
      "  (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): GELU(approximate='none')\n",
      ")\n",
      "x.shape: torch.Size([1, 5, 1])\n",
      "output.shape: torch.Size([1, 6, 192])\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Residual connection"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.644523Z",
     "start_time": "2024-05-01T12:52:45.631984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualAdd(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`ResuidalAdd`"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.691667Z",
     "start_time": "2024-05-01T12:52:45.676133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 定義一個簡單的神經網絡模塊，將輸入乘以2\n",
    "class DoubleBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# 使用ResidualAdd類來創建一個帶殘差連接的模塊\n",
    "residual_block = ResidualAdd(DoubleBlock())\n",
    "\n",
    "# 創建一個輸入張量\n",
    "input_tensor = torch.tensor([1, 2, 3], dtype=torch.int8)\n",
    "\n",
    "# 通過殘差模塊傳遞輸入\n",
    "output = residual_block(input_tensor)\n",
    "\n",
    "# 輸入 [1,2,3] 經過 DoubleBlock 得到 [2,4,6]，再加上輸入得到 [3,6,9]\n",
    "# 此例子充分展示 Residual add 的功能，通過 x + self.block(x) 進行殘差連接\n",
    "# x 是原本的輸入 tensor，self.block(x) 是經過模塊處理後的輸出\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 6, 9], dtype=torch.int8)\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Head-Attention\n",
    "\n",
    "### 簡單說明 Multi-Head-Attention 用到的變數\n",
    "這段代碼定義了一個名為 MultiHeadAttention 的 PyTorch 模組類別,用於實現多頭注意力機制。讓我們逐步解釋這段代碼:\n",
    "\n",
    "1. `__init__` 方法初始化了模組的參數,包括三個線性投影層 (queries_projection, values_projection, keys_projection) 用於將輸入張量投影到 queries、keys 和 values 的空間中。另一個線性投影層 final_projection 用於最終的輸出。此外,還設置了 embed_size 和 num_heads 參數。\n",
    "2. `forward` 方法定義了模組的前向傳播行為。它首先檢查輸入張量 x 的維度是否為 3 (批次大小、序列長度、嵌入維度)。\n",
    "接下來,輸入張量 x 被投影到 keys、values 和 queries 的空間中。\n",
    "3. 使用 `einops` 庫,keys、values 和 queries 被重新排列為多頭表示,其中每個頭對應一個特定的子空間。\n",
    "4. 計算 `queries` 和 keys 之間的點積,得到 energy_term。\n",
    "5. `energy_term` 被縮放以防止極端的 softmax 值,然後經過 softmax 運算得到 mh_out。\n",
    "6. 使用 `torch.einsum` 計算加權和,將 mh_out 與 values 相乘並求和,得到每個頭的輸出。\n",
    "7. 所有頭的輸出被連接起來,然後通過 `final_projection` 層得到最終的輸出張量。\n",
    "\n",
    "總的來說,這個模組實現了標準的多頭注意力機制,將輸入序列映射到一組注意力加權的表示,捕獲了不同子空間中的重要信息。這種注意力機制被廣泛應用於諸如 Transformer 等自然語言處理模型中。\n",
    "\n",
    "### Attention 的運作原理\n",
    "\n",
    "#### Attention 簡介\n",
    "- 注意力function做的事情可被描述為mapping一個query和一群key-value的pairs到輸出 (Q, K, V, Output都是vector)\n",
    "- Output is computed as a weighted sum of the values，values的weight則是由query和其相對應的key所算出\n",
    "- Q,K,V 在文字翻譯上而言， Q是在找哪個字的key vector可能會貢獻我的語意最多 , K是這個字可以貢獻給哪個字最多語意, V是最後的輸出，也就是這個字的語意是什麼。\n",
    "- 換句話說就是\n",
    "    - Q : to match other\n",
    "    - K : to be matched\n",
    "    - V : information to be extracted\n",
    "    - Attention: 吃兩個向量，輸出一個分數來代表這兩個向量有多匹配、多相關\n",
    "- Self Attention: 拿每個q去對每個k做Attention (Scaled Dot-product attention)\n",
    "\n",
    "\n",
    "#### Transformer 的 Scaled Dot-Product Attention\n",
    "一樣是做 Q 和 K 的內積，內積以矩陣乘法表示的話就是 $QK^{T}$，但是 Q, K 都是 $d_k$ 維度的輸入，也就是 embedded size。內積完的結果除 $\\sqrt{d_k}$，再送入 softmax 就是 attention 的結果。**需要注意的是 Q的計算和這個 attention 的計算是同時進行的(硬體 Pipeline 優化的方向)** \n",
    "\n",
    "> 研究員懷疑 $d_k$ 很大的時候dot product的規模太大，導致softmax後的數值太小 (extremely small gradients), 才會加開根號緩解\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "### Multi-Head Attention : Transformer 最重要的機制\n",
    "![](https://imgur-backup.hackmd.io/4xMlLna.png)\n",
    "- 很多個 self-attention concat 後所得出的結果\n",
    "- Multi-head attention 用來計算相對全部的字而言當前這個字能給予多少的資訊量 (讓每個head都能學到不同feature的特徵)\n",
    "- Transformer 原文使用 8 個 heads，每個 head 的維度是 64，也就是 embedded size。但是每個 head 都有降維，實際上的計算不會和 single-head 差太多。\n",
    "\n",
    "> Self-attention layer in Encoder: 在這一層中 所有的key, values, queries都是從同一個地方來的, 他們都是從前一層的encoder的output來的\n",
    "\n",
    "### FFN (Feed Forward Network) MLP\n",
    "也就是在 `model.py` 中的 MLP，FFN 通常由兩個線性變換(Linear)和一個 ReLU(GeLU) 組成，層和層之間(不同的head之間)是用不同的參數丟入FFN之中。\n",
    "\n",
    "> FFN 可看成kernel size=1的2個 Conv\n",
    "\n",
    "\n",
    "### Residual Connection\n",
    "\n",
    "![](https://imgur-backup.hackmd.io/JBqsUsH.png)\n",
    "\n",
    "`TransformerEncoderLayer` 描述的就是 encoder 的灰色方框，由Multi-head self-attention 和 FC Feed Forward所組成\n",
    "，其中每兩個 sublayers 之間用Residual connection連接。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, attention_store=None):\n",
    "        super().__init__()\n",
    "        # 通過 nn.Linear 相當於做通過全連接層\n",
    "        self.queries_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.values_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.keys_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.final_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Size=[batch, seq_len, embed_size]\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 3\n",
    "        keys = self.keys_projection(x)\n",
    "        values = self.values_projection(x)\n",
    "        queries = self.queries_projection(x)\n",
    "        keys = einops.rearrange(keys, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        queries = einops.rearrange(queries, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        values = einops.rearrange(values, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        # q, k 做 dot-product -> self-attention\n",
    "        energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)\n",
    "        divider = sqrt(self.embed_size)\n",
    "        mh_out = torch.softmax(energy_term, -1)\n",
    "        out = torch.einsum('bihv, bvhd -> bihd ', mh_out / divider, values)\n",
    "        out = einops.rearrange(out, \"b n h e -> b n (h e)\")\n",
    "        return self.final_projection(out)\n",
    "    \n",
    "\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_channels, expansion=4):\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, input_channels * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_channels * expansion, input_channels)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "class TransformerEncoderLayer(torch.nn.Sequential):\n",
    "    def __init__(self, embed_size=768, expansion=4, num_heads=8, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__(\n",
    "            *[\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MultiHeadAttention(embed_size, num_heads),\n",
    "                    nn.Dropout(dropout)\n",
    "                ])),\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MLP(embed_size, expansion),\n",
    "                    nn.Dropout(dropout)\n",
    "                ]))\n",
    "            ]\n",
    "        )"
   ],
   "metadata": {
    "id": "s-06i9LCFWPW",
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.753864Z",
     "start_time": "2024-05-01T12:52:45.738317Z"
    }
   },
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`einops.rearrange` \n",
    "\n",
    "- b 表示批次大小。\n",
    "- n 表示序列長度。\n",
    "- h 表示頭的數量，這裡是 self.num_heads。\n",
    "- e 表示每個頭的嵌入維度，這裡是 embed_size / num_heads。\n",
    "\n",
    "將輸入張量 keys 的形狀從 (batch_size, sequence_length, embed_size) 變為 (batch_size, sequence_length, num_heads, embed_size / num_heads)。這樣做的目的是將嵌入維度 embed_size 分割成 num_heads 個子空間，每個子空間的維度是 embed_size / num_heads。舉例來說，假設我們有一個形狀為 (32, 10, 64) 的張量，num_heads 為 8。那麼這行程式碼將會將這個張量的形狀變為 (32, 10, 8, 8)。這意味著我們將 **64 維的嵌入空間分割成了 8 個 8 維的子空間，每個子空間對應一個頭**。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi-Head-Attention Calculation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 查看 b n (h e) -> b n h e 如何拆解\n",
    "\n",
    "\n",
    "- `b` represents the batch size.\n",
    "-  `q` and `k` represent the sequence length, but they are different dimensions because one is for queries and the other is for keys.\n",
    "-  `h` represents the number of attention heads.\n",
    "-  `e` represents the embedding size."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:45.801037Z",
     "start_time": "2024-05-01T12:52:45.794036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:52:47.938378Z",
     "start_time": "2024-05-01T12:52:47.905274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 假設我們有一個批次大小為 32，序列長度為 10，頭數為 8，嵌入維度為 64 的模型\n",
    "batch_size = 1\n",
    "seq_length = 186\n",
    "num_heads = 6\n",
    "embed_size = 192\n",
    "\n",
    "# 創建隨機張量 queries 和 keys\n",
    "Linear = Model(embed_size)\n",
    "queries = Linear(torch.randn(batch_size, seq_length, embed_size))\n",
    "print(f\"queries.shape: {queries.shape}\")\n",
    "print(queries[0][0][32:64]) # 查看 queries [0, 186, 192] 中 186 的第 0 個的 32 到 64 的值\n",
    "queries_head_embed = einops.rearrange(queries, \"b n (h e) -> b n h e\", h=num_heads)\n",
    "print(queries_head_embed[0][0].shape)# 查看 queries [0, 186, 192] 中 186 的第 0 個\n",
    "print(queries_head_embed[0][0][1]) # 查看 qu\n",
    "\n",
    "# (h e) -> (h e) 是按照矩陣的順序拆解的"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape: torch.Size([1, 186, 192])\n",
      "tensor([-0.4552, -1.1705, -0.0840,  0.7994, -0.2486,  0.4171, -0.7824, -0.3609,\n",
      "        -0.5861, -0.4159,  0.5052, -0.4152,  0.0929,  1.2724, -0.8910,  0.1214,\n",
      "        -0.4691,  1.1529,  0.1098, -0.2445,  0.7564, -0.3510,  0.2852, -1.2411,\n",
      "         0.3565, -0.3883,  0.0108,  0.7387, -0.4135, -0.0317,  0.2832,  0.4710],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([6, 32])\n",
      "tensor([-0.4552, -1.1705, -0.0840,  0.7994, -0.2486,  0.4171, -0.7824, -0.3609,\n",
      "        -0.5861, -0.4159,  0.5052, -0.4152,  0.0929,  1.2724, -0.8910,  0.1214,\n",
      "        -0.4691,  1.1529,  0.1098, -0.2445,  0.7564, -0.3510,  0.2852, -1.2411,\n",
      "         0.3565, -0.3883,  0.0108,  0.7387, -0.4135, -0.0317,  0.2832,  0.4710],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 計算 queries 和 keys 的點積\n",
    "\n",
    "`energy_term`: Size = (batch_size, seq_length_q, num_heads, seq_length_k)。假設 batch_size = 1，第 h 個 head 中第 q 個 query 和第 k 個 key 的點積等於 `energy_term[0][q][h][k]`。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T08:44:28.949704Z",
     "start_time": "2024-05-01T08:44:28.910106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "# Assume queries_head_embed and keys_head_embed have shape (1, 186, 6, 32)\n",
    "batch_size, seq_len, num_heads, embed_size = 1, 186, 6, 32\n",
    "\n",
    "# Create random numpy arrays for queries_head_embed and keys_head_embed\n",
    "queries_head_embed = np.random.randn(batch_size, seq_len, num_heads, embed_size)\n",
    "keys_head_embed = np.random.randn(batch_size, seq_len, num_heads, embed_size)\n",
    "\n",
    "# bqhd,bkhd->bqhk 針對每個 head 做 q,k 的 dot product\n",
    "energy_term_einsum = np.einsum('bqhd,bkhd->bqhk', queries_head_embed, keys_head_embed)\n",
    "energy_term_head_zero_q0_k1 = sum(queries_head_embed[0][0][0][:] * keys_head_embed[0][1][0][:])\n",
    "energy_term_head_zero_q185_k185 = sum(queries_head_embed[0][185][0][:] * keys_head_embed[0][185][0][:])\n",
    "\n",
    "print(f\"energy_term.shape: {energy_term_einsum.shape}\") \n",
    "print(\"=============================================================\")\n",
    "\n",
    "\n",
    "# 第零個 head 的第一個 query 和第一個 key 的點積\n",
    "print(\"第零個 head 的第一個 query 和第一個 key 的點積\")\n",
    "energy_term_head_zero_q0_k0 = sum(queries_head_embed[0][0][0][:] * keys_head_embed[0][0][0][:])\n",
    "print(f\"energy_term_head_zero_q0_k0: {energy_term_head_zero_q0_k0}\")  # Output: (1, 186, 6, 186)\n",
    "print(f\"energy_term[0][0][0][0]: {energy_term_einsum[0][0][0][0]}\")\n",
    "print(\"=============================================================\")\n",
    "      \n",
    "# 第零個 head 的第一個 query 和第二個 key 的點積\n",
    "print(\"第零個 head 的第一個 query 和第二個 key 的點積\")\n",
    "print(f\"energy_term_head_zero_q0_k1: {energy_term_head_zero_q0_k1}\")  # Output: (1, 186, 6, 186)\n",
    "print(f\"energy_term[0][0][0][1]: {energy_term_einsum[0][0][0][1]}\")  \n",
    "print(\"=============================================================\")\n",
    "\n",
    "# 第零個 head 的第 186 個 query 和第 186 個 key 的點積\n",
    "print(\"第零個 head 的第 186 個 query 和第 186 個 key 的點積\")\n",
    "print(f\"energy_term_head_zero_q185_k185: {energy_term_head_zero_q185_k185}\")  # Output: (1, 186, 6, 186)\n",
    "print(f\"energy_term[0][0][0][185]: {energy_term_einsum[0][185][0][185]}\") \n",
    "print(\"=============================================================\")\n",
    "\n",
    "# 第二個 head 的第 130 個 query 和第 100 個 key 的點積\n",
    "print(\"第二個 head 的第 130 個 query 和第 100 個 key 的點積\")\n",
    "energy_term_head_three_q130_k100 = sum(queries_head_embed[0][130][2][:] * keys_head_embed[0][100][2][:])\n",
    "print(f\"energy_term_head_three_q130_k100: {energy_term_head_three_q130_k100}\")\n",
    "print(f\"energy_term[0][130][2][100]: {energy_term_einsum[0][130][2][100]}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy_term.shape: (1, 186, 6, 186)\n",
      "=============================================================\n",
      "第零個 head 的第一個 query 和第一個 key 的點積\n",
      "energy_term_head_zero_q0_k0: -7.70901092448986\n",
      "energy_term[0][0][0][0]: -7.709010924489862\n",
      "=============================================================\n",
      "第零個 head 的第一個 query 和第二個 key 的點積\n",
      "energy_term_head_zero_q0_k1: -2.6244431567384368\n",
      "energy_term[0][0][0][1]: -2.624443156738433\n",
      "=============================================================\n",
      "第零個 head 的第 186 個 query 和第 186 個 key 的點積\n",
      "energy_term_head_zero_q185_k185: -17.152041727777256\n",
      "energy_term[0][0][0][185]: -17.15204172777725\n",
      "=============================================================\n",
      "第二個 head 的第 130 個 query 和第 100 個 key 的點積\n",
      "energy_term_head_three_q130_k100: 5.714561656594404\n",
      "energy_term[0][130][2][100]: 5.7145616565944035\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-Head Attention Output"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T10:13:09.786896Z",
     "start_time": "2024-05-01T10:13:09.765847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "batch_size, seq_len, num_heads, embed_size = 1, 186, 6, 32\n",
    "# energy_term.shape=[1,186,6,186]       \n",
    "energy_term = torch.randn(1, 186, 6, 186)\n",
    "mh_out = torch.softmax(energy_term, -1)\n",
    "mh_out_scale = mh_out / 13.85464 # -1 代表隊最後一個維度做 softmax\n",
    "\n",
    "# embedding size = 192, sqrt(embedding size) = 13.85464\n",
    "values = torch.randn(batch_size, seq_len, num_heads, embed_size)\n",
    "out_torch = torch.einsum('bihv, bvhd -> bihd ', mh_out_scale , values)\n",
    "\n",
    "# 每個頭的計算過程\n",
    "print(mh_out_scale[0, :, 0, :].shape) # 這個操作會直接從4維張量中選取第一個batch的所有第二維度元素,以及所有第四維度元素,結果是一個2維張量。\n",
    "print(values[0, :, 0, :].shape) # 這個操作會直接從4維張量中選取第一個batch的所有第二維度元素,以及所有第四維度元素,結果是一個2維張量。\n",
    "out_matmul_head_zero = torch.matmul(mh_out_scale[0, :, 0, :], values[0, :, 0, :])\n",
    "out_matmul_head_one = torch.matmul(mh_out_scale[0, :, 1, :], values[0, :, 1, :])\n",
    "out_matmul_head_two = torch.matmul(mh_out_scale[0, :, 2, :], values[0, :, 2, :])\n",
    "out_matmul_head_three = torch.matmul(mh_out_scale[0, :, 3, :], values[0, :, 3, :])\n",
    "out_matmul_head_four = torch.matmul(mh_out_scale[0, :, 4, :], values[0, :, 4, :])\n",
    "out_matmul_head_five = torch.matmul(mh_out_scale[0, :, 5, :], values[0, :, 5, :])\n",
    "\n",
    "out_matmul_heads = torch.stack([ out_matmul_head_zero, out_matmul_head_one, out_matmul_head_two,\n",
    "    out_matmul_head_three, out_matmul_head_four, out_matmul_head_five], dim=1)\n",
    "\n",
    "out_matmul_heads = out_matmul_heads.unsqueeze(0)\n",
    "\n",
    "print(f\"out_torch.shape: {out_torch.shape}\")\n",
    "print(f\"out_matmul.shape: {out_matmul_heads.shape}\")\n",
    "\n",
    "print(f\"out_torch == out_matmul_heads? {torch.allclose(out_torch, out_matmul_heads)}\")\n",
    "# out = einops.rearrange(out, \"b n h e -> b n (h e)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 186])\n",
      "torch.Size([186, 32])\n",
      "out_torch.shape: torch.Size([1, 186, 6, 32])\n",
      "out_matmul.shape: torch.Size([1, 186, 6, 32])\n",
      "out_torch == out_matmul_heads? True\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T10:21:25.288261Z",
     "start_time": "2024-05-01T10:21:25.272625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape \n",
    "# print(out_matmul_heads.shape())\n",
    "# print(out_matmul_heads[0,:,0,:])\n",
    "# print(f\"out_matmul_heads.shape: {out_matmul_heads[0,:,0,:].shape}\")\n",
    "\n",
    "print(f\"out_matmul_heads.shape: {out_matmul_heads.shape}\")\n",
    "# 實際算法\n",
    "out_matmul_heads_manual_re = torch.cat((out_matmul_heads[0,:,0,:], out_matmul_heads[0,:,1,:], out_matmul_heads[0,:,2,:],\n",
    "                             out_matmul_heads[0,:,3,:], out_matmul_heads[0,:,4,:], out_matmul_heads[0,:,5,:]), dim=1)\n",
    "out_matmul_heads_re= einops.rearrange(out_matmul_heads,\"b n h e -> b n (h e)\")\n",
    "\n",
    "out_matmul_heads_manual_re = out_matmul_heads_manual_re.unsqueeze(0)\n",
    "print(f\"out_matmul_heads_re.shape: {out_matmul_heads_re.shape}\")\n",
    "print(f\"out_matmul_heads_manual_re == out_matmul_heads_re? {torch.allclose(out_matmul_heads_manual_re, out_matmul_heads_re)}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_matmul_heads.shape: torch.Size([1, 186, 6, 32])\n",
      "out_matmul_heads_re.shape: torch.Size([1, 186, 192])\n",
      "out_matmul_heads_manual_re == out_matmul_heads_re? True\n"
     ]
    }
   ],
   "execution_count": 137
  }
 ]
}
