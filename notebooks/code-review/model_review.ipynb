{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "mount_file_id": "1kC5OaXAzH5gPIp_YH4RjewVYJl1Q6OlK",
   "authorship_tag": "ABX9TyPyR551RQXU/DgDzdhuOCRg"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Model Reveiew",
   "metadata": {
    "id": "7BHqO2iOEDn-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from math import sqrt\n",
    "\n",
    "import einops\n",
    "from einops.layers.torch import Reduce\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtcvQe1JzOiC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712836267688,
     "user_tz": -480,
     "elapsed": 20916,
     "user": {
      "displayName": "Xaio",
      "userId": "03940136356471857780"
     }
    },
    "outputId": "14c7c985-9ea5-43a9-e52d-3cc116181bc0",
    "ExecuteTime": {
     "end_time": "2024-04-21T04:41:29.580216Z",
     "start_time": "2024-04-21T04:41:27.357036Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic PyTorch"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Dropout \n",
    "Each forward pass will zero out some of the elements of the input tensor with probability p. And it will scale the remaining elements by 1/(1-p)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T05:22:55.990898Z",
     "start_time": "2024-04-21T05:22:55.971594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = 0.5\n",
    "\n",
    "module = nn.Dropout(p=p)\n",
    "module.training\n",
    "inp = torch.ones(3,5)\n",
    "print(f'scale: {1/(1-p)}')\n",
    "print(f'before dropout:\\n{inp}')\n",
    "print(f'after droput:\\n{module(inp)}')\n",
    "# module(inp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: 2.0\n",
      "before dropout:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "after droput:\n",
      "tensor([[0., 2., 2., 2., 0.],\n",
      "        [0., 2., 0., 0., 2.],\n",
      "        [0., 0., 2., 2., 0.]])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Linear\n",
    "轉換輸入與輸出之間的維度關係"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T07:45:09.603829Z",
     "start_time": "2024-04-21T07:45:09.589868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 將輸入 dimension 10 轉換成 output dimension 20\n",
    "module = nn.Linear(in_features=10, out_features=20)\n",
    "print(f'module:{module}') \n",
    "\n",
    "n_samples = 40\n",
    "# 最後面的 10 是輸入的維度，前面是指定的 batch size\n",
    "inp_2d = torch.randn(n_samples, 10) \n",
    "inp_3d = torch.randn(n_samples, 33, 10)\n",
    "inp_5d = torch.randn(n_samples, 2, 3, 4, 5, 10)\n",
    "# inp_5d_false = torch.randn(n_samples, 2, 3, 4, 5, 20)\n",
    "\n",
    "print(f'module(inp_2d).shape: {module(inp_2d).shape}')\n",
    "print(f'module(inp_3d).shape: {module(inp_3d).shape}')\n",
    "print(f'module(inp_5d).shape: {module(inp_5d).shape}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module:Linear(in_features=10, out_features=20, bias=True)\n",
      "module(inp_2d).shape: torch.Size([40, 20])\n",
      "module(inp_3d).shape: torch.Size([40, 33, 20])\n",
      "module(inp_5d).shape: torch.Size([40, 2, 3, 4, 5, 20])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LayerNorm\n",
    "LayerNorm 會對輸入進行標準化，使得輸入的均值為 0，方差為 1。這有助於模型訓練，因為它可以使不同特徵的數值範圍保持一致，從而更容易學習權重。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:16:08.473833Z",
     "start_time": "2024-04-21T08:16:08.462354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 輸入特徵是一維資料\n",
    "inp_features = torch.tensor([0.5, 0.3, 0.7, 0.2, 0.6])\n",
    "\n",
    "# print the input and the mean and variance\n",
    "mean_inp = torch.mean(inp_features)\n",
    "variance_inp = torch.var(inp_features, unbiased=False)\n",
    "print(f'input: {inp_features}')\n",
    "print(f'before LayerNorm mean: {mean_inp}, variance: {variance_inp}')\n",
    "\n",
    "# LayerNorm\n",
    "layer_norm = nn.LayerNorm([5]) # Layer Normalization 填入特徵的維度\n",
    "output = layer_norm(inp_features)\n",
    "\n",
    "# print the output and the mean and variance\n",
    "mean_opt = torch.mean(output)\n",
    "variance_opt = torch.var(output, unbiased=False)\n",
    "print(output)\n",
    "print(f'after LayerNorm mean: {mean_opt}, variance: {variance_opt}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([0.5000, 0.3000, 0.7000, 0.2000, 0.6000])\n",
      "before LayerNorm mean: 0.46000003814697266, variance: 0.03439999744296074\n",
      "tensor([ 0.2156, -0.8625,  1.2938, -1.4016,  0.7547],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "after LayerNorm mean: 0.0, variance: 0.9997094869613647\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Positional Encoding + Input Embedding\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "class LinearEmbedding(nn.Sequential):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.LayerNorm(output_channels),\n",
    "            nn.GELU()\n",
    "        ])\n",
    "        # 創建一個可學習的類別標記(class token),並將其定義為模型的參數\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, output_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 直接使用上面創建的 Linear -> LayerNorm -> GELU()\n",
    "        embedded = super().forward(x)\n",
    "        return torch.cat([einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0]), embedded], dim=1)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`cls_token = nn.Parameter(torch.randn(1, output_channels))`\n",
    "\n",
    "這行程式碼的意思是創建一個可學習的類別標記(class token),並將其定義為模型的參數。讓我們分解這行程式碼:\n",
    "\n",
    "- `torch.randn(1, output_channels)` 創建了一個形狀為 (1, output_channels) 的隨機張量。\n",
    "    - `1` 表示這個張量只有一個元素,即類別標記。\n",
    "    - `output_channels` 表示類別標記的維度,與嵌入的維度相同。\n",
    "    - `torch.randn` 函數從標準常態分佈中隨機取樣值來初始化張量。\n",
    "- `nn.Parameter(...)` 將這個隨機初始化的張量封裝為一個可學習的參數。\n",
    "    - 通過將張量傳遞給 `nn.Parameter`,我們告訴 PyTorch 這個張量是模型的一部分,需要在訓練過程中進行優化和更新。\n",
    "    - 在這種情況下,類別標記 `cls_token` 將作為模型的一個參數,在反向傳播期間根據梯度進行更新。\n",
    "`cls_token = ...` 將這個可學習的參數賦值給 `cls_token` 變數,以便在模型的前向傳播中使用。\n",
    "\n",
    "舉個例子,假設 output_channels 的值為 20,那麼 cls_token 將是一個形狀為 (1, 20) 的張量,表示一個 20 維的類別標記。在訓練過程中,這個類別標記將與嵌入的序列一起傳遞給模型,並與序列的其他部分一起進行優化。\n",
    "\n",
    "\n",
    "`torch.cat([einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0]), embedded], dim=1)`\n",
    "\n",
    "這行程式碼的目的是將類別標記 (`cls_token`) 重複 `x.shape[0]` 次,並將其與嵌入的序列 (`embedded`) 在第二個維度 (dim=1) 上進行串聯。\n",
    "\n",
    "1. `einops.repeat(self.cls_token, \"n e -> b n e\", b=x.shape[0])`:\n",
    "    \n",
    "    - `self.cls_token` 是形狀為 `(1, output_channels)` 的類別標記張量。\n",
    "        \n",
    "    - `einops.repeat` 是一個函數,用於重塑和重複張量。它使用一種特殊的表示法來指定輸入和輸出的維度。\n",
    "        \n",
    "    - `\"n e -> b n e\"` 是重塑的表示法,其中:\n",
    "        \n",
    "        - `n` 表示類別標記的數量,這裡是 1。\n",
    "            \n",
    "        - `e` 表示類別標記的維度,即 `output_channels`。\n",
    "            \n",
    "        - `b` 表示批次大小,即 `x.shape[0]`。\n",
    "            \n",
    "    - `b=x.shape[0]` 指定了重複的次數,即批次大小。\n",
    "        \n",
    "    - 這行程式碼的作用是將類別標記 `cls_token` 重複 `x.shape[0]` 次,使其與批次中的每個樣本相對應。\n",
    "        \n",
    "2. `torch.cat([einops.repeat(...), embedded], dim=1)`:\n",
    "    \n",
    "    - `torch.cat` 是一個函數,用於在指定維度上串聯張量。\n",
    "        \n",
    "    - 這裡,我們將重複後的類別標記張量 `einops.repeat(...)` 和嵌入的序列張量 `embedded` 在第二個維度 (dim=1) 上進行串聯。\n",
    "        \n",
    "    - 串聯後的結果將是一個新的張量,形狀為 `(batch_size, sequence_length + 1, output_channels)`,其中第二維的長度增加了 1,**因為我們在序列的開頭添加了類別標記**。\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 14,
   "source": [
    "input_channels = 1\n",
    "output_channels = 192\n",
    "batch_size = 32\n",
    "sequence_length = 5\n",
    "\n",
    "x = torch.randn(batch_size, sequence_length, input_channels)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LinearEmbedding:LinearEmbedding(\n",
      "  (0): Linear(in_features=1, out_features=192, bias=True)\n",
      "  (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): GELU(approximate='none')\n",
      ")\n",
      "output.shape: torch.Size([32, 6, 192])\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "# 第一維 4 表示批次大小。\n",
    "# 第二維 6 表示串聯後的序列長度,其中包括 5 個原始序列元素和 1 個添加的類別標記。\n",
    "# 第三維 20 表示嵌入的維度。\n",
    "embedding = LinearEmbedding(input_channels, output_channels)\n",
    "output = embedding(x)\n",
    "print(f\"LinearEmbedding:{embedding}\")\n",
    "print(f\"output.shape: {output.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Residual connection"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "class ResidualAdd(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 6, 9], dtype=torch.int8)\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "import torch\n",
    "\n",
    "# 定義一個簡單的神經網絡模塊，將輸入乘以2\n",
    "class DoubleBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# 使用ResidualAdd類來創建一個帶殘差連接的模塊\n",
    "residual_block = ResidualAdd(DoubleBlock())\n",
    "\n",
    "# 創建一個輸入張量\n",
    "input_tensor = torch.tensor([1, 2, 3], dtype=torch.int8)\n",
    "\n",
    "# 通過殘差模塊傳遞輸入\n",
    "output = residual_block(input_tensor)\n",
    "\n",
    "# 輸入 [1,2,3] 經過 DoubleBlock 得到 [2,4,6]，再加上輸入得到 [3,6,9]\n",
    "# 此例子充分展示 Residual add 的功能，通過 x + self.block(x) 進行殘差連接\n",
    "# x 是原本的輸入 tensor，self.block(x) 是經過模塊處理後的輸出\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Head-Attention"
   ],
   "metadata": {
    "id": "1YI4azRsEw0a"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "這段代碼定義了一個名為 MultiHeadAttention 的 PyTorch 模組類別,用於實現多頭注意力機制。讓我們逐步解釋這段代碼:\n",
    "\n",
    "1. `__init__` 方法初始化了模組的參數,包括三個線性投影層 (queries_projection, values_projection, keys_projection) 用於將輸入張量投影到 queries、keys 和 values 的空間中。另一個線性投影層 final_projection 用於最終的輸出。此外,還設置了 embed_size 和 num_heads 參數。\n",
    "2. `forward` 方法定義了模組的前向傳播行為。它首先檢查輸入張量 x 的維度是否為 3 (批次大小、序列長度、嵌入維度)。\n",
    "接下來,輸入張量 x 被投影到 keys、values 和 queries 的空間中。\n",
    "3. 使用 `einops` 庫,keys、values 和 queries 被重新排列為多頭表示,其中每個頭對應一個特定的子空間。\n",
    "4. 計算 `queries` 和 keys 之間的點積,得到 energy_term。\n",
    "5. `energy_term` 被縮放以防止極端的 softmax 值,然後經過 softmax 運算得到 mh_out。\n",
    "6. 使用 `torch.einsum` 計算加權和,將 mh_out 與 values 相乘並求和,得到每個頭的輸出。\n",
    "7. 所有頭的輸出被連接起來,然後通過 `final_projection` 層得到最終的輸出張量。\n",
    "\n",
    "總的來說,這個模組實現了標準的多頭注意力機制,將輸入序列映射到一組注意力加權的表示,捕獲了不同子空間中的重要信息。這種注意力機制被廣泛應用於諸如 Transformer 等自然語言處理模型中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, attention_store=None):\n",
    "        super().__init__()\n",
    "        self.queries_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.values_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.keys_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.final_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3\n",
    "        keys = self.keys_projection(x)\n",
    "        values = self.values_projection(x)\n",
    "        queries = self.queries_projection(x)\n",
    "        keys = einops.rearrange(keys, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        queries = einops.rearrange(queries, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        values = einops.rearrange(values, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)\n",
    "        divider = sqrt(self.embed_size)\n",
    "        mh_out = torch.softmax(energy_term, -1)\n",
    "        out = torch.einsum('bihv, bvhd -> bihd ', mh_out / divider, values)\n",
    "        out = einops.rearrange(out, \"b n h e -> b n (h e)\")\n",
    "        return self.final_projection(out)"
   ],
   "metadata": {
    "id": "s-06i9LCFWPW",
    "ExecuteTime": {
     "end_time": "2024-04-20T15:07:29.488193Z",
     "start_time": "2024-04-20T15:07:28.967246Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMultiHeadAttention\u001B[39;00m(\u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, embed_size, num_heads, attention_store\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
